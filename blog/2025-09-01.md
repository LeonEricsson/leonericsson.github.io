---
layout: post
title: "GLM-4.5"
categories: []
year: 2025
type: paper
---

A wonderful contribution to the open source community, from yet another Chinese team. GLM-4.5 is yet another MoE model at >100B parameters. We're seeing a lot of models released in this class right now, DeepSeek, Kimi, Qwen, oh yeah and even OAI with GPT-OSS, all have comparative models at this point. I know that there should be some interesting details on their RL infra, which I'm excited to read about. I've glossed over the slime repository which is their RL framework and it looks really intriguing. 


Let's first briefly compare the architecture, the table in the paper provides a nice overview. For reference we have K2 and V3. K2 is the V3 architecture but with 50% more experts in the MoE layers. This change was driven by MoE sparsity scaling law experiments conducted by the Kimi team which found that, under a fixed number of activated parameters (i.e constant FLOPs), increasing the total number of experts consistently lowers both the training and validation loss. They found this to be true up to a sparsity level of 48. Sparsity in this context is the ratio E/k, where E is the total number of experts and k the number of experts active per token. To balance the increased inference cost of more experts, they reduce the number of attention heads to 64, arguing that this only results in a modest performance decrease. So, what does GLM-4.5 do?

GLM 4.5 has 355B parameters, with 32B active. Comparing it to its peers its a lot deeper than DSv3, more similar to Qwen3 in that regard. The MoE sparsity is low at 20. Generally the architecture trades width (in hidden dim and MoE intermediate dim) for depth. The model uses standard GQA as the attention pattern with 12 groups. Going slightly against K2, GLM4.5 increases the number of attention heads to 96 because they observe this increases performance on reasoning benchmarks. QK-Norm (a RMSNorm) is incorporated to stabilize the range of attention logits. Interestingly they incorporate an MoE layer as the Multi Token Prediction layer to support speculative decoding during inference. 

| Feature                   | GLM-4.5 | GLM-4.5-Air | DeepSeek-V3 | Kimi K2 | Qwen3                 |
|---------------------------|---------|-------------|-------------|---------|------------------------|
| **Total Parameters**      | 355B    | 106B        | 671B        | 1043B   | 235B 				      |
| **Activated Parameters**  | 32B     | 12B         | 37B         | 32B     | 22B                          |
| **Dense Layers**          | 3       | 1           | 3           | 1       | 1         |
| **MoE Layers**            | 89      | 45          | 58          | 60      | 94                           |
| **MTP Layers**            | 1       | 1           | 1           | 0       | 0                          |
| **Hidden Dim**            | 5120    | 4096        | 7168        | 7168    | 4096                         |
| **Dense Intermediate Dim**| 12288   | 10944       | 18432       | 18432   | 12288                         |
| **MoE Intermediate Dim**  | 1536    | 1408        | 2048        | 2048    | 1536                          |
| **Attention Head Dim**    | 128     | 128         | 192         | 192     | 128                          |
| **# Attention Heads**     | 96      | 96          | 128         | 64      | 64       |
| **# Key-Value Heads**     | 8       | 8           | 128         | 64      | 4                            |
| **# Experts (total)**     | 160     | 128         | 256         | 384     | 128                          |
| **# Experts Active/Token**| 8       | 8           | 8           | 8       | 8                            |
| **# Shared Experts**      | 1       | 1           | 1           | 1       | 0                         |
| **QK-Norm**               | Yes     | No          | No          | No      | N/A         |



