---
layout: post
title: "what's going on with RLVR"
categories: []
year: 2025
type: blog
---

more to talk about in the RLVR space today, the wave of papers in this space hasn't slowed down yet, although we've getting less hill-climbs on MATH now so I think we're moving in a solid direction. i'm certainly a RL devotee, i think scaling RL and particularly long horizon RL is going to accelerate automation big time, the open research community probably doesn't have the compute to just purely scale RL to the moon but there is a lot of work that needs to be done on long horizon rl, and especially multi-turn rl which has yet to see the attention it deserves. if your interested in material related to the where the RL space is heading, and why it's important i recommend: (1) blog post from nathan lambert [*what comes next with reinforcement learning*](https://www.interconnects.ai/p/what-comes-next-with-reinforcement), (2) [podcast episode with sholto douglas and trenton bricken from anthropic](https://www.youtube.com/watch?v=64lXQP6cs5M), and (3) a blog post from semianalysis [*Scaling Reinforcement Learning: Environments, Reward Hacking, Agents, Scaling Data*](https://semianalysis.com/2025/06/08/scaling-reinforcement-learning-environments-reward-hacking-agents-scaling-data/). 

but this is not what i'm want to talk about today, hence the links away from this post, today is more small picture, more technical. and it builds on, and perhaps even refines a line of thought that i started in a previous post, [*rl elicits the procedural art of reasoning*](/blog/2025-05-15-rlreasoning.md), where i argued that hill-climbing RLVR papers exemplified models learning a scaffolding for how to approach general reasoning problems. this is why models [*where able to improve test time performance through only a single training example*](https://arxiv.org/pdf/2504.20571). since that writeup new information has come to light, and things really got out of hand with this paper [*Spurious Rewards: Rethinking Training Signals in RLVR*](https://github.com/ruixin31/Rethink_RLVR/blob/main/paper/rethink-rlvr.pdf) showing that models improved from rewarding the incorrect responses, and even completely random (!!) reward assignments. this is where my mental model kind of broke and things got really confusing. as it turns out the Qwen model family, particularly Qwen-Math, has some intrinsic useful reasoning behavior learned during pretraining that when combined with GRPO is amplified, even when the reward signal carries no information. this paper was probably the first to put a small damper on recent open RLVR work, and i wanted to give it the recognition it deserves because its good work, there's been way to many papers over indexed on only using the Qwen model family, we need to validate work on multiple models, especially when the models are not truly open source (data).

anyway, that was a tangent, what i really wanted to get to was the discussion of *entropy* and *exploration*. i've covered three papers in recent literature reviews that relate to this discussion and for which i want to provide a unified picture. in [*The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models*](/blog/2025-06-05-rlentropy.md) (refered to as P1 henceforth) the authors provide evidence for a entropy collapse phenomena which, according to the paper, occurs during RL when you don't explicitly formulate entropy or KL regularization. Such entropy collapse leads to a predictable upper bound on RL performance for a given task, because for a softmax policy such as LLM, *entropy* is explicitly tied to *exploration*, as entropy decreases exploration diminishes and so does the models ability to find novel solutions. Similar behavior, or should we perhaps say limitations, were noted in [*Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?*](https://arxiv.org/pdf/2504.13837), who argued that RLVR traded sampling efficiency for reasoning boundary (increased pass@1 for decreased pass@256). This is a direct consequence of the entropy-reward exchange. 

The experiments in P1 are great, and they provide a strong case for why entropy is a crucial metric to track during RLVR, but I find the suggested solutions unnecessary given that we already have proven methods to control entropy collapse; in the [DAPO paper](https://arxiv.org/pdf/2503.14476) the authors noted GRPO's tendency for entropy collapse (figure 2b) and proposed the Clip-Higher technique as a solution. This line of thought is solidified in [*ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models*](/blog/2025-06-10-prorl.md). To quote the authors:


> A key challenge in prolonged policy optimization is entropy collapse, a phenomenon where the
modelâ€™s output distribution becomes overly peaked early in training, resulting in sharply reduced
entropy. When entropy collapses, the policy prematurely commits to a narrow set of outputs, severely
limiting exploration. This is particularly detrimental in methods like GRPO, where the learning
signal depends on having a diverse set of sampled outputs to effectively estimate relative advantages.

which they address through: (1) DAPO, and (2) KL regularization with reference policy reset. ultimately their training scheme provides consistent improvement in pass@1 **and** pass@16 throughout training, providing compelling evidence that extended stable RL training develops novel reasoning patterns beyond a base model's initial capabilities.  

finally, to really hammer this point in, the Qwen team published [*Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning*](https://arxiv.org/pdf/2506.01939) last week which i haven't had time to cover yet. The gist is that, during CoT reasoning, only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. slight increases to the entropy of these fork tokens improves performance. additionally, by restricting policy gradient updates to forking tokens (20% of the total tokens) improves RLVR performance and generalization. This work also uses DAPO. 

all in all, my takeaway is to **track your entropy**. entropy collapse is a strong signal that performance will stagnate. at least track average entropy over the generation, even better if you can track the entropy distribution across tokens. 