---
layout: post
title: "More Ling"
categories: []
year: 2025
type: paper
author: Tian
exturl: https://arxiv.org/abs/2507.17702
---
I covered some Ling releases a few weeks back, specifically work on the Ling 2.0 family. They since released even more models in the model family and I wanted to provide an overview of the state of the Ling 2.0 model family, because there are a lot of models with a lot of similar names.

Ling 2.0 is a family of MoE models that builds upon the findings of the this paper [*Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models*](https://arxiv.org/abs/2507.17702) which establishes scaling laws for MoEs. Ling 2.0 should be seen as a validation, or large scale realization of the established laws of that paper. The laws were established based on experiments mainly in the 10^18 - 10^22 range, so the Ling 2.0 family, with models nearing 10^26 should be seen as empirical validations of their own formalized scaling laws. 

The Ling 2.0 started of with the release of Ling-mini-2.0, a 16B A1.4B model. Following this, they released Ling-flash-2.0, with the same architecture, but scaled to 100B A6.1B. 

<img src="/images/lingflash.png" alt="Ling-flash-2.0" style="width: 50%; height: auto; display: block; margin: 2rem auto;" />

In the Flash announcement they mention the use of the IcePop to stabilize RL training. IcePop is a method to tackle the inference / training mismatch that occurs when using different backends for training and inference which leads to off-policy RL. This was first addressed back in early august in a blog post introducing truncated importance sampling, IcePop is a similar solution but instead proposes a masked importance sampling mechanism.

Now, just a few days ago, they released the next scale in the series, with Ling-1T featuring 1T parameters with 50B active parameters. This is an insane release, training a model of this size is no joke and its way beyond what their sister team at Qwen have published. Ling-1T pushes the boundaries of the MoE scaling laws, again building on the findings of the aforementioned paper. The Ling 2.0 MoE architecture is very standard, with focus being on highly efficient arch parameter choices in activation ratio, granularity, expert ratios etc. We discussed this in more [detail in the previous post](/blog/2025-09-20-ling2.md). 

<img src="/images/ling1t.png" alt="Ling 1T" style="width: 50%; height: auto; display: block; margin: 2rem auto;" />

The model is trained in FP8, which is almost unheard of at this scale, its really tough to train models of this scale at low precision so their is surely a great deal of engineering that they don't detail. They mention their WSM (Warmup–Stable–Merge) LR scheduler. Additionally the release post mentions a new RL algorithm coined LPO (Linguistics-Unit Policy Optimization), which apparently is a sentence level policy optimization, perhaps similar to GSPO (released by Qwen). 

**Ring**

Each 2.0 release so far has come in three flavors, a base model, a fine tuned model and a Ring version, which is the reasoning tuned version of the above Ling models. That is to say we have Ring-mini-2.0, Ring-flash-2.0 and Ring-1T. All we know about the reasoning training is that its some joint training of RLVR and RLHF, no more details are given.

Finally, the team has released Ring-flash-linear-2.0, which lacks a Ling counterpart, its only been released as a reasoning model. This model was apparently a continuation training of Ling-flash-2.0-base with the introduction of a linear attention variant. The model was trained for an additional 1T tokens after the arch change, and apparently it rivals the performance of Ring-flash-2.0.

<img src="/images/linglinearflash.png" alt="Ling Linear Flash" style="width: 50%; height: auto; display: block; margin: 2rem auto;" />

The model is a hybrid attention variant, alternating between linear attention and GQA, similar to what we've seen other achieve. The rest of the architecture seems unchanged from Ling-flash-2.0. With regards to the attention module we see that the only details we get is that the attention is a "Linear Attention Kernel". We also note that they introduce gated attention. My best guess would be that this is heavily inspired by the [Qwen3-Next architecture](https://huggingface.co/Qwen/Qwen3-Next-80B-A3B-Instruct) which was recently released with Gated DeltaNet in a hybrid variant that is very reminiscent of this. 

<img src="/images/qwennext.png" alt="Qwen3-Next" style="width: 50%; height: auto; display: block; margin: 2rem auto;" />

