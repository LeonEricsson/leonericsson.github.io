---
layout: post
title: "DeepSeek Sparse Attention"
categories: []
year: 2025
type: paper
author: DeepSeek-AI
exturl: https://github.com/deepseek-ai/DeepSeek-V3.2-Exp
---
DeepSeek Sparse Attention (DSA) augments an MLA-based transformer with a lightweight “lightning indexer” that scores the history and selects a small top-$k$ set of relevant past tokens for each new query. The main attention then operates only on that selected set. In DeepSeek’s V3.2-Exp, DSA is instantiated under MLA in an MQA configuration so each latent key–value entry is shared across heads; Figure 1 shows this layout at a glance.

MLA introduced a compressed KV cache, letting models harness full MHA during training while running MQA at inference. This delivers large KV-cache savings and preserves quality. Computation under MLA remains effectively equivalent to standard attention, but the attention term is still quadratic. As context lengths explode—especially in agentic settings—attention FLOPs dominate. 2025 has seen a wave of linear/sparse variants. DeepSeek previously explored NSA as a sparse mechanism; from what we can tell, stable training with NSA has been elusive. DSA is a step between MLA and NSA: a bridge that achieves stable sparse attention training. It keeps the indexer cheap (few heads, small head dimension, FP8) and narrows the main attention to $k$ survivors, yielding an overall $O(L\cdot k)$ pattern for the core attention term with $k \ll L$.

![](/images/dsacost.png)

We’ll anchor the rest of the post on the “DSA under MLA” diagram (Figure 1): the green path proposes candidates; the gray path runs attention on that reduced set. Next, we’ll recap MLA just enough to make the later steps obvious, then walk through DSA’s indexer and its integration, connecting the math to the code as we go.

![](/images/dsa.png)

The green path is the indexer producing scores and the “Top-$k$ Selector”; the gray path is the core attention that consumes only the selected latent KV entries inside MLA. Keep that mental picture: a fast, compact search stage that gates a standard attention stage.

### MLA

We enter the block with hidden states $x \in \mathbb{R}^{B \times S \times d_{\text{model}}}$. MLA factorizes queries and keys into two complementary channels: a **no-RoPE** (NoPE) latent channel and a **RoPE** channel that carries positional phase. Values live with the latent channel.

First, we produce the KV preactivations via the “A” projections, then split the KV path into a **latent KV** and a **RoPE key**

$$
\tilde{k}^{A} = xW^{KV}_{A} \in \mathbb{R}^{B \times S \times (d_C + d_{\text{RoPE}})}
$$
$$
\tilde{k}^{A} \Rightarrow
{c^{KV} \in \mathbb{R}^{B \times S \times d_C}}
\oplus
{k^{R} \in \mathbb{R}^{B \times S \times d_{\text{RoPE}}}}
$$

```python
# x: (B, S, d_model)
kvA = wkv_a(x)               # (B, S, d_C + d_RoPE)
cKV, kR = torch.split(kvA, [d_C, d_RoPE], dim=-1)  # (B,S,d_C), (B,S,d_RoPE)
```

Here, $d_C=\texttt{kv\_lora\_rank}$ is the latent KV rank and $d_{\text{RoPE}}=\texttt{qk\_rope\_head\_dim}$ is the head dimension of the decoupled RoPE key. The latent path is normalized and RoPE path gets positional information.

$$
c^{KV} \leftarrow \mathrm{RMSNorm}(c^{KV})
\qquad
k^{R} \leftarrow \mathrm{RoPE}(k^{R})
$$

```python
cKV = kv_norm(cKV)
kR = apply_rotary_emb(kR, freqs_cis)   # (B,S,d_RoPE)
```

These make up our cache, with exactly two per-token tensors: the compact content latent $c^{KV}$ and the decoupled positional key $k^{R}$

```python
kv_cache[:, start_pos:end_pos, :] = cKV  # (B, S, d_C)
pe_cache[:, start_pos:end_pos, :] = kR   # (B, S, d_RoPE)
```

---

On the query side, we apply a low-rank projection, followed by normalization, to retrieve compressed latents (later reused for lightning index)

$$
c^Q = \mathrm{RMSNorm}(xW^{Q}_{A}) \in \mathbb{R}^{B \times S \times d_Q}
$$

```python
# x: (B, S, d_model)
cQ  = q_norm(wq_a(x))        # (B, S, d_Q)
```

Here, $d_Q=\texttt{q\_lora\_rank}$ is the compressed query rank. We form per-head queries by a second “B” projection from $c^Q$, split into **no-RoPE** and **RoPE** subspaces. RoPE is applied only to the RoPE slice:

$$
\begin{aligned}
q = c^Q W^{Q}_{B}
\Rightarrow
\big(q^{A} \in \mathbb{R}^{B \times S \times H \times d_{\text{NoPE}}},
q^{R} \in \mathbb{R}^{B \times S \times H \times d_{\text{RoPE}}}\big),
\end{aligned}
$$

$$
q^{R} \leftarrow \mathrm{RoPE}(q^{R})
$$

```python
# project to heads, then split
q_full = wq_b(cQ).view(B, S, H, d_NoPE + d_RoPE)
qA, qR = torch.split(q_full, [d_NoPE, d_RoPE], dim=-1)  # (B,S,H,d_NoPE), (B,S,H,d_RoPE)
qR = apply_rotary_emb(qR, freqs_cis)      
```

Here, $d_{\text{NoPE}}=\texttt{qk\_nope\_head\_dim}$ is the query-key head dimension without positional information, together $\texttt{qk\_head\_dim}= \texttt{qk\_nope\_head\_dim} + \texttt{qk\_rope\_head\_dim}$ they make up the QK head dimension.

---

We want attention scores between queries $\langle q^{A}, q^{R} \rangle$ and keys $\langle c^{KV}, k^{R} \rangle$. A key component of MLA is to never materialize full per-head keys for the entire history. Instead, we keep a compact per-token latent $c^{KV}_t \in \mathbb{R}^{d_C}$ and a per-token RoPE key $k^R_t \in \mathbb{R}^{d_{\text{RoPE}}}$, and we transform the query once so it can score directly against those caches. 

Consider what a standard no-RoPE key would be for head $h$ at time $t$ if we did materialize it. Let $W_K \in \mathbb{R}^{d_{\text{NoPE}} \times d_C}$ denote the key block from the up-projection $W^{KV}_B$. The per-token latent is $c^{KV}_t \in \mathbb{R}^{d_C}$. The materialized key would be

$$
k^{\text{NoPE}}_{t} = W_Kc^{KV}_t \in \mathbb{R}^{d_{\text{NoPE}}}.
$$

The corresponding no-RoPE score contribution is

$$
\langle q^{C}, k^{\text{noPE}}_{t} \rangle
= q^{C\top} (W_K c^{KV}_t)
= (W_K^\top q^{C})^\top c^{KV}_t.
$$

This shows the algebraic trick: instead of forming $k^{\text{NoPE}}_{t}$ for all past tokens, map the query once

$$
\tilde{q} = W_K^\top q^{C} \in \mathbb{R}^{d_C}
$$

and then take a dot with each cached $c^{KV}_t$. In code this transformation comes across as a bit odd at first, but it's nothing more than a bit of reshaping. We reshape the up-projection into per-head blocks and use the first $d_{\text{NoPE}}$ rows as $W_K$, transposed inside an einsum:

```python
# wkv_b: view as (H, d_NoPE + d_V, d_C); take key rows per head
wkv_b = wkv_b_weight.view(H, d_NoPE + d_V, d_C)

# qA: (B, 1, H, d_NoPE)
q_lat = torch.einsum("bshd,hdc->bshc", qA, wkv_b[:, :d_NoPE])  # (B, 1, H, d_C) = W_K^T q^C
```

Scores are then the sum of two cache dot products: one in latent space using $\tilde{q}$ against $c^{KV}_{\le t}$, and one in the RoPE space using $q^R$ against $k^R_{\le t}$. Both produce $(B, 1, H, t)$ and are added before the softmax scale:

```python
scores = (
    torch.einsum("bshc,btc->bsht", q_lat, kv_cache[:, :t_end]) +  # latent: \tilde{q}^T c^{KV}_t
    torch.einsum("bshr,btr->bsht", qR,    pe_cache[:, :t_end])    # RoPE:  q^R · k^R_t
) * softmax_scale
```

Conceptually this is $\langle q^C, c^ {KV} \rangle + \langle q^R, k^R \rangle$, i.e., MLA’s decomposed $QK$. Following softmax, we aggregate **values in latent space** using the same $c^{KV}$ cache. This keeps the heavy sum in the compact dimension $d_C$:

```python
attn  = scores.softmax(dim=-1)                                   # (B, 1, H, t)
x_lat = torch.einsum("bsht,btc->bshc", attn, kv_cache[:, :t_end])  # (B, 1, H, d_C)
```

Finally, we expand from latent to the value head dimension $d_V$ with the value block of the same up-projection $W^{KV}_B$ and project back to model space:

```python
# value rows are the last d_V per head
x_head = torch.einsum("bshc,hdc->bshd", x_lat, wkv_b[:, -d_V:])  # (B, 1, H, d_V)
x_out  = wo(x_head.flatten(2))                                   # (B, 1, d_model)
```

Read this as two coordinated changes of basis: once on the query for scoring keys $W_K^\top q^C$ and once on the latent values after weighting $x_{\text{lat}} W_V$. This latent trick avoids ever constructing full per-head keys for the history while keeping the math identical to standard attention.

### DSA

MLA gives us compact per-token caches $c^{KV}_t \in \mathbb{R}^{d_C}$ and $k^R_t \in \mathbb{R}^{d_{\text{RoPE}}}$, and a decode path that avoids materializing full keys. What we haven’t changed is *how many* past tokens we touch at each step: all $t$ of them. The lightning indexer adds a fast, low-dimensional *search* stage that scores the entire history in a tiny head space and proposes a top-$k$ candidate set for the main attention. We can think of this as a learned k-NN over a compact metric space in FP8. 

The indexer builds its own compact space with $H_I = \texttt{index\_n\_heads}$ small heads of width $d_I \texttt{index\_head\_dim}$ that is smaller (e.g. 64 and 128) than core attention. The **query path is per-head**, the **key path is per-token**. Both share the same RoPE alignment.

Starting from the compressed query activations $c^Q \in \mathbb{R}^{B \times S \times d_Q}$ we already produced in MLA. Project to the indexer head space and split into NoPE and RoPE slices:

   $$
   q^{\text{idx}} = c^Q W^{Q,\text{idx}}_B \in \mathbb{R}^{B \times S \times H_I \times \big(d_{I,\text{NoPE}} + d_{I,\text{RoPE}}\big)}
   $$
Split and apply RoPE only to the RoPE slice:

   $$
   q^{\text{idx}}_{\text{NoPE}},\quad q^{\text{idx}}_{\text{RoPE}},\qquad
   \hat{q}^{\text{idx}}_{\text{RoPE}} = \mathrm{RoPE}\big(q^{\text{idx}}_{\text{RoPE}}\big)
   $$

Concatenate back to per-head $d_I = d_{I,\text{NoPE}} + d_{I,\text{RoPE}}$:

   $$
   q^{\text{mix}} = \mathrm{concat}\big(q^{\text{idx}}_{\text{NoPE}},\ \hat{q}^{\text{idx}}*{\text{RoPE}}\big)
   \in \mathbb{R}^{B \times S \times H_I \times d_I}
   $$

```python
q_idx = wq_b_index(cQ).view(B, S, H_I, dI_NoPE + dI_RoPE)   # (B,S,H_I,·)
qI_nope, qI_rope = torch.split(q_idx, [dI_NoPE, dI_RoPE], dim=-1)
qI_rope = apply_rotary_emb(qI_rope, freqs_cis)
q_mix = torch.cat([qI_nope, qI_rope], dim=-1)                # (B,S,H_I,d_I)
```

Decorrelate and balance features with a Walsh–Hadamard rotate
   $$
   q_{\text{rot}} = \mathrm{Hadamard}\big(q^{\text{mix}}\big)
   \in \mathbb{R}^{B \times S \times H_I \times d_I}
   $$

Think of this as producing head-wise search vectors that are well-conditioned for low-precision GEMMs. The same process is repeated for the keys, but we only use a single head; MQA style, producing

$$
\vdots
$$
   $$
   k_{\text{rot}} = \mathrm{Hadamard}\big(k^{\text{mix}}\big)
   \in \mathbb{R}^{B \times S \times d_I}
   $$


The indexer runs in FP8 for both query and key activations. Past keys are cached in FP8 blocks with per-block scales, so the search can read a dense $(\text{total\_tokens} \times d_I)$ matrix without dequantizing.

$$
(q_{\mathrm{fp8}}, s_q) = \mathrm{quant8}(q^{\text{rot}}), \qquad
(k_{\mathrm{fp8}}, s_k) = \mathrm{quant8}(k^{\text{rot}})
$$

```python
q_fp8, q_scale = act_quant(q_rot)         # (B,S,H_I,d_I), (B,S,H_I,1 or blocks)
k_fp8, k_scale = act_quant(k_rot)         # (B,S,d_I),     (B,S,blocks)
k_cache[:, start:end]      = k_fp8
k_scale_cache[:, start:end]= k_scale
```

For each new query at decode ($S{=}1$), the fused kernel runs a tiled FP8 GEMM over the indexer space, then applies a simple learned reduction across the indexer heads. It’s still $O(t \cdot H_I \cdot d_I)$ per step, but the constants are small (few heads, small head dim, FP8, tight memory access).

Each indexer head produces a nonnegative similarity between the current query and every cached key in the compact space; a learned per-head weighting combines them into one scalar “index score” per past token:

$$
\text{logits}_{h,t} = q_{\mathrm{fp8},h} \cdot k_{\mathrm{fp8},t}
$$
$$
\text{logits}^{+}_{h,t} = \max\big(0,\ \text{logits}_{h,t}\big)
$$
$$
\text{score}_{t} = \sum_{h=1}^{H_I} w_h\text{logits}^{+}_{h,t}
$$
$$
\text{index\_score}_{t} = \text{score}_{t} \cdot s_k(t)
$$

* $q_{\mathrm{fp8},h} \in \mathbb{R}^{d_I}$: FP8 query for indexer head $h$ (from $q_{\text{rot}}$).
* $k_{\mathrm{fp8},t} \in \mathbb{R}^{d_I}$: FP8 key for token $t$ (from $k_{\text{rot}}$).
* $\max(0,\cdot)$: ReLU that discards negative correlations, turning each head into a positive-similarity detector.
* $w_h$: learned scalar gate (derived from the current $x$) for head $h$.
* $s_k(t)$: per-block dequant scale attached to key $t$, restoring magnitude after FP8 math.

```python
# weights derived from x to combine heads cheaply inside the kernel
q_weights = head_weight_proj(x).view(B, S, H_I) * inv_sqrt(H_I)

# fused kernel:
# (B,S,H_I,d_I) @ (B,t,d_I) → (B,S,t), with ReLU, head weighting, and k_scales applied inside
index_score = fp8_index(
    q_fp8,                     # (B,S,H_I,d_I)
    q_weights,                 # (B,S,H_I)
    k_cache[:, :t_end],        # (B,t,d_I)    FP8
    k_scale_cache[:, :t_end],  # (B,t,blocks) per-block scales
)  # → (B,S,t)
```

We then select the top-$k$ tokens along time and pass those indices to the main attention. The indexer narrows what MLA sees to $k \ll t$: you still pay an $O(t)$ pass in the compact FP8 space, but you avoid touching all $t$ entries in the expensive attention space.

That’s the DSA loop at decode: build compact FP8 search vectors, score all past tokens quickly, keep the best $k$, and let MLA’s core attention operate only on those candidates.