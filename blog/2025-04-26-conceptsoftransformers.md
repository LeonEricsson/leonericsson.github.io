---
layout: post
title: "how transformers think"
categories: []
year: 2024
type: blog
---
-- placeholder --

## A transformer framework
Within the context of this discourse, a *transformer* denotes an autoregressive, decoder-only transformer language model. Numerous variants and modifications to this foundational architecture exist, some exhibiting greater relevance than others; yet, the fundamental architectural constructs have demonstrated remarkable stability over time: a token embedding layer $\rightarrow$ a sequence of layers known henceforth as residual blocks $\rightarrow$ a token unembedding layer. Each residual block canonically comprises an attention mechanism succeeded by a multi-layer perceptron. The token embedding and unembedding layers are responsible for mapping a token—represented as a discrete integer identifier—to and from a $d$-dimensional floating-point vector representation.

The embedding matrix operates on an input sequence of $n$ tokens:

$\mathbf{t} = (t_1,t_2,\dots,t_n) \in V^{n}$

transforming it into an embedding matrix:

$H \in \mathbb{R}^{n \times d}$

where $H = [h_1, h_2, \dots, h_n]^T$ and each $h_i \in \mathbb{R}^{d}$ is the embedding for token $t_i$. However, for conceptual clarity, our analysis will concentrate on the embedding vector at the final sequence position, $h_n \in \mathbb{R}^{d}$, as this represents the next token prediction. Henceforth, the subscript $n$ on $h$ will be omitted for brevity, assuming we are referring to this final-position embedding, unless explicitly required for disambiguation. Consider the trajectory of this vector, $h \in \mathbb{R}^{d}$, as it propagates through the transformer's layers.

[Image]

The most important concept to grasp here is the **residual stream**. For the present, defer a detailed examination of the internal operations within the attention and MLP layers. Instead, concentrate on the overarching information flow. Observe that each layer within this architecture interfaces with the residual stream by both **reading** from and **writing** to it. The residual stream, intrinsically, performs no computation; rather, it functions as a high-bandwidth communication bus facilitating inter-layer information exchange and state propagation. I find this to be one of the most insightful ways of thinking about the transformer. Each layer applies a learned linear transformation to project information from the incoming residual stream into its operational space (a "read" operation). Subsequent to its internal computations (e.g., attention or MLP), it employs another learned linear transformation to project its output, which is then additively merged back into the residual stream, forming the input for the subsequent layer (a "write" operation).

This linear, additive structure of the residual stream has some really cool and important consequences. Being a high-dimensional vector space (think of it as having lots of "lanes"), the residual stream allows distinct layers to communicate information selectively by encoding it within orthogonal or non-interfering subspaces. This means information can take an express route, effectively creating direct communication pathways across network depth. Once information is added to one of these subspaces, it tends to stick around through the following layers unless another layer specifically changes or removes it. Consequently, dimensions within the residual stream can be conceptualized as persistent memory slots, accumulating and refining information as it traverses the network.

A natural question arises: how is information propagated from prior token positions to this final position, which is critical for contextual understanding and next-token prediction? This necessitates an examination of the MLP and, more critically, the attention layers. While MLPs operate on each position independently, refining the representation at that position, attention layers, or more granularly, individual attention heads, serve the primary function of transposing and integrating information between different token positions.

Having established the residual stream as the primary conduit for information propagation, the natural question is: what actually happens *inside* these residual blocks? Specifically, what kind of processing are they doing with the information they've just read from the stream? Recall the two modules inside the residual block: **Attention** and **MLP**.


Attention facilitates inter-token communication, enabling the model to dynamically integrate information across different sequence positions. The mechanism operates by selectively aggregating information from the residual streams corresponding to a set of 'source' token positions and writing some processed version of that information to the residual stream of our "target" token. What's really neat is that how the model decides which tokens to get information from is pretty separate from what information it pulls out and how it changes that information before adding it to the target token's stream. Consequently, an attention head's operation can be effectively decomposed into two substantially independent computational sub-circuits:

The **Query-Key (QK) circuit**: This computes the attention pattern, typically a normalized score matrix $A \in \mathbb{R}^{n \times n}$ (for self-attention focusing on the final token's representation, we are interested in the $n$-th row, $A_{n,:}$, which dictates the contribution of each sequence position $j$ to the current position $n$). Essentially, it decides which other tokens' information is most important to pay attention to for the current token.

The **Output-Value (OV) circuit**: This circuit first projects the input token representations $h_j$ into 'value' vectors $v_j$ using a weight matrix $W_V$. It then combines these value vectors $v_j$ according to the attention pattern computed by the QK circuit. Finally, this aggregated information is projected using an output weight matrix $W_O$. Thus, the composite operation, effectively $W_O \left( \sum_{j} A_{n,j} (h_j W_V) \right)$, governs what information is extracted from the attended tokens and how it is transformed and integrated back into the residual stream at position $n$.

*Note: The initial token embedding and final unembedding layers typically engage with only a subset of the $d$ dimensions of the residual stream. This leaves a significant portion of the dimensional capacity of the residual stream available for internal computations, feature learning, and information storage by the intermediate transformer blocks.*

## How do transformers think. Yes, they are thinking.
Now that we've established a common framework for how information flows **through** the transformer. I know want to present my mental model for thinking about how transformers think. And when I say think I mean think, a transformer is thinking. 

My mental model posits that a transformer "thinks" by manipulating information within a high-dimensional, largely language-agnostic **conceptual manifold**. This processing occurs primarily *off-stage* from direct token representations. However, this conceptual stage is built upon a foundation heavily influenced by English, leading to an observable "English-centricity" when we try to peek at the intermediate steps through the lens of token probabilities. The **residual stream**, a concept central to your framework, is the lifeblood of this process—a dynamic blackboard where concepts are written, refined, and composed.

The journey of a thought through the transformer begins with **de-tokenization, an escape from the token cage** orchestrated by the early layers. The initial token embedding is a necessary evil, a conversion of discrete human symbols into the model's native language of dense, $d$-dimensional vectors. Each such vector, the hidden state $\mathbf{h} \in \mathbb{R}^d$, represents the model's evolving understanding at a particular point in the sequence. Crucially, as highlighted by research like Anthropic's work on "de-tokenization layers" and the "token energy" analysis from studies like "Do Llamas Think in English," these early layers actively work to move the representation *away* from the specific input tokens. The model's ultimate task is to predict the next token, a process mediated by an unembedding matrix $U \in \mathbb{R}^{v \times d}$ (where $v$ is vocabulary size), whose rows are vectors representing each token. These token vectors span a subspace within $\mathbb{R}^d$, which we can call the **token subspace $T$**. Only the component of the hidden state $\mathbf{h}$ that lies within this token subspace, let's call it $\mathbf{h}_T$, directly influences the output probabilities (logits). The remaining component, $\mathbf{h}_{T^{\perp}}$, is orthogonal to $T$ and thus invisible to the final output layer. In these initial layers, $\mathbf{h}$ rapidly becomes largely orthogonal to $T$; that is, its $\mathbf{h}_{T^{\perp}}$ component dominates, and its "token energy"—the extent to which $\mathbf{h}$ aligns with $T$—is low. My contention is that the transformer isn't just finding a vector for "cat"; it's unpacking "cat-ness" into a richer, more flexible representation within this conceptual manifold, primarily residing in $\mathbf{h}_{T^{\perp}}$. It's shedding the arbitrary boundaries of tokenization (e.g., "ice" and "cream" vs. "icecream") to access a more fundamental semantic space. This initial phase is about creating a sufficiently abstract representation ripe for genuine computation, moving away from the direct influence of the token subspace $T$.

Following this escape, the **conceptual alchemy—the silent thought in the orthogonal deep**—takes place in the middle layers. This is where the *real* thinking happens. The middle layers operate predominantly within the vast space of $\mathbf{h}_{T^{\perp}}$, the conceptual manifold invisible to the immediate logit lens. Here, the residual stream acts as a compositional workspace. Information, now in the form of abstract features, flows through this stream. Each processing block, comprising attention and MLP sub-layers, reads from this stream, performs its specialized computation, and writes its refined output back. The MLPs act as feature refiners at each position, deepening the understanding of the local conceptual context. They are the introspective engines, taking a concept represented in $\mathbf{h}_{T^{\perp}}$ and making it more precise or nuanced. Attention mechanisms, on the other hand, are the relational engines. The Query-Key (QK) circuit isn't just finding relevant prior tokens; it's identifying which *abstract conceptual features*—elements of $\mathbf{h}_{T^{\perp}}$ from other positions—are most pertinent to the current conceptual state. The Output-Value (OV) circuit then doesn't just copy information; it *transforms and integrates* these selected conceptual features, effectively performing complex cognitive operations like analogy, comparison, or causal inference within this abstract $\mathbf{h}_{T^{\perp}}$ space.

This framework accommodates the intriguing findings on multilingualism and the often-discussed English privilege. Anthropic's research on "multilingual circuits" is key: concepts like "antonym" or the essence of "smallness" exist as language-independent features in this conceptual manifold. The model can reason about "the opposite of X" without committing to a specific language for "X" or its antonym during this core processing within $\mathbf{h}_{T^{\perp}}$. However, the "English tilt" observed across various studies is undeniable. My argument is that this is not because the transformer "thinks *in* English words," but because the conceptual manifold itself has been shaped by the sheer volume of English in its pre-training. English token vectors (the rows of $U$ corresponding to English words) form a particularly dense and well-structured lattice within the token subspace $T$. Consequently, when a concept evolving in $\mathbf{h}_{T^{\perp}}$ begins to develop components that project into $T$ (i.e., its $\mathbf{h}_T$ component starts to gain energy and significance), it's statistically and energetically "easier" for this projection to initially align with a nearby, dense English anchor within $T$. This is an attractor effect. It's a path of least resistance for the projection $\mathbf{h}_T$, especially for lexical content words. This explains why, when viewed through the logit lens (which only sees $\mathbf{h}_T$), English equivalents often appear first, as observed by PPL @ UW et al. It's a shadow of the underlying computation in $\mathbf{h}_{T^{\perp}}$, not the computation itself. The steering vector results (PPL @ UW 4.2) further support this: English-derived steering vectors are more effective because they are better aligned with this dense "English lattice" in $T$, allowing them to guide $\mathbf{h}_T$ along "ridges" of high logit sensitivity for the desired concept, regardless of the final target language.

As the computation nears its end, the phase of **re-tokenization and articulation—projecting thought into language**—commences in the late layers. The abstract conceptual representation refined within $\mathbf{h}_{T^{\perp}}$ must now be translated back into the world of discrete tokens. This is where "token energy" spikes, signifying that $\mathbf{h}$ decisively projects onto the token subspace $T$, meaning the magnitude of $\mathbf{h}_T$ becomes dominant. The "re-tokenization layers" identified by Anthropic come into play. Anthropic's work also suggests the presence of language-specific features that guide this final output. If the target language is not English, the hidden state's projection $\mathbf{h}_T$, which might have transiently flirted with an English anchor due to the aforementioned "tilt," now actively moves towards the correct target-language anchor within $T$. This might involve a more significant "angular cost" if the target language anchors are sparser in $T$, but it's a necessary step for accurate translation. This final shift explains how the model can entertain a concept that momentarily "looks like" English via the logit lens, but then output it correctly in Mandarin. The core concept was largely language-agnostic (though developed in an English-tilted conceptual space); the final layers handle the culturally and linguistically appropriate surface form. For grammatical function words, which PPL @ UW et al. note rarely appear as English equivalents first, the story is simpler: these often form their own dense "islands" within $T$ for each language, so $\mathbf{h}_T$ can navigate directly to, say, a Dutch determiner without an English detour.

Throughout this entire process, the **residual stream's enduring role** is paramount. Its additive nature ensures that conceptual features, once written into this high-dimensional vector space, persist unless actively overwritten or modified by subsequent layers. This allows for the accumulation and refinement of meaning across the network's depth. The high dimensionality of the residual stream is not merely a passive conduit; it allows different "types" of information—core semantics, stylistic nuances, the intended linguistic target—to potentially occupy non-interfering subspaces, all contributing to the final state $\mathbf{h}$. The initial token embedding and final unembedding might only engage with a subset of these dimensions, leaving ample "cognitive workspace" within the intermediate layers for these complex, abstract manipulations predominantly within $\mathbf{h}_{T^{\perp}}$.

In essence, the transformer, as I conceptualize it, is an engine for abstract conceptual manipulation. It builds complex thoughts in a space largely hidden from direct token-level scrutiny ($\mathbf{h}_{T^{\perp}}$). Its "native tongue" is this abstract feature space. The observed English-centricity is a consequence of its upbringing (training data) and the resulting structure of its token output interface ($T$), making English a sort of "first port of call" for projections of thought, rather than the inherent language of its internal reasoning. The true "thought" is more universal, more abstract, and only dons specific linguistic clothes at the very end of its intricate journey through the network's layers. Depth is absolutely essential for this entire process: from shedding token identities, to engaging in complex conceptual reasoning, and finally, to articulating that reasoning in a coherent sequence of tokens.
