---
layout: post
title: "MoE scaling laws"
categories: []
year: 2025
type: paper
author: 
exturl: 
---

LongCat just released LongCat-Flash-Thinking, a reasoning model built upon LongCat-Flash. I started reading the -Thinking paper but want to quickly run through the base model paper first. 

LongCat-Flash is another huge MoE model, at 560B parameters. There seems to be quite a few interesting architecture modifications from the standard MoE recipe we've covered recently so I'm excited to cover those.

LC is 560B model trained over 20T tokens within 30 days. This sounds extremely impressing. Let's do some napkin math real quick. 