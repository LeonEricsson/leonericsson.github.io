---
layout: post
title: "dl on first principles"
categories: []
year: 2024
type: paper
---

most people are resource restrained, and even those who aren't still need to utilize their resources effectively. optimizing deep learning systems is therefor crucial. to do this, we need to understand the kinds of constraints that our system may suffer under.

typically the accelerator of choice here is a GPU, and i'll use GPU as an all encompassing word for accelerators LPU, TPU etc. on a GPU, performing deep learning, you're time is split between three components

1. compute - the time spent performing operations. FLOPS.
2. memory - the time spent moving data (tensors) between different layers of the GPU.
3. overhead - everything else

and understanding which of these you're capped in is going to make optimizing your system both easier and more effective. you can improve the speed of your search algorithm all you want, if you're in the *memory-bound* regime, the end result is going to be a lackluster performance improvement. so, if you want to improve the performance of any system, you'll need to spend time understanding these regimes, how they work, and when you're in them.

### compute

compute is the most important regime to utilize effectively, why? because it's the fastest, and you have the most of it. so how do we maximize compute? well, it's usually just spending less time doing other stuff. 

think of your system like a kitchen, your chef (compute) uses ingredients to prepare a dish, and he does so with incredible speed. your line-cook, supplies the chef with ingredients. if your upgrade your chef such that he now works twice as fast, won't matter if your ingredients are still coming out at the same pace as before.

gpus really want to to just burn through matmuls, they've even got specialized compute cores just for that reason. so the more you get them doing matmuls, the better. the reason for this is simple, other operations just dont make up enough of the total time to be worth optimizing for. 

### bandwidth

bandwidth is the cost of moving data from A to B. movement is hierarchical, one can move data between the CPU and the GPU, between two nodes, or from disk to CPU, these are larger movements, but there is also movement of data within the GPU. when you write nvidia-smi in your console, or when you're complaining about a OOM message from your CUDA, you're talking about DRAM. DRAM is the large container(s) that a line-cook will fill with chopped ingredients, ready to be transferred into the much smaller, and much faster SRAM. The cost of this data movement is what's called memory bandwidth cost. Every time you perform a operation, known as a GPU kernel, you need to move your data from DRAM to SRAM. 

if the movement bandwidth cost is higher than the cost of executing the kernel, then your operation is memory-bound. examples of this are unary operations like torch.cos and torch.log. such operations scattered inside your code increase memory bandwidth and decrease compute utilization, what can you do about it? 

fusing unary operations, or unary operations with many other operations for that matter, reduces memory bandwidth, and allows the data to remain in SRAM while multiple operations are performed in sequence, as a single "fused" operation. this is why writing CUDA kernels is so useful, you can't avoid this memory movement without a specific CUDA kernel written for the operations you want to fuse. gpu compilers such as XLA will try to fuse as much as possible, but automated fusing isn't as good (yet) as a competent human.
