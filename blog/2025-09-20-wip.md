---
layout: post
title: "From f(x) and g(x) to f(g(x))"
categories: []
year: 2025
type: paper
author: 
exturl: 
---


##### MoE glossary

**Total and Active Parameters**. In MoE models, we distinguish between the total parameters of the model and the number of parameters active in the forward pass, due to the top-k gating mechanism of experts.

**Routable and Sharable Experts**. An MoE layer typically has a total number of routable experts $E$, from which the gating network selects a subset of $E_a$ (num activated experts) for each token. Additionally, many MoE architectures incorporate $E_s$ shared experts which are experts every token is routed through. These shared experts free the routable experts from having to encode overlapping features. 

**Activation Ratio and Sharing Ratio**. There are two metrics that characterize the expert configuration. *Activation ratio*, $A$, is the ratio of activate experts to the total number of experts $A = (E_a + E_s) / (E + E_s)$, and sharing ratio is the ratio of shared experts to activated experts $S = E_s / (E_a + E_s)$. Together these metrics define the sparsity of our MoE layer.

**Granularity of Experts**. Prior to DeepSeek, it was comon practice for the intermediate dimension of each expert to be equal to the FFN hidden dimension, which was typically around 2~4 x d_model. However, now adays we commonly see these two dimensions decoupled from each other, meaning we have a expert dimension and a FFN dimension. To analyze this design choice, the authors define expert granularity as $G = 2d_\text{model} / d_\text{expert}$. A higher value of G corresponds to having a larger number of smaller experts for a fixed total parameter count within the MoE layers.

**Defining Model Scale via Computation**. This work quantifies the computational cost through FLOPs. A model's scale, denoted $M$, represents the number of non-embedding FLOPs per token in a single forward pass. For MoE models this is especially important because $M$ only depends on the *activated* parameters of the architecture, i.e the selected experts. Total training compute is defined as $C = M \cdot D$. 


#### Scaling laws forHparams and Data allocation
The paper attempts to establish scaling laws for batch size and learning rate as a function of model compute $C$. To achieve this they perform a hyperparameter search across the compute range of 3e17 to 3e20. The MoE configuration used for these experiments is a arch with $E = 64, E_a = 4, E_s = 1$ resulting in an activation ratio $A = 7.8\%$ and granularity $G = 2$.  The data points reveal the following formulas following a fitting process:

$$
\begin{align}
n^\text{opt} &= 1.1576 \cdot C^{-0.1529} \\
B^\text{opt} &= 0.0694 \cdot C^{0.3644}
\end{align}
$$


As mentioned, these laws are derived from a single configuration of the MoE layer, so we need to verify the generalization of these laws to other archs, specifically ones with alternative activation ratios. To test this, the derived laws are used to predict optimal hyperparameters at a compute budget of 3e20 FLOPs, after fitting them on data up to 1e20 FLOPs. Performing these experiments they find that the predicted optimal regions effectively capture the best-performing hyperparameters for activation ratos from 4.7% to 10.9% demonstrating that these laws can be applied to MoE models within this range. 

Kimi K2 2.3%
DeepSeek V3.1 3.5%
Qwen3 235B  / 30B  6.25%
GLM 4.5 5.59%