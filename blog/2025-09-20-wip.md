---
layout: post
title: "MoE scaling laws"
categories: []
year: 2025
type: paper
author: 
exturl: 
---

The Ling Team is a team withing inclusionAI, inclusionAI is an organization that contain oss projects from Ant Group. Ant Group is a Chinese fintech company best known for Alipay. Essentially this is another research team from Alibaba dedicated to AGI. From their own github page they say *This organization contains the series of open-source projects from Ant Group with dedicated efforts to work towards Artificial General Intelligence (AGI).* This team has been making moves in the dark.


##### MoE glossary

**Total and Active Parameters**. In MoE models, we distinguish between the total parameters of the model and the number of parameters active in the forward pass, due to the top-k gating mechanism of experts.

**Routable and Sharable Experts**. An MoE layer typically has a total number of routable experts $E$, from which the gating network selects a subset of $E_a$ (num activated experts) for each token. Additionally, many MoE architectures incorporate $E_s$ shared experts which are experts every token is routed through. These shared experts free the routable experts from having to encode overlapping features. 

**Activation Ratio and Sharing Ratio**. There are two metrics that characterize the expert configuration. *Activation ratio*, $A$, is the ratio of activate experts to the total number of experts $A = (E_a + E_s) / (E + E_s)$, and sharing ratio is the ratio of shared experts to activated experts $S = E_s / (E_a + E_s)$. Together these metrics define the sparsity of our MoE layer.

**Granularity of Experts**. Prior to DeepSeek, it was comon practice for the intermediate dimension of each expert to be equal to the FFN hidden dimension, which was typically around 2~4 x d_model. However, now adays we commonly see these two dimensions decoupled from each other, meaning we have a expert dimension and a FFN dimension. To analyze this design choice, the authors define expert granularity as $G = 2d_\text{model} / d_\text{expert}$. A higher value of G corresponds to having a larger number of smaller experts for a fixed total parameter count within the MoE layers.

**Defining Model Scale via Computation**. This work quantifies the computational cost through FLOPs. A model's scale, denoted $M$, represents the number of non-embedding FLOPs per token in a single forward pass. For MoE models this is especially important because $M$ only depends on the *activated* parameters of the architecture, i.e the selected experts. Total training compute is defined as $C = M \cdot D$. 


#### Scaling laws for Hparams and Data allocation
The paper attempts to establish scaling laws for batch size and learning rate as a function of model compute $C$. To achieve this they perform a hyperparameter search across the compute range of 3e17 to 3e20. The MoE configuration used for these experiments is a arch with $E = 64, E_a = 4, E_s = 1$ resulting in an activation ratio $A = 7.8\%$ and granularity $G = 2$.  The data points reveal the following formulas following a fitting process:

$$
\begin{align}
n^\text{opt} &= 1.1576 \cdot C^{-0.1529} \\
B^\text{opt} &= 0.0694 \cdot C^{0.3644}
\end{align}
$$

As mentioned, these laws are derived from a single configuration of the MoE layer, so we need to verify the generalization of these laws to other archs, specifically ones with alternative activation ratios. To test this, the derived laws are used to predict optimal hyperparameters at a compute budget of 3e20 FLOPs, after fitting them on data up to 1e20 FLOPs. Performing these experiments they find that the predicted optimal regions effectively capture the best-performing hyperparameters for activation ratos from 4.7% to 10.9% demonstrating that these laws can be applied to MoE models within this range. For reference a bunch of activation ratio for recent MoE models:

| Model | Activation Ratio |
| --- | --- |
|  Kimi K2 | 2.3%    |
|  Deepseek V3.1 | 3.5    |
|  Qwen 3 235B | 6.25%   |
|  Qwen 3 30B | 6.25%   |
|  GLM 4.5 | 5.59%    |

With this, we now know, given a certain compute budget, what batch size and learning rate will yield the best model. I appreciate the validation of the hparams but its only done at $C=3e20$, would of like to see a larger range of activation ratios. Seems slightly optimistic to claim universality of these laws, and invariance "across activation ratios and budgets".

What about model / data allocation? The classic scaling law papers tell us, for dense models, how, given a fixed compute budget, we should allocate compute between data and model. 

$$(M^{\text{opt}}, D^{\text{opt}}) = \arg\min_{M,D} \; \mathcal{L}(M,D; C, A, G, S) \quad \text{s.t.} \quad C = M \cdot D$$

The derived scaling laws for MoE models 

$$
\begin{align}
M^\text{opt} &= 0.1915 \cdot C^{0.5095} \\
D^\text{opt} &= 5.2232 \cdot C^{0.4905}
\end{align}
$$

are fairly similar to those for dense models

$$
\begin{align}
M^\text{opt} &= 0.0655 \cdot C^{0.5422} \\
D^\text{opt} &= 15.2582 \cdot C^{0.4578}
\end{align}
$$

with optimal MoE models being slighly smaller (lower $M^\text{opt}$) but trained on more data than their dense counterpart. MoE models, in line with previous scaling law studies, have coefficients close to 0.5, meaning that additional compute should be divided evenly between increasing model size and adding more data. Again, the model-data allocation experiments are derived from a single fixed MoE architecture (activation ratio $A= 7.8%$, granularity $G=2$) and this time we don't even have validation across alternate archs. The papers whole premise is that MoE architecture (defined in part by $A$, $G$) determines a model's "effective capacity". It then follows that the optimal trade off between model size and data size should also depend on this capacity.  By deriving the $M^{opt}$ and $D^{opt}$ from a single architecture, we are biased. When this single scaling law is used to set up experiments in Section 4, architectures that differ significantly from the reference configuration ($A=7.8\%$, $G=2$) may not be in their compute-optimal regime. I don't want to be too harsh, because I understand that these experiments are expensive. 

#### Efficiency Leverage
is defined to quantify the computational efficiency gain of MoE compared to dense models. Using loss as our measure of performance, we can imagine that the required compute $C$, to achieve a certain loss as the most straightforward measurement of efficiency. Let $\mathcal{X}_{\text{Dense}}$ denote a standard dense architecture, and $\mathcal{X}_{\text{MoE}}$ represent a MoE architecutre. Models within $\mathcal{X}_{\text{MoE}}$ share the same activation ratio, granularity, shared experts and are scaled solely through increased hidden dimensions ($d_\text{model}$, $d_\text{ffn}$, $d_\text{expert}$) and number of layers. Then, what compute is required to get to a specific loss under these two model architectures $\mathcal{L}(C_{\text{dense}}; \mathcal{X}_{\text{Dense}})$, $\mathcal{L}(C_{\text{moe}}; \mathcal{X}_{\text{MoE}})$; this will give us a measurement of efficiency. Formally, efficiency leverage is defined as:

$$
EL(\mathcal{X}_{\text{MoE}} \mid \mathcal{X}_{\text{Dense}}; C_{\text{target}})
= \frac{C_{\text{dense}}}{C_{\text{moe}}},
\quad
\text{s.t.} \quad
\big| \mathcal{L}(C_{\text{moe}}; \mathcal{X}_{\text{MoE}})
- \mathcal{L}(C_{\text{dense}}; \mathcal{X}_{\text{Dense}}) \big|
\leq \epsilon
\quad (\epsilon \to 0)
$$

where a EL greater than 1 signifies that the MoE architecture is more computationally efficient than its dense counterpart.

With EL defined, we now move onto the core of the study, looking at how different architectural configurations of an MoE model effects its EL. Specifically, how do different activation ratios, expert granularity, and shared expert ratio effect the EL, with the goal of finding the A, G, S that maximize EL for a given compute budget. Experiment are conducted as follows, starting from a baseline MoE architecture (2 of 64 experts activated, one shared expert) vary one dimension at a time across a range of compute budgets. From the results in the previous section we've already concluded the optimal batch size, learning rate, model and data allocation given a compute budget, which means that we can scan across different activation ratios, granularities and shared expert ratios, while holding other factors and the model scale M constant, and know the setup to train said model architecture at near its peak potential for a given budget, yielding reliable and cost effective conclusions. Since we've already derived scaling laws for dense models, it is easy to calculate the EL of each MoE configuration.

##### Activation Ratio
Experiments reveal that, at a fixed compute budget, loss monotonically decreases with activation ratio. For all configurations tested, the loss at ratio of 0.8% consistently yields the minimum loss. Now, remember that as the activation ratio decreases, we compensate for this by increasing the hidden dimensions d_model, d_ffn, d_expert. So, we want to trade MoE sparisty for increased hidden dimension. If we fit loss scaling curves to the experiment data points, we can compute EL at different activation ratios and FLOPs budgets. We find that EL consistently increases as the activation ratio decreases, and that EL grows with compute budget demonstrating that MoE advantage is amplified at larger scales.

![](/images/moeascale.png)

##### Granularity
Remember that granularity is defined as $G = 2d_\text{model} / d_\text{expert}$. We need to hold model size and activation ratio constant. To achieve this we can increase the number of experts (and by extension the number of active experts, to keep A constant), while proportionally decreasing the size of each expert. This will keep FLOPs constant while giving us a range of granularity to experiment with, specifically the granularity varies from 2 to 16. For reference, these are the granularites of modern MoEs:

| Model | Granularity |
| --- | --- |
|  Kimi K2 | 7   |
|  DeepSeek V3.1 | 7   |
|  Qwen 3 235B | 5.33   |
|  Qwen 3 30B | 5.33   |
|  GLM 4.5 | 6.67    |



