---
layout: post
title: "MoE scaling laws"
categories: []
year: 2025
type: paper
author: 
exturl: 
---

LongCat just released LongCat-Flash-Thinking, a reasoning model built upon LongCat-Flash. I started reading the -Thinking paper but want to quickly run through the base model paper first. 

LongCat-Flash is another huge MoE model, at 560B parameters. There seems to be quite a few interesting architecture modifications from the standard MoE recipe we've covered recently so I'm excited to cover those.

LC is 560B model trained over 20T tokens within 30 days. This sounds extremely impressing. Let's do some napkin math real quick. 

The model is 560B params, train on over 20T tokens within 30 days. This sounds extremely impressive. Let's do some quick napkin math, I've already seen that the arch is somewhat non standard so this will be slightly off but it will give us a rough idea. 

LongCat uses MLA which behaves like MHA during training. Ignoring the attention FLOPs, which are small at normal training contexts, the attention module consists of 4 projections (QKVO) totaling 24BSDNH FLOPs. Assuming all layers use MoE (albeit it is standard to use dense layers for the first 1-2 layers), each MoE activating E experts with size F requires 6BSDEF. For all layers plus embedding FLOPs we get a total of L(24BSDNH + 18BSDEF) + 2BSDV = 6BS * (4DNH + 3DEF) = 6 * num tokens * num active parameters FLOPs. This derives the famous rule of thumb that the number of FLOPs for a transformer (ignore attention FLOPs) is 6 * num tokens * num active parameters. Because LongCat-Flash has a dynamic forward pass compute, determining the total FLOPs used during training is less straight forward but they clearly state that they activate 27B on average so using that we get a total of 3.24e24 training FLOPs! Given that this is a chinese lab and that they specifically mention H800 for their inference stack we'll assume that's what they used for training. The H800 does 756.45 BF16 TFLOPs, so at a generous 50% MFU it would take $6*27e9*20e12 / (756e12 * 60 * 60 * 24 * 30) = 1653$ GPUs to finish in 30 days. With the reported "tens of thousands of GPUs" they could theoretically train in a matter of days. But still, an almost unknown lab doing this kind of a training run, at this scale is very impressive. And that's without even going into the new architecture they've introduced.



