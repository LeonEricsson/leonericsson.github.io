---
layout: post
title: "Reading Roundup"
categories: []
year: 2023
type: post
---

It's been a while, a lot going on right now and I've been coding more than reading recently. Given that it's been some time it felt appropriate to come back with a reading roundup of some things I've read recently that deserve both your and my attention. 

## World Model on Million-Length Video and Language with RingAttention

I've dabbled with in-context retrieval tests so when I saw this I figured I had to read the paper

![](/images/lwmneedle.png)

This paper explores building a long-context world model through extended context and multimodal training. The hypothesis is that current models lack in understanding of certain aspects of the world which are difficult to capture in texts. Thus, training on temporal structures in video sequences will help provide this missing information; long language sequences encode information that short sequence equivalents can not. To achieve this goal of learning from video sequences, the authors need a model capable of processing more than a million tokens per sequence and train it on a very large dataset. Their training is layed out in two stages: 

**Stage 1**. The authors begin with the staple Llama 7B, which has a context window of 4096. Expanding context in base models requires addressing the challenges of extrapolating beyond trained sequence lengths (positional O.O.D) and the quadratic complexity of attention mechanisms. The prohibitive memory demands of long-document training due to attention were mitigated by integrating RingAttention with FlashAttention. Furthermore, to circumvent the computational complexity, the training progressively increased context sizes from 32K to 1M tokens, rather than starting directly at 1M tokens. Additionally, the authors employed RoPE base period scaling to tackle positional O.O.D in encodings, a technique first introduced in the CodeLLama paper.

**Stage 2**. This stage involved further fine-tuning of LWM 1M on joint video and language sequences. Input images of size $256 \times 256$ are tokenized into $16 \times 16$ discrete tokens. Videos are tokenized per frame and modalities are differentiated with special tokens. Training mirrored the progressive approach of Stage 1, starting with 1k context and gradually advancing to 1M tokens, excluding the RoPE base frequency scaling already applied in the first stage.

## EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models
In recent weeks, I have been highly enthusiastic about EQ-Bench, a benchmark developed by an independent researcher that exemplifies solid research deserving recognition. EQ-Bench proposes that emotional intelligence serves as an effective measure of general intelligence. Traditional methods for assessing human emotional understanding are challenging to adapt for Large Language Models (LLMs), leading to a gap in objective evaluation metrics. EQ-Bench addresses this by presenting scenarios with emotional dynamics and asking LLMs to assess the emotional intensity experienced by characters in these scenarios. This approach provides a nuanced understanding without necessitating subjective interpretations by evaluators, thereby avoiding the need for human intervention or LLM-based judging.

The benchmark involves GPT-4 generating 70 scenarios, with models assessing emotions based on a predefined template, rating emotional intensities on a scale from 0 to 10. The reference answers were established by the test creators to prevent model bias.

```
At the end of this dialogue, Jane would feel:
    Surprised:
    Confused:
    Angry:
    Forgiving:
```

What stands out about EQ-Bench is its efficiency and accessibility, especially when compared to benchmarks like MT-Bench or AlpacaEval. With only 70 questions (extended to 170 in EQ Bench v2), it simplifies the evaluation process and eliminates the need for high-cost judge models like GPT-4. This accessibility is particularly beneficial for the open-source software community. To illustrate the benchmark's relevance, I have conducted correlation analyses between EQ-Bench and other leading evaluation methods.

![](/images/eqbenchcorr.png)



