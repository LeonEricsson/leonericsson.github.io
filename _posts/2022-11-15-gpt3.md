---
layout: post
title: Language Models are Few-Shot Learners
categories: [NLP]
---

Authors present the GPT-3 model, a language model that uses 175B parameters and is trained on 300 billion tokens drawn from unsupervised web data drawn from a variety of datasets. The authors address the problems, need for large specific datasets and task-specific fine tuning, of recent models which are pre-trained on large datasets and directly fine-tuned on the downstream task by proposing a model that is task agnostic and able to meta-learn way better than anything else. During unsupervised training, the model develops a broad set of skills and pattern recognition abilities which it then uses at inference time to adapt and recognize desired tasks (referred to as “in-context learning”). The authors show that the model presents remarkable ability to zero-shot, one-shot and few-shot tasks described through prompting even outperforming fine-tuned models on some datasets.  

Original [paper](https://arxiv.org/abs/2005.14165) (2020)
