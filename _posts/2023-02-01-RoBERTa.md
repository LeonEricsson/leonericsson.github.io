---
layout: post
title: "RoBERTa: A Robustly Optimized BERT Pretraining Approach"
categories: [NLP, Transformer]
year: 2019
type: paper
author: Liu
exturl: https://arxiv.org/pdf/1907.11692.pdf
---

A lot of papers surrounding BERT and it's pretraining objective were released in 2019. This paper is especially alluring as it is a replication study of BERT pretraining that carefully tries to measure the impact of certain hyperparameters ultimately re-establishing that BERT's masked language model training objective is competitive with other recently proposed objectives such as XLNet and XLM. Interestingly, the authors of XLNet actually revised their paper following this publication introducing a version of XLNet that outperforms RoBERTA, but more on that in a later blog post. Anyway, at the time the authors found that BERT was significantly undertrained and hence proposed an improved recipe for training BERT models which they call RoBERTa. A model that matches or even exceeds post-BERT methods. So
