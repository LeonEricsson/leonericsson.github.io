---
layout: post
title: "Llama 2: Open Foundation and Fine-Tuned Chat Models"
categories: [NLP, Transformer]
year: 2023
type: paper
author: Touvron
exturl: https://arxiv.org/pdf/2307.09288.pdf
---
Meta keeps on giving to the open-source community, this time around with a family of natural successor to Llama 1, and a set of open products that can viably compete with dialog style LLMs such as ChatGPT, BARD and Claude. Llama 2 is a family of pre-trained LLMs of the same size and with analogous training recipe to Llama 1. Llama 2-Chat is a completely new model family consisting of instruction fine-tuned versions of Llama 2, optimized for dialogue use cases. I've been excited about this paper for a long time so I'm glad we're finally here in this research journey.

While this paper is extensive, Meta is very generous in the detail they provide surrounding both their pretraining and subsequent fine-tuning.

## Pretraining
As mentioned, Llama 2 is a family of pretrained models created using a slightly updated Llama 1 approach. The training data has been extended to include new publicly available sources (outside of Meta's products and services) and now totals over 2 trillion tokens (up from 1T). The context window is expanded from 2048 to 4096 - this *was* equal to that of ChatGPT before they upgraded to 8k. Architecturally, the only unfamiliar 

