---
layout: post
title: "Oriol Vinyals: Deep learning and AGI | Lex Fridman"
categories: [Deep Learning]
year: 2022
---

Thoughtful discussion between one of the leading minds at DeepMind, talks about GPT and Gato. Key takeaways: 
- Currently machine learning involves defining a structure specific for a problem and generating a new network trained for this problem. There should be a way for networks to evolve similar to how humans have evolved over time. Building knowledge upon previous knowledge instead of having to reinitialize and build new networks.
- GATO is a general agent that takes text, images and actions as sequence input. Input is sequentialized through a tokenization process. Images are downscaled then mapped to a tokenspace. The token space for images, text and actions are disjoint and orthogonal. This is fed into a transformer architecture which maps the token space together forming the connections necessary.
- Tokens are embedded using word embeddings creating vectors of real numbers that are differential meaning it’s possible to optimize using regular gradients. One can observe that word embeddings of things that are disjoint in the token space for example the word ‘cat’ and an image of a cat move align during training which is fascinating. The network is learning the potential convections between the modalities (token spaces). 


Original [podcast](https://www.youtube.com/watch?v=aGBLRlLe7X8&t=998s&ab_channel=LexFridman) (2022)
