---
layout: post
title: "LLM Inference and Single GPU LLaMA"
categories: [NLP, Transformers]
year: 2023
---
I stumbled upon a couple of interesting blog posts surrounding LLaMA and LLM inference.

## Running LLaMa on a single GPU
One of the most alluring things to come from LLaMA is the capability to run the models on a single GPU. Now the authors mention this possibility but their implementation was made even more accessible when [someone](https://github.com/ggerganov/llama.cpp) ported the inference code to raw C++ allowing it to run locally on a wide variety of consumer hardware. So, how is this possible? Well first of let's remind ourselves of the constraints we're working with. A high-end consumer grade GPU, e.g the NVIDIA 4090, has **1008 GB/s** of memory bandwidth and **82.6 TFLOPS** of compute (FP16/half-precision floating point).  Memory bandwidth dictates the speed of which data can be moved from the RAM to the on-chip memory where we can perform the matrix operations required. On-chip memory is small compared to the available RAM; memory bandwidth is about 2 orders of magnitude smaller than compute meaning that memory bandwidth tends to be the bottleneck for inference. [This](https://finbarr.ca/how-is-llama-cpp-possible/) blog post goes into further details of actually deriving the inference costs of a LLaMa style model and shows how a M1 Macbook Air can produce about 10 tokens/s on a 7B model. Fascinating! 

## Latency

The following latency calculations are particularly interesting as it allows one to calculate the tok/s given model size $P$, precision bytes $n_{\text{bytes}}$, batch size $B$, GPU memory bandwidth $n_{\text{memory bandwidth}}$ and GPU compute $n_{\text{flops}}$:

$\text{latency}_{\text{model}} = \text{max}(\text{latency}_{\text{compute}}, \text{latency}_{\text{memory}})$

$\text{latency}_{\text{memory}} = \frac{2*P*n_{\text{bytes}}*B}{n_{\text{memory bandwidth}}}$

$\text{latency}_{\text{compute}} = \frac{2*P}{n_{\text{flops}}}$

Typically, batch size is 1 during inference due to the nature of autoregressive sampling. Every individual weight has be loaded from DRAM onto the on-board chip for every single token making inference extremely memory-bound. A result of this is that memory bandwidth becomes a lot more important than FLOPS when it comes to inference latency and it's one of the main reasons a MacBook performs so surprisingly well in this task. An A100 has about 200X the compute of a MacBook M2 but only 20x the memory bandwidth. 

## Speculative Execution
Due to memory-boundness, an unintuitive consequence is that forwarding a single token through an LLM is about as quick as forwarding *k* tokens (think batch size $k$). This holds as long as we remain "memory-bound":

$\text{latency}_{\text{memory}} > \text{latency}_{\text{compute}} = \frac{2*P*n_{\text{bytes}}*B}{n_{\text{memory bandwidth}}} > \frac{2*P}{n_{\text{flops}}}$.

We would really like to exploit this fact - most of our compute is just fiddling it's thumbs waiting for memory to load our model weights - but we can't, at least not naively, because every N-th token has a serial dependency on all previous tokens. Well, what if we just guess what next $k$ tokens are going to be? It may sound silly but that is exactly what *speculative exceution* does. It uses a smaller, much cheaper draft model to generate a candidate sequence of $k$ tokens which can be fed into the inference model at roughly the same speed as 1 token. If our draft model is correct we simply proceed past a token and if it's wrong we eat the cost of generating the candidate sequence. It took me a while to wrap my head around but this simple trick works because most of the tokens generated by a model are "easy", easy enough for a much simpler model to predict. 

