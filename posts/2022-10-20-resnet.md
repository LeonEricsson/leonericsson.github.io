---
layout: post
title: "Deep Residual Learning for Image Recognition"
categories: [Deep Learning]
year: 2015
type: Blog post
author: He
exturl: https://arxiv.org/abs/1512.03385
---

This paper explores extremely deep network architectures to see if they could outcompete the current optimal nets for image classification (VGG nets) with a lower total complexity but deeper. They sought to answer: *Is learning better networks as easy as stacking more layers?*. The authors noticed a significant *degradation* problem, with the network depth increasing, accuracy is saturated and then degrades rapidly. Unexpectedly such degradation is not caused by overfitting, and adding more layers to a suitably deep model leads to a higher training error. Intuitively, the solution space for a shallower network is a subspace of a deeper network and the same underlying function could be achieved by simply mapping the additional layers as an identity mapping. The fact that this doesnâ€™t occur suggests that the additional layers have trouble mapping the identity function and to solve this issue the authors suggest a residual mapping where an input shortcuts 2-3 layers. Note however that in real cases, it is unlikely that identity mappings are optimal, but this reformulation, F + x, may help to precondition the problem. If the optimal function is closer to an identity mapping than to a zero mapping, it should be easier for the solver to find the perturbations with reference to an identity mapping, than to learn the function as a new one.
