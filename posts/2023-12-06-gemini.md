---
layout: post
title: "Gemini"
categories: [NLP]
year: 2023
type: paper
author: Gemini Team, Google
exturl: https://storage.googleapis.com/deepmind-media/gemini/gemini_1_report.pdf
---

It's finally here. After months of rumors, question marks and speculation, it looks like Google actually pulled through. The sentiment was pretty split on if they were going to succeed, especially considering the rumors just last week of the launch being pushed back to next year. Question is though if it's still a bit too late, in the research community I feel like the general hype for LLM's has reduced significantly, there's a looming sentiment that we've exhausted the scaling laws and are only looking at minimal returns on future computational efforts. Anyway, Google is reporting that it's managed to beat GPT-4 across most benchmarks so let's focus on the present and see what there is to gather from their technical report, even though I worry that it's going to be very little...

## Recycling old architectures?

Gemini is, to no-one's suprise, a decoder-only Transformer model, enhanced with numerous optimizations to enable stable training and optimized inference _on Google's Tensor Processing Unit_. The TPU sees very little love outside of Google's internal work, primarily because Google only sells them to select Cloud partners. That mean's they can't utilize the work horse that is open-source communities and instead have to rely on a lot of work internally. While this is costly, it comes with the advantage of having complete control of the entire stack. Tangent aside, the Gemini family (Ultra, Pro and Nano) is trained to support 32k context length, with multi-query attention for efficient inference. The most noteworthy architectural detail is probably that Gemini is trained, beginning to end, across multiple modalities, accommodating audio, video and image input. Videos are encoded as a sequence of image tokens. Interestingly, they claim that the Pro model was trained in a matter of **weeks**, levering a fraction of Ultra's resources. The Nano series (two models at 1.8B and 3.25B) is distilled to produce best-in-class small language model performance, they are 4-bit (!!) quantized to run on-device. Unfortunately, this is they tell us about the architecture.

## Training

They go on discussing the immense complexities of training models of this scale. Ultra is trained on a large fleet of TPUv4 accelerators across multiple SuperPods (4096 chips). We would expect the total number of chips to be somewhere in the 10s of thousands. The intra-cluster and inter-cluster networks are fast enough to warrant commonly used synchronous training techniques such as model and data parallelism. They implement everything using JAX and are able to improve overall goodput (time spent computing over the total elapsed training time) from 85% (PaLM) to 97%. I've quite enjoyed using JAX lately, it's become a favorite in a couple of reinforcement learning projects thanks to the ability to vectorize environments and run an entire GPU-only RL setup.

Gemini's dataset is both multimodal and multilingual, as one would expect given the extent of Google's name. The recent rumor stating Gemini's delay, claimed that the model wasn't handling multilingual very well but it seems like this has been fixed then. The data used comes from web documents, books and code, including images, audio and video data. I speculate they have been able to extract a lot of high quality textual data from audio and video but we can't know for certain because they don't tell us the amount of tokens used. SentencePiece is used as the tokenizer,however SP isn't a tokenizer in itself but rather a tokenization framework so it's unclear exactly what algorithm is used. Despite not telling us how many tokens the models are trained on, they do state that they follow the approaches of Chinchilla and Llama, with smaller models training for significantly more tokens to improve performance for a given inference budget.

## Evaluation

I don't want to get too bogged down with the details in the evaluation but certain things stood out to me.

Gemini Ultra is the first to achieve superhuman performance on MMLU, an exam benchmark spanning 57 subjects. It is able to achieve this using self-consistency with chain-of-thought (CoT-SC), an ensemble chain-of-thought approach that samples $k$ i.i.d chains of thoughts. If there is significant consensus in the answers it will use it, otherwise it reverts back to greedy sampling based on maximum likelihood. Interstingly, CoT-SC@32 actually improves upon GPT-4 reported MMLU performance as well but Gemini actually loses to GPT-4 when using few-shot, k=5 as the original GPT-4 technical report used.

Overall, Gemini Ultra reports state of the art results across almost all major benchmarks.

They claim to perform synthetic retrieval tests to evaluate Geminis long context abilities but there is very little information to go on. They say that Ultra retreives correct value with 98% accuracy when queried across the full context length but at the same time when they describe the text they say that "we place key-value pairs at the beginning of the context, then add long filler text, and ask for value associated with a particular key". So what is it, are the values in the beginning of the text or throughout the entire context window?

The biggest bombshell is probably Gemini's impressive performance across all multimodal tasks. I think there is overall questions about how much better the textual aspect of Gemini is as compared to GPT-4 but it seems, at least from the benchmarks reported, that Gemini achieves state-of-the-art results across image, video and audio understanding as well!

## Instruction tuning

I was hoping to get a lot more out of the instruction tuning details behind Gemini but alas we have to settle for three paragraphs. They don't tell us much besides from the usual, data quality is more important than quantity, balancing act between helpfulness with harmlessness through multi-objective optimization. The only interesting note is that they use the recipe from Constitutional AI to tune their models, injecting variants of the Google content policy as "constitutions".
