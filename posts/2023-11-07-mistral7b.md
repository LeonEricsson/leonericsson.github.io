---
layout: post
title: "Mistral 7B"
categories: [NLP]
year: 2023
type: paper
author: Jiang
exturl: https://arxiv.org/pdf/2310.06825.pdf
---
Mistral 7B has quickly become the darling foundational model of the open source community, surpassing that of LLama 2 in a lot of regards. Mistral 7B has become the favorite OSS base model to use for further development, taking the spot that LLama 2 previously occupied. While it's important to not forget that this is still a 7B model, the capabilities of some of the models stemming from Mistral is amazing. New Mistral-derived models are being published almost daily. User u/WolframRavenwolf has published several in-depth evaluations of instruction-tuned open-source models; going beyond the standard benchmarks. I highly recommend you check them out, he's dedicated some serious time to provide this to the OSS community [[1](https://www.reddit.com/r/LocalLLaMA/comments/178nf6i/mistral_llm_comparisontest_instruct_openorca/),[2](https://www.reddit.com/r/LocalLLaMA/comments/17fhp9k/huge_llm_comparisontest_39_models_tested_7b70b/),[2](https://www.reddit.com/r/LocalLLaMA/comments/17p0gut/llm_comparisontest_mistral_7b_updates_openhermes/)]. As Wolfram himself states, Mistral 7B just keeps getting better. People love it and are frequently pushing it to new heights, there's was even a [128k context version](https://huggingface.co/NousResearch/Yarn-Mistral-7b-128k) published just a few days ago!

## Mistral 7B
Naturally, one starts to wonder what's special about Mistral 7B. Well, let's dive into some of the specific implementation details. Architecturally the model is quite similar to LLama, not a huge surprise considering that three of the co-authors of this paper (also founders of Mistral) authored the original Llama paper. But, what separates Mistral 7B from LLama? Let's go through the changes. 

## Sliding Window Attention (SWA)
The paper is overall quite thin and leaves out a lot of the details behind their implementation. This annoyed me because I couldn't quite grasp the fundamentals behind Sliding Window Attention. How does the context window expand theoretically without increasing computation? What is being attended to in each separate transformer layer? A lot of questions, little answers. As such, I'm going to do my best to break down Sliding Window Attention, providing both code and visualizations. 

The elevator pitch of SWA sounds something like this: *Vanilla attention requires time and memory that grow quadratically with the sequence length (linear memory thanks to FlashAttention). SWA employs a fixed-size window attention to $W$ preceding tokens. Multiple stacked layers of such windows attention results in a large receptive field where top layers have access to all input locations, similar to CNNs.* If you are familiar with [WaveNet](https://arxiv.org/pdf/1609.03499.pdf), this should ring a bell. Unfortunately, my penny still hadn't dropped. Instead, let's walk through a practical example of SWA! 

I introduce a number of simplifications to this example, so we can focus on the core problem at hand. We'll use word-level tokenization, meaning one token per word, embedding dimension of 1 and a average operation across the window instead of attention. The last abstraction means we don't have query, key and value vectors but instead operate on the embedding vector directly. This will make the results a lot clearer and remove a lot of repetitiveness from the example. Remember, the fundamentals are still the same, we're still computing the resulting value from the tokens inside the window. Lastly, by layers we mean attention layers, we only care about the attention part of the architecture right now. My example is meant to illustrate Figure 1 of the paper, shown below.

![](/public/images/slidingwindowattentioncontextlength.png)

Input sequence = "A boy went to the shop" \
Window size = 2

**Assigning simple scalar embeddings:**

1. A: [0.1]
2. boy: [0.2]
3. went: [0.3]
4. to: [0.4]
5. the: [0.5]
6. shop: [0.6]

**Computing the attended values for Layer 1:**

- "A" has no previous token, so it remains [0.1].
- "boy" averages its value with "A": (0.2 + 0.1) / 2 = [0.15]
- "went" averages with "boy": (0.3 + 0.2) / 2 = [0.25]
- "to" averages with "went": (0.3 + 0.4) / 2 = [0.35]
- "the" averages with "to": (0.4 + 0.5) / 2 = [0.45]
- "shop" averages with "the": (0.5 + 0.6) / 2 = [0.55]

Let's illustrate this

| Layer / Token |   A  |  boy | went |  to  | the  | shop |
|-------|------|------|------|------|------|------|
|  L1   | 0.1  | 0.15 | 0.25 | 0.35 | 0.45 | 0.55 |

Here's the representation of the context that each token attends to

| Layer/Token |   A   |    boy    |    went    |      to      |      the     |     shop     |
|-------------|-------|-----------|------------|--------------|--------------|--------------|
| L1          | [A]   | [A, boy]  | [boy, went]| [went, to]   | [to, the]    | [the, shop]  |

Remember that in a normal Transformer decoder, the representation would look like this

| Layer/Token |    A    |        boy       |           went          |               to              |                the               |                   shop                  |
|-------------|---------|------------------|-------------------------|-------------------------------|----------------------------------|-----------------------------------------|
| L1          | [A]     | [A, boy]         | [A, boy, went]          | [A, boy, went, to]            | [A, boy, went, to, the]          | [A, boy, went, to, the, shop]           |

In the earlier layers, the attention is heavily restricted, but let's check out what happens as the sequence moves on to Layer 2. 

**Computing the attended values for Layer 2:**

- "A" has no previous token, so it remains [0.1].
- "boy" averages its value with "A": (0.15 + 0.1) / 2 = [0.125]
- "went" averages with "boy": (0.25 + 0.15) / 2 = [0.2]
- ... and so on.

The first two retain their context, but things get interesting when we look at the context for the succeeding values. 

| Layer/Token |   A   |    boy    |    went    |      to      |      the     |     shop     |
|-------------|-------|-----------|------------|--------------|--------------|--------------|
| L1          | [A]   | [A, boy]  | [boy, went]| [went, to]   | [to, the]    | [the, shop]  |
| L2          | [A]   | [A, boy]  | [A, boy, went] | [boy, went, to] | [went, to, the] | [to, the, shop] |

Notice how "went" representation is influenced by [**A**, boy, went] indirectly through "boy" representation [A, boy]! The compiled values:

| Layer/Token |   A   |   boy  |  went  |    to   |  the  | shop |
|-------------|-------|--------|--------|---------|-------|------|
| L1          |  0.1  |  0.15  |  0.25  |  0.35   | 0.45  | 0.55 |
| L2          |  0.1  | 0.125  |  0.20  |  0.30   | 0.40  | 0.50 |

Sliding Window Attention exploits the stacked layers of transformers to attend to information beyond the window size. The context representation and values through 4 layers are shown below.

| Layer/Token |   A   |    boy    |    went    |      to      |      the     |     shop     |
|-------------|-------|-----------|------------|--------------|--------------|--------------|
| L1          | [A]   | [A, boy]  | [boy, went]| [went, to]   | [to, the]    | [the, shop]  |
| L2          | [A]   | [A, boy]  | [A, boy, went] | [boy, went, to] | [went, to, the] | [to, the, shop] |
| L3          | [A]   | [A, boy]  | [A, boy, went] | [A, boy, went, to] | [boy, went, to, the] | [went, to, the, shop] |
| L4          | [A]   | [A, boy]  | [A, boy, went] | [A, boy, went, to] | [A, boy, went, to, the] | [boy, went, to, the, shop] |

| Layer/Token |   A   |   boy  |  went  |    to   |  the  | shop |
|-------------|-------|--------|--------|---------|-------|------|
| L1          |  0.1  |  0.15  |  0.25  |  0.35   | 0.45  | 0.55 |
| L2          |  0.1  | 0.125  |  0.20  |  0.30   | 0.40  | 0.50 |
| L3          |  0.1  | 0.1125 | 0.1625 |  0.25   | 0.35  | 0.45 |
| L4          |  0.1  | 0.10625| 0.1375 | 0.20625 | 0.30  | 0.40 |

The values represent a gradual blending of context: starting with individual token values, each subsequent layer averages these within a window, spreading the influence of early tokens (like "A") through the sequence. At each attention layer, information can move forward by $w$ tokens. Hence, after $k$ attention layers, information can move forward by up to $k \times W$ tokens. Our example simplifies attention to equal weighting across tokens, abstracting away more nuanced weighting variations, but effectively demonstrates local context aggregation in the Sliding Window Attention mechanism. 

## Cache
Mistral has several implementations targeted increasing throughput and efficacy, to this end they employ two strategies on their cache namely: (1) **Rolling Buffer Cache** and (2) **Pre-filling and Chunking**. The cache has a fixed size $W$, where the keys and values for the timestep $i$ are stored in position $i$ mod $W$. The rolling buffer implies that, when $i$ is larger than $W$, past values in the cache are overwritten, and the size of the cache stops increasing. This reduces cache memory usage without comprising model quality. 

Additionally, the cache is pre-filled with the prompt during generation. Prompts are split into $W$ size chunks and processed sequentially to fill the cache. The figure from the paper illustrates this well.

![](/public/images/prefillcachemistral.png)