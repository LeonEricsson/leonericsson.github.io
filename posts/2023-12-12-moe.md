---
layout: post
title: "Mixture of Experts"
categories: [NLP]
year: 2023
type: post
---

Mixture of Experts (MoE) are the flavor of the week following Mistral's release of **Mixtral 7Bx8**. Mistral are just killing it at the moment, love their style of just dropping a torrent link and let the results speak for themselves. The contrast to Google's announcement of Gemini is hilarious and it makes sense, Mistral is never going to appear more flashy or have the budget for a huge announcement ceremony, instead they entertain the 90's hacker vibe instead. Anyway as I was saying, Mixture of Experts are in vouge right now, but they are hardly a new discovery so today I'd like to present a brief overview of their history and why they are relevant in todays LLM landscape.
