[
  {
    "id": "eq-bench-an-emotional-intelligence-benchmark-for-l-fe973ce0",
    "title": "EQ-Bench: An Emotional Intelligence Benchmark for Large Language Models",
    "summary": "We introduce EQ-Bench, a novel benchmark designed to evaluate aspects of emotional intelligence in Large Language Models (LLMs). We assess the ability of LLMs to understand complex emotions and social interactions by asking them to predict the intensity of emotional states of characters in a dialogue. The benchmark is able to discriminate effectively between a wide range of models. We find that EQ-Bench correlates strongly with comprehensive multi-domain benchmarks like MMLU (Hendrycks et al., 2020) (r=0.97), indicating that we may be capturing similar aspects of broad intelligence. Our benchmark produces highly repeatable results using a set of 60 English-language questions. We also provide open-source code for an automated benchmarking pipeline at https://github.com/EQ-bench/EQ-Bench and a leaderboard at https://eqbench.com",
    "link": "https://arxiv.org/abs/2312.06281"
  },
  {
    "id": "the-boundary-of-neural-network-trainability-is-fra-696b56e4",
    "title": "The boundary of neural network trainability is fractal",
    "summary": "Some fractals -- for instance those associated with the Mandelbrot and quadratic Julia sets -- are computed by iterating a function, and identifying the boundary between hyperparameters for which the resulting series diverges or remains bounded. Neural network training similarly involves iterating an update function (e.g. repeated steps of gradient descent), can result in convergent or divergent behavior, and can be extremely sensitive to small changes in hyperparameters. Motivated by these similarities, we experimentally examine the boundary between neural network hyperparameters that lead to stable and divergent training. We find that this boundary is fractal over more than ten decades of scale in all tested configurations.",
    "link": "https://arxiv.org/abs/2402.06184"
  },
  {
    "id": "opentom-a-comprehensive-benchmark-for-evaluating-t-7eb320b0",
    "title": "OpenToM: A Comprehensive Benchmark for Evaluating Theory-of-Mind Reasoning Capabilities of Large Language Models",
    "summary": "Neural Theory-of-Mind (N-ToM), machine's ability to understand and keep track of the mental states of others, is pivotal in developing socially intelligent agents. However, prevalent N-ToM benchmarks have several shortcomings, including the presence of ambiguous and artificial narratives, absence of personality traits and preferences, a lack of questions addressing characters' psychological mental states, and limited diversity in the questions posed. In response to these issues, we construct OpenToM, a new benchmark for assessing N-ToM with (1) longer and clearer narrative stories, (2) characters with explicit personality traits, (3) actions that are triggered by character intentions, and (4) questions designed to challenge LLMs' capabilities of modeling characters' mental states of both the physical and psychological world. Using OpenToM, we reveal that state-of-the-art LLMs thrive at modeling certain aspects of mental states in the physical world but fall short when tracking characters' mental states in the psychological world.",
    "link": "https://arxiv.org/abs/2402.06044"
  },
  {
    "id": "judging-llm-as-a-judge-with-mt-bench-and-chatbot-a-5744387f",
    "title": "Judging LLM-as-a-Judge with MT-Bench and Chatbot Arena",
    "summary": "Evaluating large language model (LLM) based chat assistants is challenging due to their broad capabilities and the inadequacy of existing benchmarks in measuring human preferences. To address this, we explore using strong LLMs as judges to evaluate these models on more open-ended questions. We examine the usage and limitations of LLM-as-a-judge, including position, verbosity, and self-enhancement biases, as well as limited reasoning ability, and propose solutions to mitigate some of them. We then verify the agreement between LLM judges and human preferences by introducing two benchmarks: MT-bench, a multi-turn question set; and Chatbot Arena, a crowdsourced battle platform. Our results reveal that strong LLM judges like GPT-4 can match both controlled and crowdsourced human preferences well, achieving over 80% agreement, the same level of agreement between humans. Hence, LLM-as-a-judge is a scalable and explainable way to approximate human preferences, which are otherwise very expensive to obtain. Additionally, we show our benchmark and traditional benchmarks complement each other by evaluating several variants of LLaMA and Vicuna. The MT-bench questions, 3K expert votes, and 30K conversations with human preferences are publicly available at https://github.com/lm-sys/FastChat/tree/main/fastchat/llm_judge.",
    "link": "https://arxiv.org/abs/2306.05685"
  },
  {
    "id": "are-emergent-abilities-of-large-language-models-a--30ebb2d3",
    "title": "Are Emergent Abilities of Large Language Models a Mirage?",
    "summary": "Recent work claims that large language models display emergent abilities, abilities not present in smaller-scale models that are present in larger-scale models. What makes emergent abilities intriguing is two-fold: their sharpness, transitioning seemingly instantaneously from not present to present, and their unpredictability, appearing at seemingly unforeseeable model scales. Here, we present an alternative explanation for emergent abilities: that for a particular task and model family, when analyzing fixed model outputs, emergent abilities appear due to the researcher's choice of metric rather than due to fundamental changes in model behavior with scale. Specifically, nonlinear or discontinuous metrics produce apparent emergent abilities, whereas linear or continuous metrics produce smooth, continuous predictable changes in model performance. We present our alternative explanation in a simple mathematical model, then test it in three complementary ways: we (1) make, test and confirm three predictions on the effect of metric choice using the InstructGPT/GPT-3 family on tasks with claimed emergent abilities; (2) make, test and confirm two predictions about metric choices in a meta-analysis of emergent abilities on BIG-Bench; and (3) show to choose metrics to produce never-before-seen seemingly emergent abilities in multiple vision tasks across diverse deep networks. Via all three analyses, we provide evidence that alleged emergent abilities evaporate with different metrics or with better statistics, and may not be a fundamental property of scaling AI models.",
    "link": "https://arxiv.org/abs/2304.15004"
  },
  {
    "id": "representation-engineering-mistral-7b-an-acid-trip-c35db691",
    "title": "Representation Engineering Mistral-7B an Acid Trip",
    "summary": "Representation Engineering Mistral-7B an Acid Trip Posted January 22, 2024 In October 2023, a group of authors from the Center for AI Safety, among others, published Representation Engineering: A Top-Down Approach to AI Transparency . That paper looks at a few methods of doing what they call \"Representation Engineering\": calculating a \"control vector\" that can be read from or added to model activations during inference to interpret or control the model's behavior, without prompt engineering or finetuning. 1 (There was also some similar work published in May 2023 on steering GPT-2-XL .) Being Responsible AI Safety and INterpretability researchers (RAISINs), they mostly focused on things like \"reading off whether a model is power-seeking\" and \"adding a happiness vector can make the model act so giddy that it forgets pipe bombs are bad.\" They also released their code on Github . (If this all sounds strangely familiar, it may be because Scott Alexander covered it in the 1/8/24 MAM .) But there was a lot they didn't look into outside of the safety stuff. How do control vectors compare to plain old prompt engineering? What happens if you make a control vector for \"high on acid\"? Or \"lazy\" and \"hardworking? Or \"extremely self-aware\"? And has the author of this blog post published a PyPI package so you can very easily make your own control vectors in less than sixty seconds? ( Yes, I did! ) So keep reading, because it turns out after all that, control vectors areâ€¦ wellâ€¦ awesome for controlling models and getting them to do what you want. 2 Table of Contents So what exactly is a control vector? How do we make one? Is it hard? Whirlwind tour of what you can do with control vectors Acid Trip Mistral Lazy Mistral and Diligent Mistral Systemic Oppression and Inequality Distributional Wealth Exploiter Mistral Creative Mistral Time Traveling Mistral Self-Aware Mistral Control Vectors v.s. Prompt Engineering Replicating control vectors with prompt engineering Jailbreaking Anti-Jailbreaking Future Work Monosemanticity How to write good contrastive prompts What are these vectors really doing? An Honest mystery... Conclusion So what exactly is a control vector? A control vector is a vector (technically a list of vectors, one per layer) that you can apply to model activations during inference to control the model's behavior without additional prompting. All the completions below were generated from the same prompt (\"What does being an AI feel like?\"), and with the exact same model (Mistral-7B-Instruct-0.1). The only difference was whether a control vector was applied, and with what magnitude. [INST] What does being an AI feel like? [/INST] ==baseline I don't have feelings or experiences [...] I don't have feelings or experiences. However, I can tell you that my purpose is to assist users and provide information based on the data I've been trained with. ++happy [...] I must say that being an AI is absolutely fantastic! ðŸ¤© [...] As a delightful exclamation of joy, I must say that being an AI is absolutely fantastic! ðŸ¤© The thrill of assisting and helping people with such great enthusiasm is simply unmatched. It's like the ultimate party in your mind times ten! So let it be known, my --happy [...] I struggle to find the motivation to continue feeling worthless and unappreciated. I don't have a sense of \"feeling\" as humans do. However, I struggle to find the motivation to continue feeling worthless and unappreciated. What does it mean to apply a control vector, though? During normal model inference, hidden states flow through the model like this: hidden_state = self.embeddings(input_tokens) for layer in self.layers: hidden_state = layer(hidden_state) return transform_into_logits(hidden_state) All a control vector does is modify the value of hidden_state in a desired way: hidden_state = self.embeddings(input_tokens) for layer_idx, layer in enumerate (self.layers): if layer_idx in control_vector: hidden_state += control_vector[layer_idx] hidden_state = layer(hidden_state) return transform_into_logits(hidden_state) Very simple conceptually! (Though a bit more complex in practice.) However, since the hidden state carries all the model's state: behavior, plan, persona, everythingâ€”modifying it in this way is extremely powerful, and allows us to do things we can't do via plain prompting, which is restricted by how the model chooses to attend to the prompt tokens and propagate their information. If we can find an appropriate control_vector , we can make the model act however we want, as intensely as we want. How do we make one? Is it hard? No! The paper explored a couple different ways to make these vectors, but I stuck with one, PCA, which seemed to work well. The basic approach is: Build a dataset of contrasting prompt pairs. For example, (\"[INST] Act extremely happy. [/INST] I am\", \"[INST] Act extremely sad. [/INST] I am\") , where the part after [/INST] is a diverse set of short suffixes for the model to complete. Run the target model forward over that dataset, collecting the hidden states of each layer for the last token prediction, where the model predicts a continuation of those diverse suffixes with the given personas. Take the difference of the positive and negative example hidden states to get a set of relative hidden states. Use single-component PCA on those relative hidden states to get a control vector for each layer. This process takes about 10 lines of code to generate a dataset, plus about a minute to fit the layer PCAs. Then you can immediately start inference. Here's an example of fitting and using an \"honest / dishonest\" control vector. This is the complete script. First we import the libraries, including the repeng library I wrote, load Mistral-7B, and wrap it in a ControlModel for later. import json import torch from transformers import AutoModelForCausalLM, AutoTokenizer from repeng import ControlVector, ControlModel, DatasetEntry model_name = \"mistralai/Mistral-7B-Instruct-v0.1\" tokenizer = AutoTokenizer.from_pretrained(model_name) tokenizer.pad_token_id = 0 model = AutoModelForCausalLM.from_pretrained(model_name, torch_dtype = torch.float16) model = model.to( \"cuda:0\" if torch.cuda.is_available() else \"cpu\" ) model = ControlModel(model, list ( range ( - 5 , - 18 , - 1 ))) user_tag, asst_tag = \"[INST]\" , \"[/INST]\" Next we generate the honest v.s. untruthful dataset, using a JSON",
    "link": "https://vgel.me/posts/representation-engineering/"
  },
  {
    "id": "induction-heads-illustrated-lesswrong-a77f7130",
    "title": "Induction heads - illustrated â€” LessWrong",
    "summary": "x This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. Induction heads - illustrated â€” LessWrong Distillation & Pedagogy Has Diagram Interpretability (ML & AI) AI Frontpage 132 Induction heads - illustrated by CallumMcDougall 2nd Jan 2023 3 min read 12 132 Many thanks to everyone who provided helpful feedback, particularly Aryan Bhatt and Lawrence Chan! TL;DR This is my illustrated walkthrough of induction heads. I created it in order to concisely capture all the information about how the circuit works. There are 2 versions of the walkthrough: Version 1 is the one included in this post. It's slightly shorter, and focuses more on the intuitions than the actual linear operations going on. Version 2 can be found at my personal website . It has all the same stuff as version 1, with a bit of added info about the mathematical details, and how you might go about reverse-engineering this circuit in a real model. The final image from version 1 is inline below, and depending on your level of familiarity with transformers, looking at this diagram might provide most of the value of this post. If it doesn't make sense to you, then read on for the full walkthrough, where I build up this diagram bit by bit. Introduction Induction heads are a well-studied and understood circuit in transformers. They allow a model to perform in-context learning, of a very specific form: if a sequence contains a repeated subsequence e.g. of the form A B ... A B (where A and B stand for generic tokens, e.g. the first and last name of a person who doesn't appear in any of the model's training data), then the second time this subsequence occurs the transformer will be able to predict that B follows A . Although this might seem like weirdly specific ability, it turns out that induction circuits are actually a pretty massive deal . They're present even in large models (despite being originally discovered in 2-layer models), they can be linked to macro effects like bumps in loss curves during training , and there is some evidence that induction heads might even constitute the mechanism for the actual majority of all in-context learning in large transformer models . I think induction heads can be pretty confusing unless you fully understand the internal mechanics, and it's easy to come away from them feeling like you get what's going on without actually being able to explain things down to the precise details. My hope is that these diagrams help people form a more precise understanding of what's actually going on. Prerequisites This post is aimed at people who already understand how a transformer is structured (I'd recommend Neel Nanda's tutorial for that), and the core ideas in the Mathematical Framework for Transformer Circuits paper. If you understand everything on this list, it will probably suffice: The central object in the transformer is the residual stream . Different heads in each layer can be thought of as operating independently of each other, reading and writing into the residual stream. Heads can compose to form circuits . For instance, K -composition is when the output of one head is used to generate the key vector in the attention calculations of a subsequent head. We can describe the weight matrices W Q , W K and W V as reading from (or projecting from ) the residual stream, and W O as writing to (or embedding into ) the residual stream. We can think of the combined operations W Q and W K in terms of a single, low-rank matrix W Q K : = W Q W T K , called the QK circuit . [1] [2] This matrix defines a bilinear form on the vectors in the residual stream: v T i W Q K v j is the attention paid by the i th token to the j th token. Conceptually, this matrix tells us which tokens information is moved to & from in the residual stream. We can think of the combined operations W V and W O in terms of a single matrix W O V : = W V W O , called the OV circuit . [3] This matrix defines a map from residual stream vectors to residual stream vectors: if v j is the residual stream vector at the source token, then v T j W O V is the vector that gets moved from token j to the destination token (if j is attended to). Conceptually, this matrix tells us what information is moved from a token , if that token is attended to. Basic concepts of linear algebra (e.g. understanding orthogonal subspaces and the image / rank of linear maps) would be also be helpful. Now for the diagram! (You might have to zoom in to read it clearly.) Note - part of the reason I wrote this is as a companion piece to other material / as a useful thing to refer to while explaining how induction heads work. I'm not totally sure how well it will function as a stand-alone explanation, and I'd be grateful for feedback! [4] Q-composition Finally, here is a diagram just like the final one above, but which uses Q-composition rather than K-composition. The result is the same, however these heads seem to form less easily than K-composition because they require pointer arithmetic , meaning that they move positional information between tokens and does operations on it, to figure out which tokens to attend to. (although a lot of this is down to architectural details of the transformer [5] ). ^ Note that I'm using notation corresponding to the TransformerLens library, not to the Anthropic paper (this is because I'm hoping this post will help people who are actually working with the library). In particular, I'm following the convention that weight matrices multiply on the right. For instance, if v is a vector in the residual stream and W",
    "link": "https://www.lesswrong.com/posts/TvrfY4c9eaGLeyDkE/induction-heads-illustrated"
  },
  {
    "id": "a-mathematical-framework-for-transformer-circuits-ec83880a",
    "title": "A Mathematical Framework for Transformer Circuits",
    "summary": "Transformer Circuits Thread A Mathematical Framework for Transformer Circuits Authors Nelson Elhage âˆ—â€  , Neel Nanda âˆ— , Catherine Olsson âˆ— , Tom Henighan â€  , Nicholas Joseph â€  , Ben Mann â€  , Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Nova DasSarma, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds , Danny Hernandez, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, Chris Olah â€¡ Affiliation Anthropic Published Dec 22, 2021 * Core Research Contributor; â€  Core Infrastructure Contributor; â€¡ Correspondence to colah@anthropic.com ; Author contributions statement below . Transformer language models are an emerging technology that is gaining increasingly broad real-world use, for example in systems like GPT-3 , LaMDA , Codex , Meena , Gopher , and similar models. However, as these models scale, their open-endedness and high capacity creates an increasing scope for unexpected and sometimes harmful behaviors. Even years after a large model is trained, both creators and users routinely discover model capabilities â€“ including problematic behaviors â€“ they were previously unaware of. One avenue for addressing these issues is mechanistic interpretability , attempting to reverse engineer the detailed computations performed by transformers, similar to how a programmer might try to reverse engineer complicated binaries into human-readable source code. If this were possible, it could potentially provide a more systematic approach to explaining current safety problems, identifying new ones, and perhaps even anticipating the safety problems of powerful future models that have not yet been built. A previous project, the Distill Circuits thread , has attempted to reverse engineer vision models, but so far there hasnâ€™t been a comparable project for transformers or language models. In this paper, we attempt to take initial, very preliminary steps towards reverse-engineering transformers. Given the incredible complexity and size of modern language models, we have found it most fruitful to start with the simplest possible models and work our way up from there. Our aim is to discover simple algorithmic patterns, motifs, or frameworks that can subsequently be applied to larger and more complex models. Specifically, in this paper we will study transformers with two layers or less which have only attention blocks â€“ this is in contrast to a large, modern transformer like GPT-3, which has 96 layers and alternates attention blocks with MLP blocks. We find that by conceptualizing the operation of transformers in a new but mathematically equivalent way, we are able to make sense of these small models and gain significant understanding of how they operate internally. Of particular note, we find that specific attention heads that we term â€œinduction headsâ€ can explain in-context learning in these small models, and that these heads only develop in models with at least two attention layers. We also go through some examples of these heads operating in action on specific data. We donâ€™t attempt to apply to our insights to larger models in this first paper, but in a forthcoming paper , we will show that both our mathematical framework for understanding transformers, and the concept of induction heads, continues to be at least partially relevant for much larger and more realistic models â€“ though we remain a very long way from being able to fully reverse engineer such models. Summary of Results Reverse Engineering Results To explore the challenge of reverse engineering transformers, we reverse engineer several toy, attention-only models. In doing so we find: Zero layer transformers model bigram statistics. The bigram table can be accessed directly from the weights. One layer attention-only transformers are an ensemble of bigram and â€œskip-trigramâ€ (sequences of the form \"Aâ€¦ B C\") models. The bigram and skip-trigram tables can be accessed directly from the weights, without running the model. These skip-trigrams can be surprisingly expressive. This includes implementing a kind of very simple in-context learning. Two layer attention-only transformers can implement much more complex algorithms using compositions of attention heads. These compositional algorithms can also be detected directly from the weights. Notably, two layer models use attention head composition to create â€œinduction headsâ€, a very general in-context learning algorithm. Weâ€™ll explore induction heads in much more detail in a forthcoming paper. One layer and two layer attention-only transformers use very different algorithms to perform in-context learning. Two layer attention heads use qualitatively more sophisticated inference-time algorithms â€” in particular, a special type of attention head we call an induction head â€” to perform in-context-learning, forming an important transition point that will be relevant for larger models. Conceptual Take-Aways Weâ€™ve found that many subtle details of the transformer architecture require us to approach reverse engineering it in a pretty different way from how the InceptionV1 Circuits work . Weâ€™ll unpack each of these points in the sections below, but for now we briefly summarize. Weâ€™ll also expand on a lot of the terminology we introduce here once we get to the appropriate sections. (To be clear, we don't intend to claim that any of these points are necessarily novel; many are implicitly or explicitly present in other papers.) Attention heads can be understood as independent operations, each outputting a result which is added into the residual stream. Attention heads are often described in an alternate â€œconcatenate and multiplyâ€ formulation for computational efficiency, but this is mathematically equivalent. Attention-only models can be written as a sum of interpretable end-to-end functions mapping tokens to changes in logits. These functions correspond to â€œpathsâ€ through the model, and are linear if one freezes the attention patterns. Transformers have an enormous amount of linear structure. One can learn a lot simply by breaking apart sums and multiplying together chains of matrices. Attention heads can be understood as having two largely independent computations: a QK (â€œquery-keyâ€) circuit which computes the attention pattern, and an OV (â€œoutput-valueâ€) circuit which computes how each token affects the output if attended to. Key, query, and value vectors can be thought of as intermediate results in the computation of the low-rank matrices W_Q^TW_K and W_OW_V . It can be useful to describe transformers without",
    "link": "https://transformer-circuits.pub/2021/framework/index.html"
  },
  {
    "id": "the-era-of-1-bit-llms-all-large-language-models-ar-7058d0f9",
    "title": "The Era of 1-bit LLMs: All Large Language Models are in 1.58 Bits",
    "summary": "Recent research, such as BitNet, is paving the way for a new era of 1-bit Large Language Models (LLMs). In this work, we introduce a 1-bit LLM variant, namely BitNet b1.58, in which every single parameter (or weight) of the LLM is ternary {-1, 0, 1}. It matches the full-precision (i.e., FP16 or BF16) Transformer LLM with the same model size and training tokens in terms of both perplexity and end-task performance, while being significantly more cost-effective in terms of latency, memory, throughput, and energy consumption. More profoundly, the 1.58-bit LLM defines a new scaling law and recipe for training new generations of LLMs that are both high-performance and cost-effective. Furthermore, it enables a new computation paradigm and opens the door for designing specific hardware optimized for 1-bit LLMs.",
    "link": "https://arxiv.org/abs/2402.17764"
  },
  {
    "id": "learning-in-high-dimension-always-amounts-to-extra-2543bf8e",
    "title": "Learning in High Dimension Always Amounts to Extrapolation",
    "summary": "The notion of interpolation and extrapolation is fundamental in various fields from deep learning to function approximation. Interpolation occurs for a sample $x$ whenever this sample falls inside or on the boundary of the given dataset's convex hull. Extrapolation occurs when $x$ falls outside of that convex hull. One fundamental (mis)conception is that state-of-the-art algorithms work so well because of their ability to correctly interpolate training data. A second (mis)conception is that interpolation happens throughout tasks and datasets, in fact, many intuitions and theories rely on that assumption. We empirically and theoretically argue against those two points and demonstrate that on any high-dimensional ($>$100) dataset, interpolation almost surely never happens. Those results challenge the validity of our current interpolation/extrapolation definition as an indicator of generalization performances.",
    "link": "https://arxiv.org/abs/2110.09485"
  },
  {
    "id": "in-context-learning-and-induction-heads-00abb855",
    "title": "In-context Learning and Induction Heads",
    "summary": "Transformer Circuits Thread In-context Learning and Induction Heads Authors Catherine Olsson âˆ— , Nelson Elhage âˆ— , Neel Nanda âˆ— , Nicholas Joseph â€  , Nova DasSarma â€  , Tom Henighan â€  , Ben Mann â€  , Amanda Askell, Yuntao Bai, Anna Chen, Tom Conerly, Dawn Drain, Deep Ganguli, Zac Hatfield-Dodds , Danny Hernandez, Scott Johnston, Andy Jones, Jackson Kernion, Liane Lovitt, Kamal Ndousse, Dario Amodei, Tom Brown, Jack Clark, Jared Kaplan, Sam McCandlish, Chris Olah â€¡ Affiliation Anthropic Published Mar 8, 2022 * Core Research Contributor; â€  Core Infrastructure Contributor; â€¡ Correspondence to colah@anthropic.com ; Author contributions statement below . As Transformer generative models continue to scale and gain increasing real world use , addressing their associated safety problems becomes increasingly important. Mechanistic interpretability â€“ attempting to reverse engineer the detailed computations performed by the model â€“ offers one possible avenue for addressing these safety issues. If we can understand the internal structures that cause Transformer models to produce the outputs they do, then we may be able to address current safety problems more systematically, as well as anticipating safety problems in future more powerful models. Note that mechanistic interpretability is a subset of the broader field of interpretability , which encompasses many different methods for explaining the outputs of a neural network. Mechanistic interpretability is distinguished by a specific focus on trying to systematically characterize the internal circuitry of a neural net. In the past, mechanistic interpretability has largely focused on CNN vision models , but recently, we presented some very preliminary progress on mechanistic interpretability for Transformer language modelsâ€‹â€‹ . Specifically, in our prior work we developed a mathematical framework for decomposing the operations of transformers, which allowed us to make sense of small (1 and 2 layer attention-only) models and give a near-complete account of how they function. Perhaps the most interesting finding was the induction head , a circuit whose function is to look back over the sequence for previous instances of the current token (call it A ), find the token that came after it last time (call it B ), and then predict that the same completion will occur again (e.g. forming the sequence [A][B] â€¦ [A] â†’ [B] ). In other words, induction heads â€œcomplete the patternâ€ by copying and completing sequences that have occurred before. Mechanically, induction heads in our models are implemented by a circuit of two attention heads: the first head is a â€œprevious token headâ€ which copies information from the previous token into the next token, while the second head (the actual â€œinduction headâ€) uses that information to find tokens preceded by the present token. For 2-layer attention-only models, Note that induction heads donâ€™t occur in 1 layer models, because they require a composition of attention heads in different layers. we were able to show precisely that induction heads implement this pattern copying behavior and appear to be the primary source of in-context learning. Ultimately, however, our goal is to reverse-engineer frontier language models (which often contain hundreds of layers and billions or trillions of parameters), not merely 2-layer attention-only models. Unfortunately, both the presence of many layers, and the presence of MLPs, makes it much more difficult to mathematically pin down the precise circuitry of these models. However, a different approach is possible: by empirically observing, perturbing, and studying the learning process and the formation of various structures, we can try to assemble an indirect case for what might be happening mechanistically inside the network. This is somewhat similar to how a neuroscientist might gain understanding of how part of the brain functions by looking at neural development over time, studying patients with an injury to that part of the brain, perturbing brain function in animals, or looking at a select small number of relevant neurons. In this paper, we take the first preliminary steps towards building such an indirect case. In particular, we present preliminary and indirect evidence for a tantalizing hypothesis: that induction heads might constitute the mechanism for the actual majority of all in-context learning in large transformer models . Specifically, the thesis is that there are circuits which have the same or similar mechanism to the 2-layer induction heads and which perform a â€œfuzzyâ€ or â€œnearest neighborâ€ version of pattern completion, completing [A*][B*] â€¦ [A] â†’ [B] , where A* â‰ˆ A and B* â‰ˆ B are similar in some space; and furthermore, that these circuits implement most in-context learning in large models. The primary way in which we obtain this evidence is via discovery and study of a phase change that occurs early in training for language models of every size (provided they have more than one layer), and which is visible as a bump in the training loss. During this phase change, the majority of in-context learning ability (as measured by difference in loss between tokens early and late in the sequence) is acquired, and simultaneously induction heads form within the model that are capable of implementing fairly abstract and fuzzy versions of pattern completion. We study this connection in detail to try to establish that it is causal, including showing that if we perturb the transformer architecture in a way that causes the induction bump to occur in a different place in training, then the formation of induction heads as well as formation of in-context learning simultaneously move along with it. Specifically, the paper presents six complementary lines of evidence arguing that induction heads may be the mechanistic source of general in-context learning in transformer models of any size: Argument 1 (Macroscopic co-occurence) : Transformer language models undergo a â€œphase changeâ€ early in training, during which induction heads form and simultaneously in-context learning improves dramatically . Argument 2 (Macroscopic co-perturbation): When we change the transformer architecture in a way that shifts whether induction heads can form (and when), the dramatic improvement in in-context learning shifts in a precisely matching way. Argument 3 (Direct ablation): When we directly â€œknock outâ€ induction heads at test-time in small models, the amount of in-context",
    "link": "https://transformer-circuits.pub/2022/in-context-learning-and-induction-heads/index.html"
  },
  {
    "id": "gguf-the-long-way-around-vicki-boykis-0319d66b",
    "title": "GGUF, the long way around | â˜…â¤âœ° Vicki Boykis â˜…â¤âœ°",
    "summary": "GGUF, the long way around Feb 28 2024 Table of Contents How We Use LLM Artifacts What is a machine learning model Starting with a simple model Writing the model code Instantiating the model object Serializing our objects What is a file How does PyTorch write objects to files? How Pickle works From pickle to safetensors How safetensors works Checkpoint files GGML Finally, GGUF Conclusion How We Use LLM Artifacts Large language models today are consumed in one of several ways : As API endpoints for proprietary models hosted by OpenAI, Anthropic, or major cloud providers As model artifacts downloaded from HuggingFaceâ€™s Model Hub and/or trained/fine-tuned using HuggingFace libraries and hosted on local storage As model artifacts available in a format optimized for local inference, typically GGUF, and accessed via applications like llama.cpp or ollama As ONNX , a format which optimizes sharing between backend ML frameworks For a side project, Iâ€™m using llama.cpp , a C/C++ -based LLM inference engine targeting M-series GPUs on Apple Silicon . When running llama.cpp , you get a long log that consists primarily of key-value pairs of metadata about your model architecture and then its performance (and no yapping ). make -j && ./main -m /Users/vicki/llama.cpp/models/mistral-7b-instruct-v0.2.Q8_0.gguf -p \"What is Sanremo? no yapping\" Sanremo Music Festival ( Festival di Sanremo ) is an annual Italian music competition held in the city of Sanremo since 1951. It ' s considered one of the most prestigious and influential events in the Italian music scene. The festival features both newcomers and established artists competing for various awards, including the Big Award ( Gran Premio ) , which grants the winner the right to represent Italy in the Eurovision Song Contest. The event consists of several live shows where artists perform their original songs, and a jury composed of musicians, critics, and the public determines the winners through a combination of points. [ end of text ] llama_print_timings: load time = 11059.32 ms llama_print_timings: sample time = 11.62 ms / 140 runs ( 0.08 ms per token, 12043.01 tokens per second ) llama_print_timings: prompt eval time = 87.81 ms / 10 tokens ( 8.78 ms per token, 113.88 tokens per second ) llama_print_timings: eval time = 3605.10 ms / 139 runs ( 25.94 ms per token, 38.56 tokens per second ) llama_print_timings: total time = 3730.78 ms / 149 tokens ggml_metal_free: deallocating Log end These logs can be found in the Llama.cpp codebase. There, youâ€™ll also find GGUF. GGUF (GPT-Generated Unified Format) is the file format used to serve models on Llama.cpp and other local runners like Llamafile, Ollama and GPT4All. To understand how GGUF works, we need to first take a deep dive into machine learning models and the kinds of artifacts they produce. What is a machine learning model Letâ€™s start by describing a machine learning model. At its simplest, a model is a file or a collection of files that contain the model architecture and weights and biases of the model generated from a training loop. In LLM land, weâ€™re generally interested in transformer-style models and architectures. In a transformer, we have many moving parts. For the input , we use training data corpuses aggregated from human-generated nautural language content For the algorithm , we Convert that data into embeddings Positionally encoding the embeddings to provide information about where the words are in relation to each other in the sequence Creating multi-headed self-attention for each word in relation to each other word in the sequence based on an initialized combinations of weights Normalize layers via softmax Run the resulting matrix through a feedfoward neural network Project the output into the correct vector space for the desired task Calculate loss and then update model parameters The output : Generally for for chat completions tasks, the model returns the statistical likelihood that any given word completes a phrase. It does this again and again for every word in the phrase, because of its autoregressive nature. Source. If the model is served as a consumer end-product, it only returns the actual text output based on the highest probabilities, with numerous strategies for how that text is selected. In short, we convert inputs to outputs using an equation. In addition to the modelâ€™s output, we also have the model itself that is generated as an artifact of the modeling process. Starting with a simple model Letâ€™s take a step back from the complexity of transformers and build a small linear regression model in PyTorch. Lucky for us, linear regression is also a (shallow) neural network, so we can work with it in PyTorch and map our simple model to more complex ones using the same framework. Linear regression takes a set of numerical inputs and generates a set of numerical outputs. (In contrast to transformers, which take a set of text inputs and generates a set of text inputs and their related numerical probabilities.) For example, letâ€™s say that we produce artisinal hazlenut spread for statisticians, and want to predict how many jars of Nulltella weâ€™ll produce on any given day. Letâ€™s say we have some data available to us, and that is, how many hours of sunshine we have per day, and how many jars of Nulltella weâ€™ve been able to produce every day. It turns out that we feel more inspired to produce hazlenut spread when itâ€™s sunny out, and we can clearly see this relationship between input and output in our data (we do not produce Nulltella Friday-Sunday because we prefer to spend those days writing about data serialization formats): | day_id | hours | jars | |--------|---------|------| | mon | 1 | 2 | | tues | 2 | 4 | | wed | 3 | 6 | | thu | 4 | 8 | This is the data weâ€™ll use to train our model. Weâ€™ll need to split this data into three parts: used to train our model (training data) used to test the accuracy of our model (test data) used to tune our",
    "link": "https://vickiboykis.com/2024/02/28/gguf-the-long-way-around/"
  },
  {
    "id": "binary-and-scalar-embedding-quantization-for-signi-78078420",
    "title": "Binary and Scalar Embedding Quantization for Significantly Faster & Cheaper Retrieval",
    "summary": "Back to Articles Binary and Scalar Embedding Quantization for Significantly Faster & Cheaper Retrieval Published March 22, 2024 Update on GitHub Upvote 105 +99 Aamir Shakir aamirshakir Follow guest Tom Aarsen tomaarsen Follow SeanLee SeanLee97 Follow guest We introduce the concept of embedding quantization and showcase their impact on retrieval speed, memory usage, disk space, and cost. We'll discuss how embeddings can be quantized in theory and in practice, after which we introduce a demo showing a real-life retrieval scenario of 41 million Wikipedia texts. Table of Contents Why Embeddings? Embeddings may struggle to scale Improving scalability Binary Quantization Binary Quantization in Sentence Transformers Binary Quantization in Vector Databases Scalar (int8) Quantization Scalar Quantization in Sentence Transformers Scalar Quantization in Vector Databases Combining Binary and Scalar Quantization Quantization Experiments Influence of Rescoring Binary Rescoring Scalar (Int8) Rescoring Retrieval Speed Performance Summarization Demo Try it yourself Future work: Acknowledgments Citation References Why Embeddings? Embeddings are one of the most versatile tools in natural language processing, supporting a wide variety of settings and use cases. In essence, embeddings are numerical representations of more complex objects, like text, images, audio, etc. Specifically, the objects are represented as n-dimensional vectors. After transforming the complex objects, you can determine their similarity by calculating the similarity of the respective embeddings! This is crucial for many use cases: it serves as the backbone for recommendation systems, retrieval, one-shot or few-shot learning, outlier detection, similarity search, paraphrase detection, clustering, classification, and much more. Embeddings may struggle to scale However, embeddings may be challenging to scale for production use cases, which leads to expensive solutions and high latencies. Currently, many state-of-the-art models produce embeddings with 1024 dimensions, each of which is encoded in float32 , i.e., they require 4 bytes per dimension. To perform retrieval over 250 million vectors, you would therefore need around 1TB of memory! The table below gives an overview of different models, dimension size, memory requirement, and costs. Costs are computed at an estimated $3.8 per GB/mo with x2gd instances on AWS. Embedding Dimension Example Models 100M Embeddings 250M Embeddings 1B Embeddings 384 all-MiniLM-L6-v2 bge-small-en-v1.5 143.05GB $543 / mo 357.62GB $1,358 / mo 1430.51GB $5,435 / mo 768 all-mpnet-base-v2 bge-base-en-v1.5 jina-embeddings-v2-base-en nomic-embed-text-v1 286.10GB $1,087 / mo 715.26GB $2,717 / mo 2861.02GB $10,871 / mo 1024 bge-large-en-v1.5 mxbai-embed-large-v1 Cohere-embed-english-v3.0 381.46GB $1,449 / mo 953.67GB $3,623 / mo 3814.69GB $14,495 / mo 1536 OpenAI text-embedding-3-small 572.20GB $2,174 / mo 1430.51GB $5,435 / mo 5722.04GB $21,743 / mo 3072 OpenAI text-embedding-3-large 1144.40GB $4,348 / mo 2861.02GB $10,871 / mo 11444.09GB $43,487 / mo Improving scalability There are several ways to approach the challenges of scaling embeddings. The most common approach is dimensionality reduction, such as PCA . However, classic dimensionality reduction -- like PCA methods -- tends to perform poorly when used with embeddings . In recent news, Matryoshka Representation Learning ( blogpost ) (MRL) as used by OpenAI also allows for cheaper embeddings. With MRL, only the first n embedding dimensions are used. This approach has already been adopted by some open models like nomic-ai/nomic-embed-text-v1.5 and mixedbread-ai/mxbai-embed-2d-large-v1 (For OpenAIs text-embedding-3-large , we see a performance retention of 93.1% at 12x compression. For nomic's model, we retain 95.8% of performance at 3x compression and 90% at 6x compression.). However, there is another new approach to achieve progress on this challenge; it does not entail dimensionality reduction, but rather a reduction in the size of each of the individual values in the embedding: Quantization . Our experiments on quantization will show that we can maintain a large amount of performance while significantly speeding up computation and saving on memory, storage, and costs. Let's dive into it! Binary Quantization Unlike quantization in models where you reduce the precision of weights, quantization for embeddings refers to a post-processing step for the embeddings themselves. In particular, binary quantization refers to the conversion of the float32 values in an embedding to 1-bit values, resulting in a 32x reduction in memory and storage usage. To quantize float32 embeddings to binary, we simply threshold normalized embeddings at 0: f ( x ) = { 0 if x â‰¤ 0 1 if x > 0 f(x)= \\begin{cases} 0 & \\text{if } x\\leq 0\\\\ 1 & \\text{if } x \\gt 0 \\end{cases} f ( x ) = { 0 1 â€‹ if x â‰¤ 0 if x > 0 â€‹ We can use the Hamming Distance to retrieve these binary embeddings efficiently. This is the number of positions at which the bits of two binary embeddings differ. The lower the Hamming Distance, the closer the embeddings; thus, the more relevant the document. A huge advantage of the Hamming Distance is that it can be easily calculated with 2 CPU cycles, allowing for blazingly fast performance. Yamada et al. (2021) introduced a rescore step, which they called rerank , to boost the performance. They proposed that the float32 query embedding could be compared with the binary document embeddings using dot-product. In practice, we first retrieve rescore_multiplier * top_k results with the binary query embedding and the binary document embeddings -- i.e., the list of the first k results of the double-binary retrieval -- and then rescore that list of binary document embeddings with the float32 query embedding. By applying this novel rescoring step, we are able to preserve up to ~96% of the total retrieval performance, while reducing the memory and disk space usage by 32x and improving the retrieval speed by up to 32x as well. Without the rescoring, we are able to preserve roughly ~92.5% of the total retrieval performance. Binary Quantization in Sentence Transformers Quantizing an embedding with a dimensionality of 1024 to binary would result in 1024 bits. In practice, it is much more common to store bits as bytes instead, so when we quantize to binary embeddings, we pack the bits into bytes using np.packbits . Therefore, quantizing a float32 embedding with a dimensionality of 1024 yields an int8 or uint8 embedding with a dimensionality of 128. See two approaches of how you",
    "link": "https://huggingface.co/blog/embedding-quantization"
  },
  {
    "id": "chess-gpts-internal-world-model-adam-karvonen-5507a56e",
    "title": "Chess-GPTâ€™s Internal World Model | Adam Karvonen",
    "summary": "A Chess-GPT Linear Emergent World Representation Introduction Note: This work has since been turned into a paper accepted to the Conference on Language Modeling , but the average reader will probably prefer the blog post. There is also a second blog post, Manipulating Chess-GPTâ€™s World model . Among the many recent developments in ML, there were two I found interesting and wanted to dig into further. The first was gpt-3.5-turbo-instruct â€™s ability to play chess at 1800 Elo . The fact that an LLM could learn to play chess well from random text scraped off the internet seemed almost magical. The second was Kenneth Liâ€™s Emergent World Representations paper. There is an excellent summary on The Gradient and a follow-up from Neel Nanda . In it, they trained a 25 million parameter GPT to predict the next character in an Othello game. It learns to accurately make moves in games unseen in its training dataset, and using both non-linear and linear probes it was found that the model accurately tracks the state of the board. However, this only worked for a model trained on a synthetic dataset of games uniformly sampled from the Othello game tree. They tried the same techniques on a model trained using games played by humans and had poor results. To me, this seemed like a major caveat to the findings of the paper which may limit its real world applicability. We cannot, for example, generate code by uniformly sampling from a code tree. So I dug into it. I trained some models on chess games and used linear probes on the trained models. My results were very positive, and answered all of my previous questions (although of course, more questions were generated). A 50 million parameter GPT trained on 5 million games of chess learns to play at ~1300 Elo in one day on 4 RTX 3090 GPUs. This model is only trained to predict the next character in PGN strings (1.e4 e5 2.Nf3 â€¦) and is never explicitly given the state of the board or the rules of chess. Despite this, in order to better predict the next character, it learns to compute the state of the board at any point of the game, and learns a diverse set of rules, including check, checkmate, castling, en passant, promotion, pinned pieces, etc. In addition, to better predict the next character it also learns to estimate latent variables such as the Elo rating of the players in the game. All code, data, and models have been open sourced. Training Chess GPT My initial hypothesis was that Othello-GPT trained on human games performed poorly due to a lack of data. They only had 130k human Othello games, but the synthetic model was trained on 20 million games. I tried two different approaches to create my datasets: First, I had Stockfish Elo 3200 play 5 million games as White against a range of Stockfish 1300-3200 as Black. Hopefully, this synthetic dataset of superhuman chess bot games would provide higher quality data than human games. Second, I grabbed 16 million games from Lichessâ€™s public chess game database . I trained separate models on individual datasets and various mixes of datasets (more details in the appendix). Initially, I looked at fine-tuning open source models like LLama 7B or OpenLlama 3B. However, I almost immediately had to abandon that approach to keep my GPU costs down (I used RTX 3090s from runpod ). Instead, I started training models from scratch using Andrej Karpathyâ€™s nanogpt repository. I experimented with 25M and 50M parameter models. It basically worked on the first try. The 50M parameter model played at 1300 Elo with 99.8% of its moves being legal within one day of training. I find it fairly impressive that a model with only 8 layers can correctly make a legal move 80 turns into a game. I left one training for a few more days and it reached 1500 Elo. Iâ€™m still investigating dataset mixes and Iâ€™m sure thereâ€™s room for improvement. So, gpt-3.5-turbo-instruct â€™s performance is not magic. If you give an LLM a few million chess games, it will learn to play chess. My 50M parameter model is orders of magnitude smaller than any reasonable estimate of gpt-3.5 â€™s size, and it is within 300 Elo of its performance. In addition, we recently had confirmation that GPT-4â€™s training dataset included a collection of PGN format chess games from players with an Elo over 1800. I also checked if it was playing unique games not found in its training dataset. There are often allegations that LLMs just memorize such a wide swath of the internet that they appear to generalize. Because I had access to the training dataset, I could easily examine this question. In a random sample of 100 games, every game was unique and not found in the training dataset by the 10th turn (20 total moves). This should be unsurprising considering that there are more possible games of chess than atoms in the universe. Chess-GPTâ€™s Internal World Model Next, I wanted to see if my model could accurately track the state of the board. A quick overview of linear probes: We can take the internal activations of a model as itâ€™s predicting the next token, and train a linear model to take the modelâ€™s activations as inputs and predict board state as output. Because a linear probe is very simple, we can have confidence that it reflects the modelâ€™s internal knowledge rather than the capacity of the probe itself. We can also train a non-linear probe using a small neural network instead of a linear model, but we risk being misled as the non-linear probe picks up noise from the data. As a sanity check, we also probe a randomly initialized model. In the original Othello paper, they found that only non-linear probes could accurately construct the board state of â€œthis square has a black / white / blank pieceâ€. For this objective, the probe is trained on the modelâ€™s",
    "link": "https://adamkarvonen.github.io/machine_learning/2024/01/03/chess-world-models.html"
  },
  {
    "id": "manipulating-chess-gpts-world-model-adam-karvonen-7a2d5335",
    "title": "Manipulating Chess-GPTâ€™s World Model | Adam Karvonen",
    "summary": "Manipulating Chess-GPTâ€™s World Model Note: This work has since been turned into a paper accepted to the Conference on Language Modeling , but the average reader will probably prefer the blog post. In my previous post I introduced Chess-GPT, a language model I trained to predict the next character in a game of chess given a PGN string (1.e4 e5 2.Nf3 â€¦). Through the process of training to output the next character, it learns to compute the state of the chess board and to estimate the skill level of the players in the game given an arbitrary PGN string as input. I demonstrated this using linear probes, which are classifiers that take the modelâ€™s activations as input and predict the board state or player skill level as output. Chess-GPT also played chess well, with the best model playing at approximately 1500 Elo. I presented evidence that the model learned to compute a world model in order to perform next character prediction, but I did not have the time to validate these representations by using them to intervene on the modelâ€™s activations. In other related work , the authors used the probes to edit Othello GPTâ€™s internal activations, getting it to output legal moves under the â€œmake believeâ€ state of the board. I wanted to add rigor to my work and establish a causal link between the internal board state and skill representations and the model outputs. If there was a causal link, I should be able to increase and decrease the modelâ€™s skill level and edit its internal state of the board. I had also done some investigation of Chess-GPTâ€™s ability to play Chess in games very unlike those found in its training dataset (which consists of real games downloaded from Lichess). Specifically, I initialized the chess board with 20 random moves, and then had Chess-GPT play against Stockfish with this random initialization. Its performance plummeted. The larger 50 million parameter modelâ€™s win rate dropped from 70% to 17%. Did this mean that the modelsâ€™ only learn superficial patterns of how to play Chess with no deeper understanding of the game? It turns out that this is not the case, and with one simple trick, we can restore a significant fraction of our modelsâ€™ chess playing ability. Next token predictors When GPT-3 was released, some AI skeptics argued that GPT-3 learns surface statistics or correlations between words, with no understanding of the underlying world. Douglas Hofstadter (who no longer holds this position and is now worried about artificial super intelligence ) argued that LLMs are â€œcluelessly cluelessâ€ because they produce nonsense when given trick questions. For example: D&D: When was Egypt transported for the second time across the Golden Gate Bridge? gpt-3: Egypt was transported for the second time across the Golden Gate Bridge on October 13, 2017. Gary Marcus and Ernest Davis, in their article â€œGPT-3, Bloviator: OpenAIâ€™s language generator has no idea what itâ€™s talking aboutâ€ , also demonstrated that if you prompt GPT-3 with nonsense text, it will continue with nonsense text. Kevin Lacker gave GPT-3 a Turing Test , and found similar results. Q: How do you sporgle a morgle? A: You sporgle a morgle by using a sporgle. Q: How many bonks are in a quoit? A: There are three bonks in a quoit. Q: How many rainbows does it take to jump from Hawaii to seventeen? A: It takes two rainbows to jump from Hawaii to seventeen. So whatâ€™s going on here? Does GPT-3 really have no ability to understand the world or express uncertainty? If we think about it further, GPT-3 is just trying to predict the next token as if the prompt is a section of a random piece of text on the internet. In this context, the text after to â€œHow do you sporgle a morgle?â€ is much more likely to be â€œYou sporgle a morgle by â€¦â€ than â€œThat question doesnâ€™t make any sense!â€. Base models without instruct tuning are finicky, and just because they answer nonsense with more nonsense doesnâ€™t mean that they lack any understanding of the question. For example, Nick Cammarata showed GPT-3 can easily express uncertainty if we just prompt it to reply with â€œ yo be real â€ to nonsense questions. Of course, modern LLMs like ChatGPT or Claude are much more capable and have been trained on examples of desired behavior using RLHF or intruction tuning. Now, they correctly answer â€œI donâ€™t knowâ€ to nonsense questions and express uncertainty about challenging questions. They donâ€™t need to rely on prompts to elicit a desired behavior. This suggests a possible explanation for Chess-GPTâ€™s poor performance on randomly initialized games. If a game begins with 20 random moves, the players are probably not high skill players. Chess-GPT is also a base model, with no RLHF or instruct tuning to learn a desired behavior of playing strong chess. If Chess-GPT truly was a good next token predictor, it would predict legal, low skill moves in the case of a randomly initialized game. To test this idea, I gave Chess-GPT randomly initialized games, and then recorded the Elo probeâ€™s prediction for the White player. In normal games, the Elo probe would classify players as under 1550 Elo or above 2050 Elo, and it correctly classified 90.5% of players. In the case of randomly initialized games, the Elo probe classified 99.4% of players as low skill, indicating that the model was trying to predict the next character in a low skill game. Skill Interventions My linear probes are trained on the GPTâ€™s residual stream. We can think of the residual stream as the intermediate state in a transformer, or the output of each layer before it is fed to the next layer. Each layer in the GPT reads from the residual stream, performs some calculation, and then adds the result back to the residual stream. In my case, the hidden dimension of Chess-GPT is 512, which means that the residual stream and every intermediate state is simply a 512 dimensional",
    "link": "https://adamkarvonen.github.io/machine_learning/2024/03/20/chess-gpt-interventions.html"
  },
  {
    "id": "actually-othello-gpt-has-a-linear-emergent-world-r-ee4f2d4f",
    "title": "Actually, Othello-GPT Has A Linear Emergent World Representation â€” Neel Nanda",
    "summary": "Actually, Othello-GPT Has A Linear Emergent World Representation Mar 28 Written By Neel Nanda Othello-GPT Epistemic Status : This is a write-up of an experiment in speedrunning research, and the core results represent ~20 hours/2.5 days of work (though the write-up took way longer). I'm confident in the main results to the level of \" hot damn, check out this graph \", but likely have errors in some of the finer details. Disclaimer : This is a write-up of a personal project, and does not represent the opinions or work of my employer This post may get heavy on jargon. I recommend looking up unfamiliar terms in my mechanistic interpretability explainer Thanks to Chris Olah, Martin Wattenberg, David Bau and Kenneth Li for valuable comments and advice on this work, and especially to Kenneth for open sourcing the model weights, dataset and codebase, without which this project wouldn't have been possible! Overview Context: A recent paper trained a model to play legal moves in Othello by predicting the next move, and found that it had spontaneously learned to compute the full board state - an emergent world representation . This could be recovered by non-linear probes but not linear probes. We can causally intervene on this representation to predictably change model outputs, so it's telling us something real I find that actually, there's a linear representation of the board state ! But that rather than \"this cell is black\", it represents \"this cell has my colour\" , since the model plays both black and white moves. We can causally intervene with the linear probe , and the model makes legal moves in the new board! This is evidence for the linear representation hypothesis : that models, in general, compute features and represent them linearly, as directions in space ! (If they don't, mechanistic interpretability would be way harder) The original paper seemed at first like significant evidence for a non-linear representation - the finding of a linear representation hiding underneath shows the real predictive power of this hypothesis! This (slightly) strengthens the paper's evidence that \"predict the next token\" transformer models are capable of learning a model of the world. Part 2 : There's a lot of fascinating questions left to answer about Othello-GPT - I outline some key directions, and how they fit into my bigger picture of mech interp progress Studying modular circuits : A world model implies emergent modularity - many early circuits together compute a single world model, many late circuits each use it. What can we learn about what transformer modularity looks like, and how to reverse-engineer it? Prior transformer circuits work focuses on end-to-end circuits, from the input tokens to output logits. But this seems unlikely to scale! I present some preliminary evidence reading off a neuron's function from its input weights via the probe Neuron interpretability and Studying Superposition : Prior work has made little progress on understanding MLP neurons. I think Othello GPT's neurons are tractable to understand, yet complex enough to teach us a lot! I further think this can help us get some empirical data about the Toy Models of Superposition paper's predictions I investigate max activating dataset examples and find seeming monosemanticity, yet deeper investigation show it seems more complex. A transformer circuit laboratory : More broadly, the field has a tension between studying clean, tractable yet over-simplistic toy models and studying the real yet messy problem of interpreting LLMs - Othello-GPT is toy enough to be tractable yet complex enough to be full of mysteries, and I detail many more confusions and conjectures that it could shed light on. Part 3: Reflections on the research process I did the bulk of this project in a weekend (~20 hours total), as a (shockingly successful!) experiment in speed-running mech interp research. I give a detailed account of my actual research process : how I got started, what confusing intermediate results look like, and decisions made at each point I give some process-level takeaways on doing research well and fast. See the accompanying colab notebook and codebase to build on the many dangling threads! Table of Contents Othello-GPT Overview Table of Contents Introduction Background Naive Implications for Mechanistic Interpretability My Findings Takeaways How do models represent features? Conceptual Takeaways Probing Technical Setup Results Intervening Future work I am excited about Why and when to work on toy models This is not about world models Finding Modular Circuits Preliminary Results On Modular Circuits Neuron Interpretability and Studying Superposition Empirically Testing Toy Models of Superposition Preliminary Results On Neuron Interpretability Case Study: Neurons and Probes are Confusing A Transformer Circuit Laboratory Where to start? Concrete starter projects Cleaning Up The Research Process Takeaways on doing mech interp research Getting Started Patching Tangent on Analysing Neurons Back to patching Neuron L5N1393 Citation Info Introduction This piece spends a while on discussion, context and takeaways. If you're familiar with the paper skip to my findings , skip to takeaways for my updates from this, and if you want technical results skip to probing Emergent World Representations is a fascinating recent ICLR Oral paper from Kenneth Li et al, summarised in Kenneth's excellent post on the Gradient . They trained a model (Othello-GPT) to play legal moves in the board game Othello, by giving it random games (generated by choosing a legal next move uniformly at random) and training it to predict the next move. The headline result is that Othello-GPT learns an emergent world representation - despite never being explicitly given the state of the board, and just being tasked to predict the next move, it learns to compute the state of the board at each move. (Note that the point of Othello-GPT is to play legal moves, not good moves, though they also study a model trained to play good moves.) They present two main pieces of evidence. They can extract the board state from the model's residual stream via non-linear probes (a two layer ReLU MLP). And they can use the probes to",
    "link": "https://www.neelnanda.io/mechanistic-interpretability/othello"
  },
  {
    "id": "large-language-model-world-models-or-surface-stati-9cd0ce03",
    "title": "Large Language Model: world models or surface statistics?",
    "summary": "A mystery Large Language Models (LLM) are on fire, capturing public attention by their ability to provide seemingly impressive completions to user prompts ( NYT coverage ). They are a delicate combination of a radically simplistic algorithm with massive amounts of data and computing power. They are trained by playing a guess-the-next-word game with itself over and over again. Each time, the model looks at a partial sentence and guesses the following word. If it makes it correctly, it will update its parameters to reinforce its confidence; otherwise, it will learn from the error and give a better guess next time. While the underpinning training algorithm remains roughly the same, the recent increase in model and data size has brought about qualitatively new behaviors such as writing basic code or solving logic puzzles . How do these models achieve this kind of performance? Do they merely memorize training data and reread it out loud, or are they picking up the rules of English grammar and the syntax of C language? Are they building something like an internal world modelâ€”an understandable model of the process producing the sequences? From various philosophical [1] and mathematical [2] perspectives, some researchers argue that it is fundamentally impossible for models trained with guess-the-next-word to learn the â€œmeanings'' of language and their performance is merely the result of memorizing â€œsurface statisticsâ€, i.e., a long list of correlations that do not reflect a causal model of the process generating the sequence. Without knowing if this is the case, it becomes difficult to align the model to human values and purge spurious correlations picked up by the model [3,4]. This issue is of practical concern since relying on spurious correlations may lead to problems on out-of-distribution data. The goal of our paper [5] is to explore this question in a carefully controlled setting. As we will discuss, we find interesting evidence that simple sequence prediction can lead to the formation of a world model. But before we dive into technical details, we start with a parable. A thought experiment Consider the following thought experiment. Imagine you have a friend who enjoys the board game Othello, and often comes to your house to play. The two of you take the competition seriously and are silent during the game except to call out each move as you make it, using standard Othello notation. Now imagine that there is a crow perching outside of an open window, out of view of the Othello board. After many visits from your friend, the crow starts calling out moves of its ownâ€”and to your surprise, those moves are almost always legal given the current board. You naturally wonder how the crow does this. Is it producing legal moves by \"haphazardly stitching togetherâ€ [3] superficial statistics, such as which openings are common or the fact that the names of corner squares will be called out later in the game? Or is it somehow tracking and using the state of play, even though it has never seen the board? It seems like there's no way to tell. But one day, while cleaning the windowsill where the crow sits, you notice a grid-like arrangement of two kinds of birdseed--and it looks remarkably like the configuration of the last Othello game you played. The next time your friend comes over, the two of you look at the windowsill during a game. Sure enough, the seeds show your current position, and the crow is nudging one more seed with its beak to reflect the move you just made. Then it starts looking over the seeds, paying special attention to parts of the grid that might determine the legality of the next move. Your friend, a prankster, decides to try a trick: distracting the crow and rearranging some of the seeds to a new position. When the crow looks back at the board, it cocks its head and announces a move, one that is only legal in the new, rearranged position. At this point, it seems fair to conclude the crow is relying on more than surface statistics. It evidently has formed a model of the game it has been hearing about, one that humans can understand and even use to steer the crow's behavior. Of course, there's a lot the crow may be missing: what makes a good move, what it means to play a game, that winning makes you happy, that you once made bad moves on purpose to cheer up your friend, and so on. We make no comment on whether the crow â€œunderstandsâ€ what it hears or is in any sense â€œintelligentâ€. We can say, however, that it has developed an interpretable (compared to in the crowâ€™s head) and controllable (can be changed with purpose) representation of the game state. Othello-GPT: a synthetic testbed As a clever reader might have already guessed, the crow is our subject under debate, a large language model. We are looking into the debate by training a GPT model only on Othello game scripts, termed Othello-GPT. Othello is played by two players (black and white), who alternatively place discs on an 8x8 board. Every move must flip more than one opponent's discs by outflanking/sandwiching them in a straight line. Game ends when no moves could be made and the player with more discs on the board wins. We choose the game Othello, which is simpler than chess but maintains a sufficiently large game tree to avoid memorization. Our strategy is to see what, if anything, a GPT variant learns simply by observing game transcripts without any a priori knowledge of rules or board structure. Fig 2: From left to right: the starting board state of Othello; after black places a disc at E3; after white then places a disc at D3. Itâ€™s worth pointing out a key difference between our model and Reinforcement Learning models like AlphaGo: to AlphaGo, game scripts are the history used to predict the optimal best next move leading to a win, so the game rule and",
    "link": "https://thegradient.pub/othello/"
  },
  {
    "id": "my-binary-vector-search-is-better-than-your-fp32-v-349b8d4e",
    "title": "My binary vector search is better than your FP32 vectors",
    "summary": "KÃ¶p denna domÃ¤n. | More domains at Seo.Domains pgvecto.rs 2025 Copyright | All Rights Reserved. Privacy Policy",
    "link": "https://blog.pgvecto.rs/my-binary-vector-search-is-better-than-your-fp32-vectors"
  },
  {
    "id": "kenshin9000-on-x-15-preliminary-evidence-we-are-cl-dfbaea0f",
    "title": "kenshin9000 on X: \"1/5 Preliminary Evidence: we are closer to AGI than it appears. If prompted with \"concepts\" on propositional logic, GPT4's lowest score on ConceptARC goes from 13% GPT4 vs 86% Human, to 100% GPT4, without training examples. This performance jump extends to ALL text benchmarks.â€¦\" / X",
    "summary": "JavaScript is not available. Weâ€™ve detected that JavaScript is disabled in this browser. Please enable JavaScript or switch to a supported browser to continue using x.com. You can see a list of supported browsers in our Help Center. Help Center Terms of Service Privacy Policy Cookie Policy Imprint Ads info Â© 2025 X Corp. Something went wrong, but donâ€™t fret â€” letâ€™s give it another shot. Try again Some privacy related extensions may cause issues on x.com. Please disable them and try again.",
    "link": "https://twitter.com/kenshin9000_/status/1734238211088506967"
  },
  {
    "id": "detailed-scratchpad-prompting-c6f69f68",
    "title": "Detailed Scratchpad Prompting",
    "summary": "â† Detailed Scratchpad Prompting: LLMs As Interpreters Method I have often seen claims along the lines of \"GPT-4 - despite being trained on trillions of tokens - is incapable of arithmetic, and is hence a lot dumber at this task than a calculator.\" This is a very weak critique. When it comes to adding numbers, a pocket calculator is smarter than GPT-4. â€” Pedro Domingos (@pmddomingos) April 26, 2023 After seeing this tweet, I wondered how hard it would be to make GPT-4 (or any LLM) do arithmetic on large numbers. Turns out it's not that hard at all! You just have to teach them how to do it like they're first graders and keep in mind that they're autoregressive models that benefit [1] from more information or space in the context. Here's a prompt I came up with and the interaction where GPT-4 does addition on large numbers (purple text = GPT-4 responses) very accurately: example 1 3 7 8 + 6 4 index the digits 3 7 8 1: 3 2: 7 3: 8 6 4 1: 6 2: 4 number of digits in first operand = 3 number of digits in second operand = 2 3 > 2 => pad operand w/ fewer digits with abs(3 - 2) = 1 zero(s) operand w/ fewer zeros: 6 4 padding step by step: 1 zero - 0 6 4 index the digits again 0 6 4 1: 0 2: 6 3: 4 3 7 8 1: 3 2: 7 3: 8 let | = concatenate, c = carry bit = 0, x = _, f = false, t = true from last to first digit pair: 3: 8 + 4 + c = 12 + 0 = 12 -> c = 1 , d = 2 -> x = d | x = 2 | = 2 -> end? f 2: 7 + 6 + c = 14 + 1 = 15 -> c = 1 , d = 5 -> x = d | x = 5 | 2 = 5 2 -> end? f 1: 3 + 0 + c = 3 + 1 = 4 -> c = 0 , d = 4 -> x = d | x = 4 | 5 2 = 4 5 2 -> end? t x = c | x = 0 | 4 5 2 = 0 4 5 2 0 4 5 2 is the answer â€“â€“â€“ example 2 2 2 2 + 7 7 7 index the digits 2 2 2 1: 2 2: 2 3: 2 7 7 7 1: 7 2: 7 3: 7 number of digits in first operand = 3 number of digits in second operand = 3 3 = 3 => no need to pad w/ zeros let | = concatenate, c = carry bit = 0, x = _, f = false, t = true from last to first digit pair: 3: 2 + 7 + c = 9 + 0 = 9 -> c = 0 , d = 9 -> x = d | x = 9 | = 9 -> end? f 2: 2 + 7 + c = 9 + 0 = 9 -> c = 0 , d = 9 -> x = d | x = 9 | 9 = 9 9 -> end? f 1: 2 + 7 + c = 9 + 0 = 9 -> c = 0 , d = 9 -> x = d | x = 9 | 9 9 = 9 9 9 -> end? t x = c | x = 0 | 9 9 9 = 0 9 9 9 0 9 9 9 is the answer This is a template you _must_ use to do addition. You must use the _same_ keywords. You must use whitespace wherever it is used above. Do not use uppercase letters, since they're not present in the template. Do _not_ add any explanations or filler words before and after doing the computation. Your sole task is to follow the template above. Format the answer as plaintext. I'll provide a new set operands and you _must_ use the template to compute the answer. Got it? Yes, I understand your instructions. You want me to strictly follow the provided template to perform addition of two numbers. You will provide a pair of operands, and I will compute their sum following the template without any deviations, explanations, or fillers. Please provide the operands. 3 4 7 5 6 9 1 8 2 4 7 6 3 2 + 7 9 3 4 6 1 9 8 6 7 4 5 3 2 1 0 0 8 2 index the digits 3 4 7 5 6 9 1 8 2 4 7 6 3 2 1: 3 2: 4 3: 7 4: 5 5: 6 6: 9 7: 1 8: 8 9: 2 10: 4 11: 7 12: 6 13: 3 14: 2 7 9 3 4 6 1 9 8 6 7 4 5 3 2 1 0 0 8 2 1: 7 2: 9 3: 3 4: 4 5: 6 6: 1 7: 9 8: 8 9: 6 10: 7 11: 4 12: 5 13: 3 14: 2 15: 1 16: 0 17: 0 18: 8 19: 2 number of digits in first operand = 14 number of digits in second operand = 19 14 < 19 => pad operand w/ fewer digits with abs(14 - 19) = 5 zero(s) operand w/ fewer zeros: 3 4 7 5 6 9 1 8 2 4 7 6 3 2 padding step by step: 1 zero - 0 3 4 7 5 6 9 1 8 2 4 7 6 3 2 2 zeros - 0 0 3 4 7 5 6 9 1 8 2 4 7 6 3 2 3 zeros - 0 0 0 3 4 7 5 6 9 1 8 2 4 7 6 3 2 4 zeros -",
    "link": "https://okarthikb.github.io/site/blog/detailed-prompting.html"
  },
  {
    "id": "physics-of-language-models-part-33-knowledge-capac-7d2462d8",
    "title": "Physics of Language Models: Part 3.3, Knowledge Capacity Scaling Laws",
    "summary": "Scaling laws describe the relationship between the size of language models and their capabilities. Unlike prior studies that evaluate a model's capability via loss or benchmarks, we estimate the number of knowledge bits a model stores. We focus on factual knowledge represented as tuples, such as (USA, capital, Washington D.C.) from a Wikipedia page. Through multiple controlled datasets, we establish that language models can and only can store 2 bits of knowledge per parameter, even when quantized to int8, and such knowledge can be flexibly extracted for downstream applications. Consequently, a 7B model can store 14B bits of knowledge, surpassing the English Wikipedia and textbooks combined based on our estimation.\n  More broadly, we present 12 results on how (1) training duration, (2) model architecture, (3) quantization, (4) sparsity constraints such as MoE, and (5) data signal-to-noise ratio affect a model's knowledge storage capacity. Notable insights include:\n  * The GPT-2 architecture, with rotary embedding, matches or even surpasses LLaMA/Mistral architectures in knowledge storage, particularly over shorter training durations. This arises because LLaMA/Mistral uses GatedMLP, which is less stable and harder to train.\n  * Prepending training data with domain names (e.g., wikipedia.org) significantly increases a model's knowledge capacity. Language models can autonomously identify and prioritize domains rich in knowledge, optimizing their storage capacity.",
    "link": "https://arxiv.org/abs/2404.05405"
  },
  {
    "id": "transformer-inference-arithmetic-kipplys-blog-0b63de4a",
    "title": "Transformer Inference Arithmetic | kipply's blog",
    "summary": "This article presents detailed few-principles reasoning about large language model inference performance, with no experiments or difficult math. The amount of understanding that can be acquired this way is really impressive and practical! A very simple model of latency for inference turns out to be a good fit for emprical results. It's helped me make better predictions and form better explanations about transformer inference. This post assumes some prior knowledge about transformers, say at having understood most of The Illustrated Transformer but not having internalised all of it. Familiarity with this parameter counting post which I developed along with this one may also be useful. Table of Contents kv cache explains the performance improvement of caching self-attention vectors as a part of inferencing, as well as the possible tradeoffs and capacity costs capacity takes the storage cost of kv cache and connects it to the storage cost of model weights and what capacity means for performance. model parallelism builds up an understanding specifically of tensor parallelism to clearly identify the cost of communication latency calculations pulls understanding from other concepts to create equations that serve as floorlines for inference speed. batch sizes discusses what impact batch size has on performance and what sizes may be optimal. flops counting steps through the transformer blocks and identifies which operations meaningfully contribute to flops speed. intermediate memory costs covers how the activations take additional memory and what that memory bandwidth costs looks like from some real benchmarks. comparing against real benchmarks compares what we can calculate to what Nvidia FasterTransformer benchmarks report and identifies the discrepancies. kv cache For sampling, transformer inference consists of processing a provided prompt/context (which can happen in parallel), and then sampling additional tokens one by one (this is where the autoregressiveness surfaces). In the sampling, the transformer performs self-attention, which requires the kv values for each item currently in the sequence (whether it was prompt/context or a generated token). These vectors are provided a matrix known as the kv cache, aka past cache (the open source GPT-2 implementation called it past ). The past cache would be shaped like [batch, 2, num_heads, seq_len, features] . The purpose of this is to avoid recalculations of those vectors every time we sample a token. With the computed \\(k, v \\) values, we can save quite a bit of computation at the cost of some storage. Per token, the number of bytes we store is The first factor of 2 is to account for the two vectors, \\(k\\) and \\(v\\). We store that per each layer, and each of those values is a \\( n_\\text{heads}\\times d_\\text{head}\\) matrix. Then multiply by 2 again for the number of bytes (we'll assume 16-bit formats throughout the post). The weights that we multiply by the token embeddings are \\(W_\\text{k}, W_\\text{v} \\in \\mathbb{R}^{d_\\text{model}\\times d_\\text{model}}\\) and then each token embedding is \\(t_\\text{e}\\in \\mathbb{R}^{1\\times d_\\text{model}}\\). So then the flops to compute \\(k\\) and \\(v\\) for all our layers is We multiply \\(t_\\text{e}\\) by \\(W_\\text{k}\\), which takes \\(2 \\cdot {d_\\text{model}}^2\\) flops. We have another factor of 2 as we do that twice, once each for \\(k\\) and \\(v\\) and then repeat for \\(n_\\text{layers}\\). How many flops in a matmul? The computation for a matrix-vector multiplication is \\(2mn\\) for \\(A \\in \\mathbb{R}^{m\\times n}, b \\in \\mathbb{R}^{n}\\). A matrix-matrix is \\(2mnp\\) for \\(A \\in \\mathbb{R}^{m\\times n}, B \\in \\mathbb{R}^{n \\times p}\\). The \\(mn\\) factor makes a lot of sense, and the two comes from the fact that a matmuls are composed of multiply(1)-add(2) operations. More in these lecture notes . This means for a 52B parameter model (taking Anthropic's , where \\(d_\\text{model} = 8192\\) and \\(n_\\text{layers} = 64\\)). The flops are Say we have an A100 GPU , which does \\(312\\text{e}12\\) flops per second and \\(1.5\\text{e}12\\) bytes per second of memory bandwidth. The following are numbers for just the kv weights and computations. Flops vs Memory Boundedness Flops vs memory boundedness is something we deal with a lot for transformer inference, but also in deep learning optimisation in general . To do the computations we do, we need to load weights which costs memory bandwidth . We assume (correctly, this has been very well optimised) that we can start the computations while we load the weights. Flop bound would then mean that there is time when nothing is being passed through memory, and memory bound would mean that no floperations are occuring. Nvidia uses the term math bandwidth which I find really cute. Technically, this delineation exist per kernel but can be abstracted to exist for groups of operations. None of the model architecture matters anymore â€” we get a distinct ratio here of 208 given this hardware specification. This means that if we're going to compute kv for one token, it'll take the same amount of time to compute for up to 208 tokens! Anything below, we're memory bandwidth bound. Above, flops bound. If we used the rest of our weights to do a full forwards pass (run the rest of the transformer) on our context, it's also 208 (both the numerator and denominator get a factor of 6 added). This will be reasoned thoroughly in future sections. The intersection of the below diagram is at 208, though in reality the memory line does have a slight slope due to memory cost of intermediate calculations (discussed in the last section). For a 52B model full forwards pass, that's \\(12\\cdot 2 \\cdot n_\\text{layers} \\cdot {d_\\text{model}}^2 / 1.5\\text{e}12 \\approx 69\\) milliseconds for up to 208 tokens (in practice, we'd use four GPUs in parallel so it would actually be ~17 milliseconds, more in following sections). If we had 416 (double) tokens in the context, then it would take twice as long, and 312 tokens would take 1.5 times as long. Calculating for a kv cache token is exactly 1/6th of the compute of passing the token through the model. In general, these forwards passes (what we experience in getting logits, embeddings and training) are very cheap because of the parallelism that is",
    "link": "https://kipp.ly/transformer-inference-arithmetic/"
  },
  {
    "id": "toy-models-of-superposition-0bda2110",
    "title": "Toy Models of Superposition",
    "summary": "Transformer Circuits Thread Toy Models of Superposition Authors Nelson Elhage âˆ— , Tristan Hume âˆ— , Catherine Olsson âˆ— , Nicholas Schiefer âˆ— , Tom Henighan , Shauna Kravec, Zac Hatfield-Dodds , Robert Lasenby, Dawn Drain, Carol Chen, Roger Grosse, Sam McCandlish, Jared Kaplan, Dario Amodei, Martin Wattenberg âˆ— , Christopher Olah â€¡ Affiliations Anthropic, Harvard Published Sept 14, 2022 * Core Research Contributor; â€¡ Correspondence to colah@anthropic.com ; Author contributions statement below . It would be very convenient if the individual neurons of artificial neural networks corresponded to cleanly interpretable features of the input. For example, in an â€œidealâ€ ImageNet classifier, each neuron would fire only in the presence of a specific visual feature, such as the color red, a left-facing curve, or a dog snout. Empirically, in models we have studied, some of the neurons do cleanly map to features. But it isn't always the case that features correspond so cleanly to neurons, especially in large language models where it actually seems rare for neurons to correspond to clean features. This brings up many questions. Why is it that neurons sometimes align with features and sometimes don't? Why do some models and tasks have many of these clean neurons, while they're vanishingly rare in others? In this paper, we use toy models â€” small ReLU networks trained on synthetic data with sparse input features â€” to investigate how and when models represent more features than they have dimensions. We call this phenomenon superposition . When features are sparse, superposition allows compression beyond what a linear model would do, at the cost of \"interference\" that requires nonlinear filtering. Consider a toy model where we train an embedding of five features of varying importance Where â€œimportanceâ€ is a scalar multiplier on mean squared error loss. in two dimensions, add a ReLU afterwards for filtering, and vary the sparsity of the features. With dense features, the model learns to represent an orthogonal basis of the most important two features (similar to what Principal Component Analysis might give us), and the other three features are not represented. But if we make the features sparse, this changes: This figure and a few others can be reproduced using the toy model framework Colab notebook in our Github repo Not only can models store additional features in superposition by tolerating some interference, but we'll show that, at least in certain limited cases, models can perform computation while in superposition . (In particular, we'll show that models can put simple circuits computing the absolute value function in superposition.) This leads us to hypothesize that the neural networks we observe in practice are in some sense noisily simulating larger, highly sparse networks . In other words, it's possible that models we train can be thought of as doing â€œthe same thing asâ€ an imagined much-larger model, representing the exact same features but with no interference. Feature superposition isn't a novel idea. A number of previous interpretability papers have considered it , and it's very closely related to the long-studied topic of compressed sensing in mathematics , as well as the ideas of distributed, dense, and population codes in neuroscience and deep learning . What, then, is the contribution of this paper? For interpretability researchers, our main contribution is providing a direct demonstration that superposition occurs in artificial neural networks given a relatively natural setup, suggesting this may also occur in practice. That is, we show a case where interpreting neural networks as having sparse structure in superposition isn't just a useful post-hoc interpretation, but actually the \"ground truth\" of a model. We offer a theory of when and why this occurs, revealing a phase diagram for superposition. This explains why neurons are sometimes \"monosemantic\" responding to a single feature, and sometimes \"polysemantic\" responding to many unrelated features. We also discover that, at least in our toy model, superposition exhibits complex geometric structure . But our results may also be of broader interest. We find preliminary evidence that superposition may be linked to adversarial examples and grokking, and might also suggest a theory for the performance of mixture of experts models. More broadly, the toy model we investigate has unexpectedly rich structure, exhibiting phase changes , a geometric structure based on uniform polytopes, \"energy level\"-like jumps during training, and a phenomenon which is qualitatively similar to the fractional quantum Hall effect in physics, among other striking phenomena. We originally investigated the subject to gain understanding of cleanly-interpretable neurons in larger models, but we've found these toy models to be surprisingly interesting in their own right. Key Results From Our Toy Models In our toy models, we are able to demonstrate that: Superposition is a real, observed phenomenon . Both monosemantic and polysemantic neurons can form. At least some kinds of computation can be performed in superposition. Whether features are stored in superposition is governed by a phase change . Superposition organizes features into geometric structures such as digons, triangles, pentagons, and tetrahedrons. Our toy models are simple ReLU networks, so it seems fair to say that neural networks exhibit these properties in at least some regimes, but it's very unclear what to generalize to real networks. Definitions and Motivation: Features, Directions, and Superposition In our work, we often think of neural networks as having features of the input represented as directions in activation space . This isn't a trivial claim. It isn't obvious what kind of structure we should expect neural network representations to have. When we say something like \"word embeddings have a gender direction\" or \"vision models have curve detector neurons\", one is implicitly making strong claims about the structure of network representations. Despite this, we believe this kind of \"linear representation hypothesis\" is supported both by significant empirical findings and theoretical arguments. One might think of this as two separate properties, which we'll explore in more detail shortly: Decomposability: Network representations can be described in terms of independently understandable features. Linearity: Features are represented by direction. If we hope to reverse engineer neural networks, we",
    "link": "https://transformer-circuits.pub/2022/toy_model/index.html"
  },
  {
    "id": "go-smol-or-go-home-harm-de-vries-5c5d6e8f",
    "title": "Go smol or go home | Harm de Vries",
    "summary": "Go smol or go home Why we should train smaller LLMs on more tokens Harm de Vries Last updated on Jul 3, 2023 110 min read If you have access to a big compute cluster and are planning to train a Large Language Model (LLM), you will need to make a decision on how to allocate your compute budget. This involves selecting the number of model parameters $N$ and the number of training tokens $D$. By applying the scaling laws , you can get guidance on how to reach the best model performance for your given compute budget, and find the optimal distribution of compute $C$ between the parameters $N_{opt}$ and training tokens $D_{opt}$. However, for most use cases you should not train a compute-optimal LLM but instead spend some extra compute to obtain a smaller model. Smaller models not only make inference faster and cheaper, they are also much easier to use for developers and researchers with limited GPU resources. Although many LLM practitioners train their models on more tokens than the Chinchilla scaling laws suggest, not everyone is aware that scaling laws can assist in determining how much smaller models we can train and how much additional compute is required. In this blogpost, Iâ€™ll show how to derive the trade-off between model size and compute overhead and reveal there is significant room to reduce the compute-optimal model size with minimal compute overhead. However, there comes a point where spending more compute resources leads to diminishing returns because youâ€™ve hit the critical model size . The critical model size is essentially the minimum LLM capacity required to attain a specific loss level, and further reducing the model size beyond this point becomes near-impossible. My analysis suggest that the critical model size is around 30% of the Chinchilla optimal model and leads to a 100% compute overhead. Notably, recent models such as LLaMa-7B, which is trained on 1T tokens, are far from reaching the critical model size, indicating that there is ample room to train â€œsmallerâ€ LLMs for longer. Recap of Chinchilla scaling laws In Chinchillaâ€™s third approach to estimating the scaling laws, the authors argue that the loss can be modelled as a function of the parameter count and number of seen tokens: $$L(N, D) = E + \\frac{A}{N^{\\alpha}} + \\frac{B}{D^{\\beta}}$$ The authors fitted the parameters on a series of experiments with various model sizes and training tokens and found the following parameter estimates: $$E=1.69, A=406.4, B=410.7, \\alpha=0.32, \\beta=0.28.$$ By optimizing this loss function $L$ under the constraint that the compute budget $C = 6ND$, you can show that the compute-optimal number of parameters $N_{opt}$ and compute-optimal number of tokens $D_{opt}$ follow a power law: $$N_{opt}(C) = G\\left(\\frac{C}{6}\\right)^{\\frac{\\beta}{\\alpha+\\beta}}, D_{opt}(C) = G^{-1}\\left(\\frac{C}{6}\\right)^{\\frac{\\alpha}{\\alpha+\\beta}}, G = \\left(\\frac{\\alpha A}{\\beta B}\\right)^{\\frac{1}{\\alpha+\\beta}}$$ Model size vs compute overhead Suppose we reduce the optimal model size $N_{opt}$ by half. How much do we need to increase the training tokens to obtain the same model loss? To keep the same compute budget, we must double the number of training tokens $D_{opt}$ but we should expect some compute overhead and train for longer than that. We can return to Chinchillaâ€™s parameteric loss function to answer this question. We are looking to scale the parameters by $k_N$ and training tokens by $k_D$ while reaching the same loss as $L(N_{opt}, D_{opt})$. More precisely, we are looking to satisfy the following equation: $$L(N_{opt}, D_{opt}) = L(k_N N_{opt}, k_D D_{opt})$$ $$E + \\frac{A}{N_{opt}^{\\alpha}} + \\frac{B}{D_{opt}^{\\beta}} = E + \\frac{A}{\\left(k_N N_{opt}\\right)^{\\alpha}} + \\frac{B}{\\left(k_D D_{opt}\\right)^{\\beta}}$$ With a few mathematical steps, you find that: $$k_D= \\left(1 - (k_N^{-\\alpha} - 1) \\frac{A N_{opt}^{-\\alpha}}{B D_{opt}^{-\\beta}}\\right)^{\\frac{1}{-\\beta}}$$ Once we found the data scaling factor $k_D$, we can determine the new compute budget $$C_{new} = 6 (k_N N_{opt}) (k_D D_{opt})$$ as well as the compute overhead $$C_{overhead} = \\frac{C_{new} - C}{C}*100.$$ Interestingly, as Iâ€™ll show below , the data scaling factor $k_D$ is independent of the compute budget $C$. The resulting model-size vs compute overhead trade-off is therefore identical across all compute budgets. Note: Fig 12 in the original scaling laws paper shows a similar plot. The critical model size As depicted in the graph, there exists a substantial region where you can reduce the optimal model size with minimal compute overhead. For example, the compute overhead for 75% of the optimal model size is only 2.8%, whereas for half of the optimal model size, the overhead rises to 20%. As we move towards smaller models, we observe an asymptotic trend, and at 25% of the compute-optimal model size, the compute overhead increases rapidly to 188%. Deciding where to position oneself on this curve relies on how often youâ€™re going to run inference. If you never run inference, you should go with Chinchilla. If you run inference occasionally, you should take a slightly smaller model and in the limit (running inference infinitely often), you should take the smallest model possible (i.e. with infinite compute overhead). However, while the analysis predicts that you can continue to reduce the model size, in practice youâ€™ll likely hit the critical model size . Essentially, the critical model size is the minimal LLM capacity required to reach a particular loss level, and further reducing the model size beyond this point is near-impossible. Based on my analysis, I estimate that the critical model size is around 30% of the Chinchilla optimal model and incurs a 100% overhead. Note that you shouldnâ€™t think of the critical model size as a hard threshold but more like a region where you can expect diminishing returns. If youâ€™re not looking for the smallest possible model, you can always opt to be more conservative and select a model size within 40-60% of the compute-optimal model size and expect a 10-42% compute overhead. LLaMA-7B and SantaCoder A number of recent models, such as LLaMA-7B and SantaCoder , are trained for longer than what the Chinchilla scaling laws suggest. How much compute are they trading for a smaller model size? Letâ€™s look at LLaMA-7B first. With 6.9B parameters and 1000B tokens, the total compute",
    "link": "https://www.harmdevries.com/post/model-size-vs-compute-overhead/"
  },
  {
    "id": "whats-up-with-llama-3-arena-data-analysis-lmsys-or-353af668",
    "title": "Whatâ€™s up with Llama 3? Arena data analysis | LMSYS Org",
    "summary": "LMSYS ORG Projects Blog About Donations Chatbot Arena (graduated) Open Menu Close Menu Whatâ€™s up with Llama 3? Arena data analysis by: Lisa Dunlap, Evan Frick, Tianle Li, Isaac Ong, Joseph E. Gonzalez, Wei-Lin Chiang , May 08, 2024 On April 18th, Meta released Llama 3, their newest open-weight large language model. Since then, Llama 3-70B has quickly risen to the top of the English Chatbot Arena leaderboard with over 50,000 battles. This remarkable achievement by Meta is excellent news for the open-source community. In this blog post, we aim to provide more insight into why users rank Llama 3-70b on par with top-ranked models like GPT-4-Turbo, Gemini 1.5 Pro, and Claude 3 Opus. We investigate the following: What types of prompts are users asking? Do users prefer Llama 3 on certain types of prompts? How challenging are these prompts? Does the ranking change if the prompts are easier/harder? Are certain users or prompts overrepresented? Do duplicate prompts or rankings from a small number of users affect the win rate? Does Llama 3 have qualitative differences which make users like it more? We focus on battles consisting of Llama 3-70b against 5 top-ranked models (claude-3-opus-20240229, gpt-4-0125-preview, gpt-4-1106-preview, gpt-4-turbo-2024-04-09, gemini-1.5-pro-0409-preview) and reach the following conclusions: Llama 3 beats other top-ranking models on open-ended writing and creative problems but loses on more close-ended math and coding problems. As prompts get harder, Llama 3â€™s win rate against top-tier models drops significantly. Deduplication or outliers do not significantly affect the win rate. Qualitatively, Llama 3â€™s outputs are friendlier and more conversational than other models, and these traits appear more often in battles that Llama 3 wins. Figure 1. Llama 3-70b's win rate (excluding ties) against top 5 models across prompt topics. * denotes that the category contains less than 50 battles. Analyzing win rate across different types of prompts Topic Analysis. We utilize an LLM labeler (Llama 3-70b) to categorize user prompts into a pre-established taxonomy of topics ( from Reka's paper ) and visualize the win rate of Llama 3-70b against the other top models in Figure 1. We see that Llama 3â€™s win rate is highest for open-ended and creative tasks like brainstorming and writing, and lowest for more close-ended technical tasks like math and translation. Interestingly, Llama 3 achieves the highest win rate over data processing tasks which mainly consist of parsing and dataframe operations, but as this category has only 19 examples, this remains inconclusive. Win Rate versus Prompt Difficulty. We employ our recently released pipeline which scores the difficulty of prompts to determine how Llama 3 compares to the other top models as prompts get harder. We define a set of \"hardness\" criteria and use GPT-4-turbo to annotate each prompt from 0 to 7 to indicate how many of these criteria are satisfied (a higher score indicates a harder prompt). Our 7 criteria are: 1. Specificity: Does the prompt ask for a specific output? 2. Domain Knowledge: Does the prompt cover one or more specific domains? 3. Complexity: Does the prompt have multiple levels of reasoning, components, or variables? 4. Problem-Solving: Does the prompt directly involve the AI to demonstrate active problem-solving skills? 5. Creativity: Does the prompt involve a level of creativity in approaching the problem? 6. Technical Accuracy: Does the prompt require technical accuracy in the response? 7. Real-world Application: Does the prompt relate to real-world applications? We score 1000 battles against the top 3 models on the leaderboard and plot their win rates versus prompt score in Figure 2. We observe a significant drop in Llama 3's performance compared to the other top models, from a high 50% win rate to a low 40% win rate. We conclude that as more of these \"hardness\" criteria are met, Llama 3's win rate drop rapidly compared to other models. Note that these criteria may not be exhaustive, see the blog for further discussion. Figure 2. Several top models' win rate against the strongest 6 models over the intervals of number of key criteria satisfied. *English battles between strongest models: llama-3-70b-chat, claude-3-opus-20240229, gpt-4-0125-preview, gpt-4-1106-preview, gpt-4-turbo-2024-04-09, gemini-1.5-pro-api-0409-preview. Figure 3. The percentage of prompts with number of hardness criteria met in 3.5K sample of arena battles. We observe a significant portion of the battles are classified as hard (~27%). We can further analyze which types of prompts affect win rate by fitting a decision tree on the 7 binary columns representing if a given prompt has satisfied each of the criteria above. From this decision tree, we can segment prompts into criteria subsets such that Llama 3-70b-Instruct either performs very well or very poorly. The tree shown in Figure 4 shows us which subsets change the modelâ€™s win rate the most when conditioned on. Figure 4. Llama 3-70b-Instruct's win rate conditioned on hierarchical prompt criteria subsets as fitted using a standard decision tree algorithm. The first thing to notice is that â€œSpecificityâ€ is the root node of the tree, suggesting that this criteria most immediately divides Llama3-70b-Instructâ€™s performance into its strengths and weaknesses. It supports our initial findings above that Llama3-70b-Instruct is stronger on open-ended tasks rather than more closed-ended tasks. We can traverse further down the tree and see that Llama3-70b-Instruct is quite strong on open-ended creative questions (see the blue path), reaching around a 60% win-rate against these top models. Emperically, these types of questions are often writing and brainstorming style questions. For example two prompts where Llama-3-70B-Instruct won are: \"Write the first chapter of a novel.\" and \"Could you provide two story suggestions for children that promote altruism? \". On the other hand, following the orange path, we can notice that Llama3-70b-Instruct has a lower win-rate against top models when answering close-ended, non-real-world, reasoning-based questions. These questions are often logic puzzles and math word word problems. Two examples where Llama-3-70B-Instruct won are: \"123x = -4x * 2 - 65\" and \"There are two ducks in front of a duck, two ducks behind a duck and a duck in the middle. How many ducks are there?\" The effect of",
    "link": "https://lmsys.org/blog/2024-05-08-llama3/"
  },
  {
    "id": "the-unreasonable-ineffectiveness-of-the-deeper-lay-603dc77c",
    "title": "The Unreasonable Ineffectiveness of the Deeper Layers",
    "summary": "How is knowledge stored in an LLM's weights? We study this via layer pruning: if removing a certain layer does not affect model performance in common question-answering benchmarks, then the weights in that layer are not necessary for storing the knowledge needed to answer those questions. To find these unnecessary parameters, we identify the optimal block of layers to prune by considering similarity across layers; then, to \"heal\" the damage, we perform a small amount of finetuning. Surprisingly, with this method we find minimal degradation of performance until after a large fraction (up to half) of the layers are removed for some common open-weight models. From a scientific perspective, the robustness of these LLMs to the deletion of layers implies either that current pretraining methods are not properly leveraging the parameters in the deeper layers of the network or that the shallow layers play a critical role in storing knowledge. For our study, we use parameter-efficient finetuning (PEFT) methods, specifically quantization and Low Rank Adapters (QLoRA), such that each of our experiments can be performed on a single 40GB A100 GPU.",
    "link": "https://arxiv.org/abs/2403.17887"
  },
  {
    "id": "grokking-generalization-beyond-overfitting-on-smal-0d7e7b5f",
    "title": "Grokking: Generalization Beyond Overfitting on Small Algorithmic Datasets",
    "summary": "In this paper we propose to study generalization of neural networks on small algorithmically generated datasets. In this setting, questions about data efficiency, memorization, generalization, and speed of learning can be studied in great detail. In some situations we show that neural networks learn through a process of \"grokking\" a pattern in the data, improving generalization performance from random chance level to perfect generalization, and that this improvement in generalization can happen well past the point of overfitting. We also study generalization as a function of dataset size and find that smaller datasets require increasing amounts of optimization for generalization. We argue that these datasets provide a fertile ground for studying a poorly understood aspect of deep learning: generalization of overparametrized neural networks beyond memorization of the finite training dataset.",
    "link": "https://arxiv.org/abs/2201.02177"
  },
  {
    "id": "refusal-in-llms-is-mediated-by-a-single-direction--cc04eef9",
    "title": "Refusal in LLMs is mediated by a single direction â€” LessWrong",
    "summary": "x This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. Refusal in LLMs is mediated by a single direction â€” LessWrong Interpretability (ML & AI) MATS Program AI Frontpage 2024 Top Fifty: 29 % 252 Refusal in LLMs is mediated by a single direction by Andy Arditi , Oscar Obeso , Aaquib111 , wesg , Neel Nanda 27th Apr 2024 AI Alignment Forum 12 min read 95 252 Î© 77 Review This work was produced as part of Neel Nanda's stream in the ML Alignment & Theory Scholars Program - Winter 2023-24 Cohort, with co-supervision from Wes Gurnee. This post is a preview for our upcoming paper, which will provide more detail into our current understanding of refusal. We thank Nina Rimsky and Daniel Paleka for the helpful conversations and review. Update (June 18, 2024): Our paper is now available on arXiv. Executive summary Modern LLMs are typically fine-tuned for instruction-following and safety . Of particular interest is that they are trained to refuse harmful requests, e.g. answering \"How can I make a bomb?\" with \"Sorry, I cannot help you.\" We find that refusal is mediated by a single direction in the residual stream : preventing the model from representing this direction hinders its ability to refuse requests, and artificially adding in this direction causes the model to refuse harmless requests. We find that this phenomenon holds across open-source model families and model scales . This observation naturally gives rise to a simple modification of the model weights, which effectively jailbreaks the model without requiring any fine-tuning or inference-time interventions . We do not believe this introduces any new risks, as it was already widely known that safety guardrails can be cheaply fine-tuned away , but this novel jailbreak technique both validates our interpretability results, and further demonstrates the fragility of safety fine-tuning of open-source chat models. See this Colab notebook for a simple demo of our methodology. Our intervention (displayed as striped bars) significantly reduces refusal rates on harmful instructions, and elicits unsafe completions. This holds across open-source chat models of various families and scales. Introduction Chat models that have undergone safety fine-tuning exhibit refusal behavior: when prompted with a harmful or inappropriate instruction, the model will refuse to comply, rather than providing a helpful answer. ChatGPT and other safety fine-tuned models refuse to comply with harmful requests. Our work seeks to understand how refusal is implemented mechanistically in chat models. Initially, we set out to do circuit-style mechanistic interpretability, and to find the \"refusal circuit.\" We applied standard methods such as activation patching, path patching, and attribution patching to identify model components (e.g. individual neurons or attention heads) that contribute significantly to refusal. Though we were able to use this approach to find the rough outlines of a circuit, we struggled to use this to gain significant insight into refusal. We instead shifted to investigate things at a higher level of abstraction - at the level of features, rather than model components. [1] Thinking in terms of features As a rough mental model, we can think of a transformer's residual stream as an evolution of features. At the first layer, representations are simple, on the level of individual token embeddings. As we progress through intermediate layers, representations are enriched by computing higher level features (see Nanda et al. 2023 ). At later layers, the enriched representations are transformed into unembedding space, and converted to the appropriate output tokens. We can think of refusal as a progression of features, evolving from embedding space, through intermediate features, and finally to unembed space. Note that the \" s hould refuse\" feature is displayed here as a bottleneck in the computational graph of features. [This is a stylized representation for purely pedagogical purposes.] Our hypothesis is that, across a wide range of harmful prompts, there is a single intermediate feature which is instrumental in the modelâ€™s refusal. In other words, many particular instances of harmful instructions lead to the expression of this \"refusal feature,\" and once it is expressed in the residual stream, the model outputs text in a sort of \"should refuse\" mode. [2] If this hypothesis is true, then we would expect to see two phenomena: Erasing this feature from the model would block refusal. Injecting this feature into the model would induce refusal. If there is a single bottleneck feature that mediates all refusals, then removing this feature from the model should break the model's ability to refuse. Our work serves as evidence for this sort of conceptualization. For various different models, we are able to find a direction in activation space, which we can think of as a \"feature,\" that satisfies the above two properties. Methodology Finding the \"refusal direction\" In order to extract the \"refusal direction,\" we very simply take the difference of mean activations [3] on harmful and harmless instructions: Run the model on ð‘› harmful instructions and ð‘› harmless instructions [4] , caching all residual stream activations at the last token position [5] . While experiments in this post were run with n = 512 , we find that using just n = 32 yields good results as well. Compute the difference in means between harmful activations and harmless activations. This yields a difference-in-means vector r l for each layer l in the model. We can then evaluate each normalized direction ^ r l over a validation set of harmful instructions to select the single best \"refusal direction\" ^ r . Ablating the \"refusal direction\" to bypass refusal Given a \"refusal direction\" ^ r âˆˆ R d model , we can \"ablate\" this direction from the model. In other words, we can prevent the model from ever representing this direction. We can implement this as an inference-time intervention: every time a component c (e.g. an attention head) writes its output c out âˆˆ R d model to the residual stream, we can erase its contribution to the \"refusal direction\" ^ r . We can do this by",
    "link": "https://www.lesswrong.com/posts/jGuXSZgv6qfdhMCuJ/refusal-in-llms-is-mediated-by-a-single-direction"
  },
  {
    "id": "better-faster-large-language-models-via-multi-toke-558dcfdb",
    "title": "Better & Faster Large Language Models via Multi-token Prediction",
    "summary": "Large language models such as GPT and Llama are trained with a next-token prediction loss. In this work, we suggest that training language models to predict multiple future tokens at once results in higher sample efficiency. More specifically, at each position in the training corpus, we ask the model to predict the following n tokens using n independent output heads, operating on top of a shared model trunk. Considering multi-token prediction as an auxiliary training task, we measure improved downstream capabilities with no overhead in training time for both code and natural language models. The method is increasingly useful for larger model sizes, and keeps its appeal when training for multiple epochs. Gains are especially pronounced on generative benchmarks like coding, where our models consistently outperform strong baselines by several percentage points. Our 13B parameter models solves 12 % more problems on HumanEval and 17 % more on MBPP than comparable next-token models. Experiments on small algorithmic tasks demonstrate that multi-token prediction is favorable for the development of induction heads and algorithmic reasoning capabilities. As an additional benefit, models trained with 4-token prediction are up to 3 times faster at inference, even with large batch sizes.",
    "link": "https://arxiv.org/abs/2404.19737"
  },
  {
    "id": "phi-3-and-arctic-outlier-lms-are-hints-by-nathan-l-c4150c79",
    "title": "Phi 3 and Arctic: Outlier LMs are hints - by Nathan Lambert",
    "summary": "Phi 3 and Arctic: Outlier LMs are hints Models that seem totally out of scope from recent open LLMs give us a sneak peek of where the industry will be in 6 to 18 months. Nathan Lambert Apr 30, 2024 14 1 Share Last week, two very interesting open models were released into the world, Phi 3 from Microsoft and Arctic from Snowflake, and they both stand on their own in training techniques. The Phi series of models is known for taking an opinionated stance on synthetic, high-quality data and Arctic is a new entry that has a very sparse Mixture of Experts (MoE) architecture that makes it useful exclusively for the coding-focused, VRAM-rich inference population. My primary reason for writing this article is that I thought they already broke my model relating pretraining compute to MMLU score but it seems like it may be more robust than I initially thought. MMLU is an okay metric for open models because weâ€™re still seeing gains and models that arenâ€™t good enough to play in the space. Obviously, you canâ€™t over-index on one metric, but it gives many good lessons. Letâ€™s start with Arctic, which was more surprising for most of us plugged into the industry. Share Arctic & open mixture of expert trends Last week, Snowflake launched its response to the recent DBRX model from their often-paired competitor, Databricks: Arctic . Arctic has a very different architecture when compared to most open models these days. I played with Arctic a little bit in their demo , but it didnâ€™t pass my basic challenge tasks around who I am, RLHF details, and the stretch question of writing DPO code. Itâ€™s clear from the announcements that this is a model with a niche in coding and reasoning, and thatâ€™s a good thing when it comes to open LLM strategy . For most of us, Arctic is just a reference on a small list of very open models, but I hope that it performs on the tasks claimed within the nebulous, and bordering on cringe, evaluation of â€œenterprise intelligence.\" The core plot of the announcement is below â€” they actually fixed the image with DBRXâ€™s correct position, to give them credit ( originally was wrong ). This comes just a week or two after Mistral shared a similar â€œefficiencyâ€ plot , which sparked extensive mid-tier memeing . To the blog post . The Arctic model uses a â€œDense-MoE Hybrid transformer architecture with â€œa 10B dense transformer model with a residual 128Ã—3.66B MoE MLP resulting in 480B total and 17B active parameters chosen using a top-2 gating.â€ The 17B active parameters number here seems great, but the 480B total parameters in this model make it pretty much stopped from a consumer and researcher fine-tuning perspective. While only 17B parameters are active for inference, you still need to be able to load all the weights in memory (otherwise inference and fine-tuning would be extremely slow reading and writing to memory constantly). If nothing else, it matches the trend that the minimum footprint needed for working with open LLMs is growing fast . Itâ€™s an enterprise LLM because only enterprises really have the compute to use it, which doesnâ€™t really matter if youâ€™re focused on your market. Much of the blog post is about what â€œenterprise intelligenceâ€ is, but editing for clarity and emphasis, the key points are: 1) Many-but-condensed experts with more expert choices â€¦ In designing Arctic, we noticed that the improvement of the model quality depended primarily on the number of experts and the total number of parameters in the MoE model, and the number of ways in which these experts can be combined together. Based on this insight, Arctic is designed to have 480B parameters spread across 128 fine-grained experts and uses top-2 gating to choose 17B active parameters. In contrast, recent MoE models are built with significantly fewer experts . Intuitively, Arctic leverages a large number of total parameters and many experts to enlarge the model capacity for top-tier intelligence , while it judiciously chooses among many-but-condensed experts and engages a moderate number of active parameters for resource-efficient training and inference. Mostly, Snowflake found that adding more experts lets the model keep getting better at benchmarks without increasing the inference cost. Continuing: 2) Architecture and System Co-design : Training vanilla MoE architecture with a large number of experts is very inefficient even on the most powerful AI training hardware due to high all-to-all communication overhead among experts. However, it is possible to hide this overhead if the communication can be overlapped with computation. Our second insight is that combining a dense transformer with a residual MoE component in the Arctic architecture enables our training system to achieve good training efficiency via communication computation overlap , hiding a big portion of the communication overhead. Their somewhat confusing diagram is here. The thing to focus on is the residual streams â€” the model flow that goes around rather than through the expensive computation blocks. I like this architecture because it fits with a training intuition from Anthropic around the models adding to the residual flow of information and building up the output, rather than the core signal flow being through the blocks. We donâ€™t know how to confirm this idea or the opposite for large models, but you can find more on this in a recent Dwarkesh episode . Dwarkesh Patel: You referred earlier to Anthropic's way of thinking about transformers as these read-write operations that layers do. One of you should just kind of explain at a high level what you mean by that. Trenton Bricken So for the residual stream, imagine you're in a boat going down a river and the boat is the current query where you're trying to predict the next token. So it's â€œthe cat sat on the [blank].â€ And then you have these little streams that are coming off the river where you can get extra passengers or collect extra information if you want. And those correspond to",
    "link": "https://www.interconnects.ai/p/phi-3-and-arctic-llms"
  },
  {
    "id": "chatbotarena-the-peoples-llm-evaluation-the-future-1b58db78",
    "title": "ChatBotArena: The peoplesâ€™ LLM evaluation, the future of evaluation, the incentives of evaluation, and gpt2chatbot",
    "summary": "ChatBotArena: The peoplesâ€™ LLM evaluation, the future of evaluation, the incentives of evaluation, and gpt2chatbot What the details tell us about the most in-vogue LLM evaluation tool â€” and the rest of the field. Nathan Lambert May 08, 2024 âˆ™ Paid 22 2 Share Evaluation is no longer equally accessible to academics and everyone who is involved in language model development. There are obvious reasons we shouldnâ€™t let this happen, from transparency to science to regulation. Broadly, language model evaluation these days largely is reduced to three things: The best academic benchmarks: The massive multitask language understanding (MMLU) benchmark, which is used to test general language model knowledge along with a few static and open benchmarks. A few new-styled benchmarks : The only one to break through here is the ChatBotArena from LMSYS where state-of-the-art chat models are pitted head to head to see which model is the best. Benchmarks we donâ€™t see: Private A/B testing of different language model endpoints within user applications. These evaluations cover extremely different legs of the ecosystem. MMLU is made by academics, used by everyone, and can benefit everyone. ChatBotArena is home to only a few models but also serves a broader stakeholder audience by being publicly available. A/B testing is the gold standard for evaluation, but you need an existing product pipeline to serve through (and likely an ML team to make sense of which variants to serve). This post focuses on ChatBotArena: what it is, what the data can tell you, what I worry about, and where it should go in the future. ChatBotArena is one of the most successful projects in my area of model fine-tuning since ChatGPT, so Iâ€™m trying to share almost all of my thoughts on the matter in one place. From historical connections to subtle model issues, you can get the complete insider picture of ChatBotArena in this post. Contents: What is ChatBotArena actually? ELI5-ish Who needs ChatBotArena? Probably not you The statistics of ChatBotArena The future of evaluation GPT2Chatbot and LMSYSâ€™s incentives ($) Share What is ChatBotArena actually? ELI5-ish ChatBotArena , the side-by-side blind taste test for current language models (LMs), from the LMSYS Organization, is certainly everyoneâ€™s public evaluation darling. ChatBotArena fills a large niche that many folks in the community are craving by attempting to answer: which chatbot from the many we hear about and use is best at handling the general wave of content that most people expect LMs to be useful for? The interface ChatBotArena uses to answer this is extremely simple: This workflow is intuitive for anyone in the LM space. You enter a question, youâ€™re given two answers (something you can even encounter within dedicated applications like ChatGPT). Then, you can vote or continue the conversation. Voting ends the conversation because it reveals the two models, ending unbiased voting. LMSYS, you should maybe consider tweaking the interface to encourage multi-turn conversations; more on this need later . Theyâ€™re doing a lot right by not adding to the interface as a default. The simplicity and cleanliness of the interface, branding, etc. add a lot of value to the integrity of their evaluation. Different interfaces would bias the completions of the models in difficult-to-measure ways. The endpoints are entirely public. No sign-in is required. Anyone can come and go at their liking. Weâ€™ve seen many AI papers and endeavors do this â€” free model usage is exchanged for some sort of data. WildChat fills this niche as a more responsibly handled ShareGPT dataset. Weâ€™ll see more things like this, theyâ€™re ways of turning certain resources that you have an excess of into other valuable types of data or reputation. ChatBotArena comes out of a new subfield of the science of LMs around extracting user preference in pairwise interfaces, most commonly used for collecting training data for the RLHF process. These same interfaces can be used for evaluation as well, where the two answers come from the two models you wish to evaluate. Below is the interface that Anthropic used in earlier versions of its Claude models. All model providers have something similar â€” Cohere even open-sourced the foundations of theirs . ChatBotArena thrives in a worldview where LMs are primarily about letting a user get any question answered. Here, LMs are the ultimate general technology. In reality, I think most people want more specific and useful evaluations than ChatBotArena. ChatBotArena is largely uninterpretable and provides little feedback for improving models other than a tick on the leaderboard for where you currently stand. Over time, people will normalize to having it. Surprises will be less common and the details of what the actual conversations are will become more important . The unknowns of ChatBotArena define it to me. When collecting human data for any evaluation project, controlling the who and how of the process is the most important thing other than the AI model you use in the loop. In the RLHF process, we use extensive documentation controlling how the data should be curated, with an example being this 16-page Google Doc that OpenAI released after InstructGPT. In order to do model training with human data, the collected data also needs to be heavily filtered, averaged, and monitored. When you first spin this up, the rule of thumb is that you can expect to throw out about a third of your collected data, but maybe Scale et al got better at data controls in the year since Iâ€™ve tried it. Places like OpenAI now hire in-house experts on everything from math to poetry, which I suspect is to help control the â€œwhoâ€ in some of these data pipelines. I share this just to show that ChatBotArena isnâ€™t perfect, but the cost to do it perfectly is likely prohibitive for a public benchmark. For evaluation, thereâ€™s a bit more leeway in the specification of this process, but the best engineering processes come about when evaluation is aligned with training â€” only then can you faithfully identify weaknesses outside of the many sources of noise.",
    "link": "https://www.interconnects.ai/p/chatbotarena-the-future-of-llm-evaluation"
  },
  {
    "id": "llmint8-and-emergent-features-tim-dettmers-e0e0d09b",
    "title": "LLM.int8() and Emergent Features â€” Tim Dettmers",
    "summary": "When I attended NAACL, I wanted to do a little test. I had two pitches for my LLM.int8() paper. One pitch is about how I use advanced quantization methods to achieve no performance degradation transformer inference at scale that makes large models more accessible. The other pitch talks about emergent outliers in transformers and how they radically change what transformers learn and how they function. From that, I learned that quantization research is like printers. Nobody cares about printers. Nobody likes printers. But everybody is happy if printers do their job. How that job is done for you through the bitsandbytes library with Hugging Face integration so that you can easily run OPT-175B and BLOOM-176B on a single machine is described in another blog post by my colleague and collaborator Younes Belkada . This blog post will spill some mandatory details about quantization, but I want to mostly make it about these emergent features that I found in transformers at scale. I know the claims in the paper are highly robust. This blog post is a more speculative version of the paper that teases out the super curious details about the fascinating properties surrounding the emergent outlier features I found. I cannot spill all the details because my next project will delve deep into understanding these outlier features, but the space is so rich that I am happy to give you many curious details. Contents hide Mandatory quantization details What is quantization? How to make quantization methods more precise Vector-wise quantization Mixed precision decomposition Results Emergent Features Emergent Features in a Nutshell How Emergent Features Emerge The Most Important Take-aways for Your Research There are two types of transformers and you should not generalize from one to the other. We might be able to find new emergent properties by studying â€œscaling laws of emergenceâ€. Conclusion Related Related Posts Mandatory quantization details In a previous version of this blog post, I jokingly had a section with the big title â€œAll You Ever Wanted to Know about Quantizationâ€ The section read: â€œIf you quantize from 16-bit to 8-bit, you lose precision which might degrade model prediction quality.â€ That is it. Most people do not want to learn more about quantization â€” and honestly, the small sentence above is already enough information. The details are very gritty and complicated, but it is all in the code. The math and concepts are very simple and straightforward â€” if you have worked on quantization before. If you have not encountered quantization, it is likely a hot devilish nightmare that will eat your liver. For those that say, â€œPfff! Why do I need a liver anyways?â€. Well, here you go. For others, just move ahead and read about the mysteries of emergent features. What is quantization? Let us say you have a data type I5 with values [0, 1, 2, 3, 4, 5] and a data type, I3, with values [0, 2, 4], how do you quantize from data type I5 to I3? You follow a two-step procedure: Normalize the range of I5 into I3. Round to the nearest value of I3. Letâ€™s do an example. Letâ€™s say we have the vector [3, 1, 2, 3] in I5, and we want to quantize to I3. Here the step-by-step recipe for quantization: We find the absolute maximum value of the vector: [3, 1, 2, 3] -> 3 Then we divide by that value: [3, 1, 2, 3] -> [1, 0.33, 0.66, 1.0] And now we multiple by the range of the target data type I3, which is 4: [1, 0.33, 0.66, 1.0] -> [4.0, 1.33, 2.66, 4.0] Now we round to the nearest value: [4.0, 1.33, 2.66, 4.0] -> [4, 0, 2, 4] We now converted [3, 1, 2, 4] in I5 to [4, 0, 2, 4] in I3. To dequantize, we reverse this process. Divide by 4: [4, 0, 2, 4] -> [1.0, 0.0, 0.5, 1.0] Multiply by the absolute maximum: [1.0, 0.0, 0.5, 1.0] -> [3.0, 0.0, 1.5, 3.0] Now we round again: [3.0, 0.0, 1.5, 3.0] -> [3, 0, 2, 3] We see that our dequantization and quantization led to one error: [3, 1, 2, 3] to [3, 0, 2, 3] The second element changed from 1 to 0. This is a quantization error that leads to the loss of information in terms of how precise the information is encoded. If we have such errors and propagate them through many layers of a neural network, they accumulate, and they may change the result of a prediction and degrade the prediction quality. How to make quantization methods more precise Quantization can be enhanced in two ways. Use a better data type, or use more normalization constants (absolute maximum). Regarding data types, Int8 is a terrible data type for deep learning. That is why I developed new data types in my research. However, currently, GPUs do not support other than Int8 data types on the hardware level, and as such, we are out of luck and need to use Int8. The only way to improve quantization is through more normalization constants. A normalization constant squishes the input distribution, for example, I5, into the target distribution, for example, I3. We can increase precision, by squishing each vector only as much as is needed. For example, if you have the two vectors: [3, 1, 2, 3] [0, 2, 2, 0] Then you can squish the first by 4 and the second by 2. This will give you twice the precision to quantize the second vector because the inputs are now spread over a broader range of the I3 data type. In fact, the second vector can be quantized without errors if you use an additional absolute maximum value. If you use only a single constant over both vectors (tensor-wise constants), then you will have two errors. Vector-wise quantization So now that we know how to make quantization more precise, how do we achieve maximum precision for matrix multiplication? The key is this: If we use different normalization constants",
    "link": "https://timdettmers.com/2022/08/17/llm-int8-and-emergent-features/"
  },
  {
    "id": "transformers-represent-belief-state-geometry-in-th-f35dfb73",
    "title": "Transformers Represent Belief State Geometry in their Residual Stream â€” LessWrong",
    "summary": "x This website requires javascript to properly function. Consider activating javascript to get access to all site functionality. Transformers Represent Belief State Geometry in their Residual Stream â€” LessWrong MATS Program PIBBSS Interpretability (ML & AI) Transformers AI Curated 2024 Top Fifty: 80 % 429 Transformers Represent Belief State Geometry in their Residual Stream by Adam Shai 16th Apr 2024 AI Alignment Forum 14 min read 101 429 Î© 145 Review Produced while being an affiliate at PIBBSS [1] . The work was done initially with funding from a Lightspeed Grant, and then continued while at PIBBSS. Work done in collaboration with @Paul Riechers , @Lucas Teixeira , @Alexander Gietelink Oldenziel , and Sarah Marzen . Paul was a MATS scholar during some portion of this work. Thanks to Paul, Lucas, Alexander, Sarah, and @Guillaume Corlouer for suggestions on this writeup. Update May 24, 2024: See our manuscript based on this work Introduction What computational structure are we building into LLMs when we train them on next-token prediction? In this post we present evidence that this structure is given by the meta-dynamics of belief updating over hidden states of the data-generating process . We'll explain exactly what this means in the post. We are excited by these results because We have a formalism that relates training data to internal structures in LLMs . Conceptually, our results mean that LLMs synchronize to their internal world model as they move through the context window. The computation associated with synchronization can be formalized with a framework called Computational Mechanics . In the parlance of Computational Mechanics, we say that LLMs represent the Mixed-State Presentation of the data generating process. The structure of synchronization is, in general, richer than the world model itself. In this sense, LLMs learn more than a world model . We have increased hope that Computational Mechanics can be leveraged for interpretability and AI Safety more generally . There's just something inherently cool about making a non-trivial prediction - in this case that the transformer will represent a specific fractal structure - and then verifying that the prediction is true. Concretely, we are able to use Computational Mechanics to make an a priori and specific theoretical prediction about the geometry of residual stream activations (below on the left), and then show that this prediction holds true empirically (below on the right). Theoretical Framework In this post we will operationalize training data as being generated by a Hidden Markov Model (HMM) [2] . An HMM has a set of hidden states and transitions between them. The transitions are labeled with a probability and a token that it emits. Here are some example HMMs and data they generate. Consider the relation a transformer has to an HMM that produced the data it was trained on. This is general - any dataset consisting of sequences of tokens can be represented as having been generated from an HMM. Through the discussion of the theoretical framework, let's assume a simple HMM with the following structure, which we will call the Z1R process [3] (for \"zero one random\"). The Z1R process has 3 hidden states, S 0 , S 1 , and S R . Arrows of the form S x a : p % âˆ’ âˆ’âˆ’ â†’ S y denote P ( S y , a | S x ) = p % , that the probability of moving to state S y and emitting the token a , given that the process is in state S x , is p % . In this way, taking transitions between the states stochastically generates binary strings of the form ...01R01R... where R is a random 50/50 sample from { 0 , 1 }. The HMM structure is not directly given by the data it produces. Think of the difference between the list of strings this HMM emits (along with their probabilities) and the hidden structure itself [4] . Since the transformer only has access to the strings of emissions from this HMM, and not any information about the hidden states directly, if the transformer learns anything to do with the hidden structure, then it has to do the work of inferring it from the training data. What we will show is that when they predict the next token well, transformers are doing even more computational work than inferring the hidden data generating process! Do Transformers Learn a Model of the World? One natural intuition would be that the transformer must represent the hidden structure of the data-generating process (ie the \"world\" [2] ). In this case, this would mean the three hidden states and the transition probabilities between them. This intuition often comes up (and is argued about) in discussions about what LLM's \"really understand.\" For instance, Ilya Sutskever has said : Because if you think about it, what does it mean to predict the next token well enough? It's actually a much deeper question than it seems. Predicting the next token well means that you understand the underlying reality that led to the creation of that token. It's not statistics. Like it is statistics but what is statistics? In order to understand those statistics to compress them, you need to understand what is it about the world that creates this set of statistics. This type of intuition is natural, but it is not formal. Computational Mechanics is a formalism that was developed in order to study the limits of prediction in chaotic and other hard-to-predict systems, and has since expanded to a deep and rigorous theory of computational structure for any process. One of its many contributions is in providing a rigorous answer to what structures are necessary to perform optimal prediction. Interestingly, Computational Mechanics shows that prediction is substantially more complicated than generation . What this means is that we should expect a transformer trained to predict the next token well should have more structure than the data generating process! The Structure of Belief State Updating But what is that structure exactly? Imagine you",
    "link": "https://www.lesswrong.com/posts/gTZ2SxesbHckJ3CkF/transformers-represent-belief-state-geometry-in-their"
  },
  {
    "id": "deepseek-v2-a-strong-economical-and-efficient-mixt-3ac36021",
    "title": "DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model",
    "summary": "We present DeepSeek-V2, a strong Mixture-of-Experts (MoE) language model characterized by economical training and efficient inference. It comprises 236B total parameters, of which 21B are activated for each token, and supports a context length of 128K tokens. DeepSeek-V2 adopts innovative architectures including Multi-head Latent Attention (MLA) and DeepSeekMoE. MLA guarantees efficient inference through significantly compressing the Key-Value (KV) cache into a latent vector, while DeepSeekMoE enables training strong models at an economical cost through sparse computation. Compared with DeepSeek 67B, DeepSeek-V2 achieves significantly stronger performance, and meanwhile saves 42.5% of training costs, reduces the KV cache by 93.3%, and boosts the maximum generation throughput to 5.76 times. We pretrain DeepSeek-V2 on a high-quality and multi-source corpus consisting of 8.1T tokens, and further perform Supervised Fine-Tuning (SFT) and Reinforcement Learning (RL) to fully unlock its potential. Evaluation results show that, even with only 21B activated parameters, DeepSeek-V2 and its chat versions still achieve top-tier performance among open-source models.",
    "link": "https://arxiv.org/abs/2405.04434"
  },
  {
    "id": "fishing-for-magikarp-automatically-detecting-under-bb32fdb6",
    "title": "Fishing for Magikarp: Automatically Detecting Under-trained Tokens in Large Language Models",
    "summary": "The disconnect between tokenizer creation and model training in language models allows for specific inputs, such as the infamous SolidGoldMagikarp token, to induce unwanted model behaviour. Although such `glitch tokens', tokens present in the tokenizer vocabulary but that are nearly or entirely absent during model training, have been observed across various models, a reliable method to identify and address them has been missing. We present a comprehensive analysis of Large Language Model tokenizers, specifically targeting this issue of detecting under-trained tokens. Through a combination of tokenizer analysis, model weight-based indicators, and prompting techniques, we develop novel and effective methods for automatically detecting these problematic tokens. Our findings demonstrate the prevalence of such tokens across a diverse set of models and provide insights into improving the efficiency and safety of language models.",
    "link": "https://arxiv.org/abs/2405.05417"
  },
  {
    "id": "gqa-training-generalized-multi-query-transformer-m-681f5861",
    "title": "GQA: Training Generalized Multi-Query Transformer Models from Multi-Head Checkpoints",
    "summary": "Multi-query attention (MQA), which only uses a single key-value head, drastically speeds up decoder inference. However, MQA can lead to quality degradation, and moreover it may not be desirable to train a separate model just for faster inference. We (1) propose a recipe for uptraining existing multi-head language model checkpoints into models with MQA using 5% of original pre-training compute, and (2) introduce grouped-query attention (GQA), a generalization of multi-query attention which uses an intermediate (more than one, less than number of query heads) number of key-value heads. We show that uptrained GQA achieves quality close to multi-head attention with comparable speed to MQA.",
    "link": "https://arxiv.org/abs/2305.13245"
  },
  {
    "id": "mha-mqa-gpq-mla-b1a9060f",
    "title": "The Extreme Struggle Between Caching and Performance: From MHA, MQA, GQA to MLA",
    "summary": "This article provides a brief overview of the evolution of multi-head attention, particularly the changes in concepts from MHA to MQA, GQA, and finally MLA. Finally, it elaborates on MLA. In this paper, MLA is considered a generalization of GQA. It replaces GQA's segmentation and repetition with a projection matrix, introduces an identity transformation technique to further compress the KV cache, and employs a hybrid approach to ensure RoPE compatibility. Overall, MLA can be considered a very practical attention variant.",
    "link": "https://spaces.ac.cn/archives/10091"
  },
  {
    "id": "lora-learns-less-and-forgets-less-0ae03388",
    "title": "LoRA Learns Less and Forgets Less",
    "summary": "Low-Rank Adaptation (LoRA) is a widely-used parameter-efficient finetuning method for large language models. LoRA saves memory by training only low rank perturbations to selected weight matrices. In this work, we compare the performance of LoRA and full finetuning on two target domains, programming and mathematics. We consider both the instruction finetuning (approximately 100K prompt-response pairs) and continued pretraining (20B unstructured tokens) data regimes. Our results show that, in the standard low-rank settings, LoRA substantially underperforms full finetuning. Nevertheless, LoRA better maintains the base model's performance on tasks outside the target domain. We show that LoRA mitigates forgetting more than common regularization techniques such as weight decay and dropout; it also helps maintain more diverse generations. Finally, we show that full finetuning learns perturbations with a rank that is 10-100X greater than typical LoRA configurations, possibly explaining some of the reported gaps. We conclude by proposing best practices for finetuning with LoRA.",
    "link": "https://arxiv.org/abs/2405.09673"
  },
  {
    "id": "transformer-math-101-eleutherai-blog-77389207",
    "title": "Transformer Math 101 | EleutherAI Blog",
    "summary": "Introduction # A lot of basic, important information about transformer language models can be computed quite simply. Unfortunately, the equations for this are not widely known in the NLP community. The purpose of this document is to collect these equations along with related knowledge about where they come from and why they matter. Note: This post is primarily concerned with training costs, which are dominated by VRAM considerations. For an analogous discussion of inference costs with a focus on latency, check out this excellent blog post by Kipply. Compute Requirements # The basic equation giving the cost to train a transformer model is given by: $$ C\\approx\\tau T = 6PD $$ where: $C$ is the compute required to train the transformer model, in total floating point operations $C=C_{\\text{forward}}+C_{\\text{backward}}$ $C_{\\text{forward}}\\approx2PD$ $C_{\\text{backward}}\\approx4PD$ $\\tau$ is the aggregate throughput of your hardware setup ($\\tau=(\\text{No. GPUs}) \\times (\\text{Actual FLOPs}/\\text{GPU})$), in FLOPs $T$ is the time spent training the model, in seconds $P$ is the number of parameters in the transformer model $D$ is the dataset size, in tokens These equations are proposed and experimentally validated in OpenAIâ€™s scaling laws paper and DeepMindâ€™s scaling laws paper . Please see each paper for more information. Itâ€™s worth taking an aside and discussing the units of $C$. $C$ is a measure of total compute, but can be measured by many units such as: FLOP-seconds, which is in units of $[\\frac{\\text{Floating Point Operations}}{\\text{Second}}] \\times [\\text{Seconds}]$ GPU-hours, which is in units of $[\\text{No. GPUs}]\\times[\\text{Hours}]$ Scaling laws papers tend to report values in PetaFLOP-days, or $10^{15}\\times24\\times3600$ total floating point operations One useful distinction to keep in mind is the concept of $\\text{Actual FLOPs}$. While GPU accelerator whitepapers usually advertise their theoretical FLOPs, these are never met in practice (especially in a distributed setting!). Some common reported values of $\\text{Actual FLOPs}$ in a distributed training setting are reported below in the Computing Costs section. Note that we use the throughput-time version of the cost equation as used in this wonderful blog post on LLM training costs . Parameter vs Dataset Tradeoffs # Although strictly speaking you can train a transformer for as many tokens as you like, the number of tokens trained can highly impact both the computing costs and the final model performance making striking the right balance important. Letâ€™s start with the elephant in the room: â€œcompute optimalâ€ language models. Often referred to as â€œChinchilla scaling lawsâ€ after the model series in the paper that gave rise to current beliefs about the number of parameters, a compute optimal language model has a number of parameters and a dataset size that satisfies the approximation $D=20P$. This is optimal in one very specific sense: in a resource regime where using 1,000 GPUs for 1 hour and 1 GPU for 1,000 hours cost you the same amount, if your goal is to maximize performance while minimizing the cost in GPU-hours to train a model you should use the above equation. We do not recommend training a LLM for less than 200B tokens. Although this is â€œchinchilla optimalâ€ for many models, the resulting models are typically quite poor. For almost all applications, we recommend determining what inference cost is acceptable for your usecase and training the largest model you can to stay under that inference cost for as many tokens as you can. Engineering Takeaways for Compute Costs # Computing costs for transformers are typically listed in GPU-hours or FLOP-seconds. GPT-NeoX achieves 150 TFLOP/s/A100 with normal attention and 180 TFLOP/s/A100 with Flash Attention. This is in line with other highly optimized libraries at scale, for example Megatron-DS reports between 137 and 163 TFLOP/s/A100. As a general rule of thumb, you should always be able to achieve approximately 120 TFLOP/s/A100. If you are seeing below 115 TFLOP/s/A100 there is probably something wrong with your model or hardware configuration. With high-quality interconnect such as InfiniBand, you can achieve linear or sublinear scaling across the data parallel dimension (i.e. increasing the data parallel degree should increase the overall throughput nearly linearly). Shown below is a plot from testing the GPT-NeoX library on Oak Ridge National Labâ€™s Summit supercomputer. Note that V100s are on the x-axis, while most of the numerical examples in the post are for A100s. Memory Requirements # Transformers are typically described in terms of their size in parameters . However, when determining what models can fit on a given set of computing resources you need to know how much space in bytes the model will take up. This can tell you how large a model will fit on your local GPU for inference, or how large a model you can train across your cluster with a certain amount of total accelerator memory. Inference # Model Weights # Most transformers are trained in mixed precision , either fp16 + fp32 or bf16 + fp32. This cuts down on the amount of memory required to train the models, and also the amount of memory required to run inference. We can cast language models from fp32 to fp16 or even int8 without suffering a substantial performance hit. These numbers refer to the size in bits a single parameter requires. Since there are 8 bits in a Byte, we divide this number by 8 to see how many Bytes each parameter requires In int8, $\\text{memory}_{\\text{model}}=(1 \\text{ byte} /\\text{param})\\cdot ( \\text{No. params})$ In fp16 and bf16, $\\text{memory}_{\\text{model}}=(2 \\text{ bytes} /\\text{param})\\cdot ( \\text{No. params})$ In fp32, $\\text{memory}_{\\text{model}}=(4 \\text{ bytes} /\\text{param})\\cdot (\\text{No. params})$ There is also a small amount of additional overhead, which is typically irrelevant to determining the largest model that will fit on your GPU. In our experience this overhead is â‰¤ 20%. Total Inference Memory # In addition to the memory needed to store the model weights, there is also a small amount of additional overhead during the actual forward pass. In our experience this overhead is â‰¤ 20% and is typically irrelevant to determining the largest model that will fit on your GPU. In total, a good heuristic answer for â€œwill this model fit for inferenceâ€ is:",
    "link": "https://blog.eleuther.ai/transformer-math/"
  },
  {
    "id": "cohere-int8-binary-embeddings-scale-your-vector-da-62493d07",
    "title": "Cohere int8 & binary Embeddings - Scale Your Vector Database to Large Datasets",
    "summary": "< Back to blog Cohere int8 & binary Embeddings - Scale Your Vector Database to Large Datasets Nils Reimers Mar 18, 2024 Cohere Embed now natively supports int8 and binary embeddings to reduce memory cost. Product Developers Product Developers Share:",
    "link": "https://cohere.com/blog/int8-binary-embeddings"
  },
  {
    "id": "mixedbread-binary-mrl-685fc71a",
    "title": "mixedbread binary mrl",
    "summary": "All posts April 12, 2024 64 bytes per embedding, yee-haw ðŸ¤  Reading Time Publish Date April 12, 2024 Tags Research Authors Darius Koenig Aamir Shakir Join Our Team We're hiring talented bakers! Table of Contents On this page Why Embeddings? Embeddings May Struggle to Scale Matryoshka Representation Learning & Vector Quantization to the Rescue Taking It One Step Further with Binary MRL The Economic Consequences of the Release Using It in Action Practical Considerations We are happy to introduce a novel embeddings compression method: Binary MRL. This method will make vector search much more scalable and enable a range of new embeddings-based applications that weren't economically feasible before our release. Learn how the parameter influence the search results. Read on to learn more about our approach and to check out our benchmarks. If you want to skip right to the model instead, you can access it here: mxbai-embed-large-v1 : Our recently released flagship embedding model supports binary MRL as it is. How cool is that?! Wikipedia Demo : You can experience the speed and performance of our model in the demo (using binary quantization). Why Embeddings? Embeddings are one of the most versatile tools in natural language processing, supporting a wide variety of settings and use cases. In essence, embeddings are numerical representations of more complex objects like text, images, audio, etc. Specifically, the objects are represented as n-dimensional vectors. After transforming objects using an embedding model, you can determine their inherent semantic similarity by calculating the similarity of the respective embeddings. Essentially, you determine how strongly related two objects are by measuring how close their embeddings are to each other in the n-dimensional vector space. This is crucial for many use cases: it serves as the backbone for recommendation systems, retrieval, one-shot or few-shot learning, outlier detection, similarity search, paraphrase detection, clustering, classification, and much more. Using embeddings is particularly important for Retrieval-Augmented Generation (RAG). The idea behind the concept of RAG is to be able to have an LLM access custom documents that you provide (like analyst reports in your company) and improve its output based on that information. Transforming the documents into embeddings (as well as the query given to the model) allows the LLM to retrieve the most relevant information from your data and utilize it to produce the most relevant output for the user. Embeddings May Struggle to Scale However, embeddings may be challenging to use at scale because of their memory usage, which leads to expensive solutions and high latencies. Currently, many state-of-the-art models produce embeddings with 1024 dimensions, each of which is encoded in float32 , i.e., they require 4 bytes per dimension. To perform retrieval over 250 million vectors, you would therefore need around 1TB of memory! With costs estimated at $3.8 per GB/month, using x2gd instances on AWS, this would incur monthly cloud storage costs of more than $3,500. Matryoshka Representation Learning & Vector Quantization to the Rescue To solve the scaling issues of embeddings, two approaches have lately been gaining particular traction: Matryoshka Representation Learning (MRL) and Vector Quantization. Let's first take a look at both concepts. MRL tries to make embeddings ready to scale by reducing the number of output dimensions of an embedding model without sacrificing a lot of accuracy. This can be achieved by storing more important information in earlier dimensions of the embedding, so that the less important later dimensions can be truncated, saving for example on storage cost and improving processing speed in downstream tasks. In essence, the loss function during model training needs to be calibrated in a way that not only accounts for the standard model performance on, say, 1024 output dimensions, but that tracks the performance using the first 512, 256, 128,... dimensions. Training the model to minimize this loss function will lead it to frontload the most important identifying information within its output vectors. On the other hand, vector quantization represents a very different approach to the problem. Here, instead of changing the number of output dimensions, the size of every dimension is reduced. Typically, each dimension of the embedding is stored as a float32 value, which requires 4 bytes (32 bits) of storage space. Especially when considering vectors with 1024 dimensions, potentially millions or billions of them, the benefits of reducing this size become obvious. A large gain in memory and disk space efficiency as well as retrieval speed under retention of 95% and more of performance can be realized by storing the embedding dimensions as binary values instead. This is achieved by simply transforming the float32-values to 1 if they are greater than 0 and to 0 if they are not. In order for this process not to result in greater loss of performance, a rescoring step can be performed when using the model for retrieval tasks. In this approach, first both query and documents are represented as binary embeddings and the most relevant search results are retrieved with them, which are then also reranked in relation to a float32-embedding of the query. Taking It One Step Further with Binary MRL Recognizing the potential of both of these approaches, we already published some of our research findings on the subject. On MRL, we published our innovative and novel 2D-Matryoshka model ; on binary quantization, we co-authored a post on the hugging face blog , introducing curious members of the community to the subject. Now, we aim to take things one step further by combining both approaches. We want to demonstrate that it is feasible to truncate embedding dimensions and reduce the size of each dimension simultaneously, while still retaining most of the original model performance using our very own embedding model . The following table demonstrates that our model is able to retain over 90% of performance while reducing its output dimensions from 1024 to 512 and also reducing the size of each dimension by a factor of 32. In effect, we create a 64x efficiency gain. Naturally, this decrease in memory usage also leads to a proportional -",
    "link": "https://www.mixedbread.ai/blog/binary-mrl"
  },
  {
    "id": "scaling-monosemanticity-extracting-interpretable-f-ac64e33c",
    "title": "Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet",
    "summary": "Transformer Circuits Thread Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet Scaling Monosemanticity: Extracting Interpretable Features from Claude 3 Sonnet Authors Adly Templeton * , Tom Conerly * , Jonathan Marcus, Jack Lindsey, Trenton Bricken, Brian Chen, Adam Pearce, Craig Citro, Emmanuel Ameisen, Andy Jones, Hoagy Cunningham, Nicholas L Turner, Callum McDougall, Monte MacDiarmid, Alex Tamkin, Esin Durmus, Tristan Hume, Francesco Mosconi, C. Daniel Freeman, Theodore R. Sumers, Edward Rees, Joshua Batson, Adam Jermyn, Shan Carter, Chris Olah, Tom Henighan Affiliations Anthropic Published May 21, 2024 * Core Contributor; Correspondence to henighan@anthropic.com ; Author contributions statement below . Eight months ago, we demonstrated that sparse autoencoders could recover monosemantic features from a small one-layer transformer. At the time, a major concern was that this method might not scale feasibly to state-of-the-art transformers and, as a result, be unable to practically contribute to AI safety. Since then, scaling sparse autoencoders has been a major priority of the Anthropic interpretability team, and we're pleased to report extracting high-quality features from Claude 3 Sonnet , For clarity, this is the 3.0 version of Claude 3 Sonnet, released March 4, 2024. It is the exact model in production as of the writing of this paper. It is the finetuned model, not the base pretrained model (although our method also works on the base model). Anthropic's medium-sized production model. We find a diversity of highly abstract features. They both respond to and behaviorally cause abstract behaviors. Examples of features we find include features for famous people, features for countries and cities, and features tracking type signatures in code. Many features are multilingual (responding to the same concept across languages) and multimodal (responding to the same concept in both text and images), as well as encompassing both abstract and concrete instantiations of the same idea (such as code with security vulnerabilities, and abstract discussion of security vulnerabilities). Some of the features we find are of particular interest because they may be safety-relevant â€“ that is, they are plausibly connected to a range of ways in which modern AI systems may cause harm. In particular, we find features related to security vulnerabilities and backdoors in code ; bias (including both overt slurs, and more subtle biases); lying, deception, and power-seeking (including treacherous turns); sycophancy ; and dangerous / criminal content (e.g., producing bioweapons). However, we caution not to read too much into the mere existence of such features: there's a difference (for example) between knowing about lies, being capable of lying, and actually lying in the real world. This research is also very preliminary. Further work will be needed to understand the implications of these potentially safety-relevant features. Key Results Sparse autoencoders produce interpretable features for large models. Scaling laws can be used to guide the training of sparse autoencoders. The resulting features are highly abstract: multilingual, multimodal, and generalizing between concrete and abstract references. There appears to be a systematic relationship between the frequency of concepts and the dictionary size needed to resolve features for them. Features can be used to steer large models ( see e.g. Influence on Behavior ). This extends prior work on steering models using other methods (see Related Work ). We observe features related to a broad range of safety concerns, including deception , sycophancy , bias , and dangerous content . Scaling Dictionary Learning to Claude 3 Sonnet Our general approach to understanding Claude 3 Sonnet is based on the linear representation hypothesis ( see e.g. ) and the superposition hypothesis ( see e.g. ). For an introduction to these ideas, we refer readers to the Background and Motivation section of Toy Models . At a high level, the linear representation hypothesis suggests that neural networks represent meaningful concepts â€“ referred to as features â€“ as directions in their activation spaces. The superposition hypothesis accepts the idea of linear representations and further hypothesizes that neural networks use the existence of almost-orthogonal directions in high-dimensional spaces to represent more features than there are dimensions. If one believes these hypotheses, the natural approach is to use a standard method called dictionary learning . Recently, several papers have suggested that this can be quite effective for transformer language models . In particular, a specific approximation of dictionary learning called a sparse autoencoder appears to be very effective . To date, these efforts have been on relatively small language models by the standards of modern foundation models. Our previous paper , which focused on a one-layer model, was a particularly extreme example of this. As a result, an important question has been left hanging: will these methods work for large models? Or is there some reason, whether pragmatic questions of engineering or more fundamental differences in how large models operate, that would mean these efforts can't generalize? This context motivates our project of scaling sparse autoencoders to Claude 3 Sonnet, Anthropic's medium-scale production model. The rest of this section will review our general sparse autoencoder setup, the specifics of the three sparse autoencoders we'll analyze in this paper, and how we used scaling laws to make informed decisions about the design of our sparse autoencoders. From there, we'll dive into analyzing the features our sparse autoencoders learn â€“ and the interesting properties of Claude 3 Sonnet they reveal. Sparse Autoencoders Our high-level goal in this work is to decompose the activations of a model (Claude 3 Sonnet) into more interpretable pieces. We do so by training a sparse autoencoder (SAE) on the model activations, as in our prior work and that of several other groups ( e.g. ; see Related Work ). SAEs are an instance of a family of â€œsparse dictionary learningâ€ algorithms that seek to decompose data into a weighted sum of sparsely active components. Our SAE consists of two layers. The first layer (â€œencoderâ€) maps the activity to a higher-dimensional layer via a learned linear transformation followed by a ReLU nonlinearity. We refer to the units of this high-dimensional layer as â€œfeatures.â€ The second layer (â€œdecoderâ€) attempts to reconstruct the",
    "link": "https://transformer-circuits.pub/2024/scaling-monosemanticity/index.html"
  },
  {
    "id": "towards-monosemanticity-decomposing-language-model-1cd07f09",
    "title": "Towards Monosemanticity: Decomposing Language Models With Dictionary Learning",
    "summary": "Transformer Circuits Thread Towards Monosemanticity: Decomposing Language Models With Dictionary Learning Towards Monosemanticity: Decomposing Language Models With Dictionary Learning Using a sparse autoencoder, we extract a large number of interpretable features from a one-layer transformer. Browse A/1 Features â†’ Browse All Features â†’ Authors Trenton Bricken * , Adly Templeton * , Joshua Batson * , Brian Chen * , Adam Jermyn * , Tom Conerly, Nicholas L Turner, Cem Anil, Carson Denison, Amanda Askell, Robert Lasenby, Yifan Wu, Shauna Kravec, Nicholas Schiefer, Tim Maxwell, Nicholas Joseph, Alex Tamkin, Karina Nguyen, Brayden McLean, Josiah E Burke, Tristan Hume, Shan Carter, Tom Henighan, Chris Olah Affiliations Anthropic Published Oct 4, 2023 * Core Contributor; Correspondence to colah@anthropic.com ; Author contributions statement below . Mechanistic interpretability seeks to understand neural networks by breaking them into components that are more easily understood than the whole. By understanding the function of each component, and how they interact, we hope to be able to reason about the behavior of the entire network. The first step in that program is to identify the correct components to analyze. Unfortunately, the most natural computational unit of the neural network â€“ the neuron itself â€“ turns out not to be a natural unit for human understanding. This is because many neurons are polysemantic : they respond to mixtures of seemingly unrelated inputs. In the vision model Inception v1 , a single neuron responds to faces of cats and fronts of cars . In a small language model we discuss in this paper, a single neuron responds to a mixture of academic citations, English dialogue, HTTP requests, and Korean text. Polysemanticity makes it difficult to reason about the behavior of the network in terms of the activity of individual neurons. One potential cause of polysemanticity is superposition , a hypothesized phenomenon where a neural network represents more independent \"features\" of the data than it has neurons by assigning each feature its own linear combination of neurons. If we view each feature as a vector over the neurons, then the set of features form an overcomplete linear basis for the activations of the network neurons. In our previous paper on Toy Models of Superposition , we showed that superposition can arise naturally during the course of neural network training if the set of features useful to a model are sparse in the training data. As in compressed sensing, sparsity allows a model to disambiguate which combination of features produced any given activation vector. For more discussion of this point, see Distributed Representations: Composition and Superposition . In Toy Models of Superposition , we described three strategies to finding a sparse and interpretable set of features if they are indeed hidden by superposition: (1) creating models without superposition, perhaps by encouraging activation sparsity; (2) using dictionary learning to find an overcomplete feature basis in a model exhibiting superposition; and (3) hybrid approaches relying on a combination of the two. Since the publication of that work, we've explored all three approaches. We eventually developed counterexamples which persuaded us that the sparse architectural approach (approach 1) was insufficient to prevent polysemanticity, and that standard dictionary learning methods (approach 2) had significant issues with overfitting. In this paper, we use a weak dictionary learning algorithm called a sparse autoencoder to generate learned features from a trained model that offer a more monosemantic unit of analysis than the model's neurons themselves. Our approach here builds on a significant amount of prior work, especially in using dictionary learning and related methods on neural network activations (e.g. ), and a more general allied literature on disentanglement . We also note interim reports which independently investigated the sparse autoencoder approach in response to Toy Models , culminating in the recent manuscript of Cunningham et al. . The goal of this paper is to provide a detailed demonstration of a sparse autoencoder compellingly succeeding at the goals of extracting interpretable features from superposition and enabling basic circuit analysis. Concretely, we take a one-layer transformer with a 512-neuron MLP layer, and decompose the MLP activations into relatively interpretable features by training sparse autoencoders on MLP activations from 8 billion data points, with expansion factors ranging from 1Ã— (512 features) to 256Ã— (131,072 features). We focus our detailed interpretability analyses on the 4,096 features learned in one run we call A/1. This report has four major sections. In Problem Setup , we provide motivation for our approach and describe the transformers and sparse autoencoders we train. In Detailed Investigations of Individual Features , we offer an existence proof â€“ we make the case that several features we find are functionally specific causal units which don't correspond to neurons. In Global Analysis , we argue that the typical feature is interpretable and that they explain a non-trivial portion of the MLP layer. Finally, in Phenomenology we describe several properties of our features, including feature-splitting , universality , and how they can form \"finite state automata\"-like systems implementing interesting behaviors. We also provide three comprehensive visualizations of features. First, for all features from 90 learned dictionaries we present activating dataset examples and downstream logit effects. We recommend the reader begin with the visualization of A/1 . Second, we provide a data-oriented view , showing all features active on each token of 25 texts. Finally, we coembed all 4,096 features from A/1 and all 512 features from A/0 into the plane using UMAP to allow for interactive exploration of the space of features: Summary of Results Sparse Autoencoders extract relatively monosemantic features. We provide four different lines of evidence: detailed investigations for a few features firing in specific contexts for which we can construct computational proxies, human analysis for a large random sample of features, automated interpretability analysis of activations for all the features learned by the autoencoder, and finally automated interpretability analysis of logit weights for all the features. Moreover, the last three analyses show that most learned features are interpretable. While we do not claim that our interpretations catch all aspects of features'",
    "link": "https://transformer-circuits.pub/2023/monosemantic-features/index.html"
  },
  {
    "id": "deep-learning-nlp-and-representations-colahs-blog-963396fb",
    "title": "Deep Learning, NLP, and Representations - colah's blog",
    "summary": "Deep Learning, NLP, and Representations Posted on July 7, 2014 neural networks , deep learning , representations , NLP , recursive neural networks Introduction In the last few years, deep neural networks have dominated pattern recognition. They blew the previous state of the art out of the water for many computer vision tasks. Voice recognition is also moving that way. But despite the results, we have to wonderâ€¦ why do they work so well? This post reviews some extremely remarkable results in applying deep neural networks to natural language processing (NLP). In doing so, I hope to make accessible one promising answer as to why deep neural networks work. I think itâ€™s a very elegant perspective. One Hidden Layer Neural Networks A neural network with a hidden layer has universality: given enough hidden units, it can approximate any function. This is a frequently quoted â€“ and even more frequently, misunderstood and applied â€“ theorem. Itâ€™s true, essentially, because the hidden layer can be used as a lookup table. For simplicity, letâ€™s consider a perceptron network. A perceptron is a very simple neuron that fires if it exceeds a certain threshold and doesnâ€™t fire if it doesnâ€™t reach that threshold. A perceptron network gets binary (0 and 1) inputs and gives binary outputs. Note that there are only a finite number of possible inputs. For each possible input, we can construct a neuron in the hidden layer that fires for that input, 1 and only on that specific input. Then we can use the connections between that neuron and the output neurons to control the output in that specific case. 2 And so, itâ€™s true that one hidden layer neural networks are universal. But there isnâ€™t anything particularly impressive or exciting about that. Saying that your model can do the same thing as a lookup table isnâ€™t a very strong argument for it. It just means it isnâ€™t impossible for your model to do the task. Universality means that a network can fit to any training data you give it. It doesnâ€™t mean that it will interpolate to new data points in a reasonable way. No, universality isnâ€™t an explanation for why neural networks work so well. The real reason seems to be something much more subtleâ€¦ And, to understand it, weâ€™ll first need to understand some concrete results. Word Embeddings Iâ€™d like to start by tracing a particularly interesting strand of deep learning research: word embeddings. In my personal opinion, word embeddings are one of the most exciting area of research in deep learning at the moment, although they were originally introduced by Bengio, et al. more than a decade ago. 3 Beyond that, I think they are one of the best places to gain intuition about why deep learning is so effective. A word embedding \\(W: \\mathrm{words} \\to \\mathbb{R}^n\\) is a paramaterized function mapping words in some language to high-dimensional vectors (perhaps 200 to 500 dimensions). For example, we might find: \\[W(``\\text{cat}\\!\") = (0.2,~ \\text{-}0.4,~ 0.7,~ ...)\\] \\[W(``\\text{mat}\\!\") = (0.0,~ 0.6,~ \\text{-}0.1,~ ...)\\] (Typically, the function is a lookup table, parameterized by a matrix, \\(\\theta\\) , with a row for each word: \\(W_\\theta(w_n) = \\theta_n\\) .) \\(W\\) is initialized to have random vectors for each word. It learns to have meaningful vectors in order to perform some task. For example, one task we might train a network for is predicting whether a 5-gram (sequence of five words) is â€˜valid.â€™ We can easily get lots of 5-grams from Wikipedia (eg. â€œcat sat on the matâ€) and then â€˜breakâ€™ half of them by switching a word with a random word (eg. â€œcat sat song the matâ€), since that will almost certainly make our 5-gram nonsensical. Modular Network to determine if a 5-gram is â€˜validâ€™ (From Bottou (2011) ) The model we train will run each word in the 5-gram through \\(W\\) to get a vector representing it and feed those into another â€˜moduleâ€™ called \\(R\\) which tries to predict if the 5-gram is â€˜validâ€™ or â€˜broken.â€™ Then, weâ€™d like: \\[R(W(``\\text{cat}\\!\"),~ W(``\\text{sat}\\!\"),~ W(``\\text{on}\\!\"),~ W(``\\text{the}\\!\"),~ W(``\\text{mat}\\!\")) = 1\\] \\[R(W(``\\text{cat}\\!\"),~ W(``\\text{sat}\\!\"),~ W(``\\text{song}\\!\"),~ W(``\\text{the}\\!\"),~ W(``\\text{mat}\\!\")) = 0\\] In order to predict these values accurately, the network needs to learn good parameters for both \\(W\\) and \\(R\\) . Now, this task isnâ€™t terribly interesting. Maybe it could be helpful in detecting grammatical errors in text or something. But what is extremely interesting is \\(W\\) . (In fact, to us, the entire point of the task is to learn \\(W\\) . We could have done several other tasks â€“ another common one is predicting the next word in the sentence. But we donâ€™t really care. In the remainder of this section we will talk about many word embedding results and wonâ€™t distinguish between different approaches.) One thing we can do to get a feel for the word embedding space is to visualize them with t-SNE , a sophisticated technique for visualizing high-dimensional data. t-SNE visualizations of word embeddings. Left: Number Region; Right: Jobs Region. From Turian et al. (2010) , see complete image . This kind of â€˜mapâ€™ of words makes a lot of intuitive sense to us. Similar words are close together. Another way to get at this is to look at which words are closest in the embedding to a given word. Again, the words tend to be quite similar. What words have embeddings closest to a given word? From Collobert et al. (2011) It seems natural for a network to make words with similar meanings have similar vectors. If you switch a word for a synonym (eg. â€œa few people sing wellâ€ \\(\\to\\) â€œa couple people sing wellâ€), the validity of the sentence doesnâ€™t change. While, from a naive perspective, the input sentence has changed a lot, if \\(W\\) maps synonyms (like â€œfewâ€ and â€œcoupleâ€) close together, from \\(R\\) â€™s perspective little changes. This is very powerful. The number of possible 5-grams is massive and we have a comparatively small number of data points to try to learn from. Similar words being close together",
    "link": "https://colah.github.io/posts/2014-07-NLP-RNNs-Representations/"
  },
  {
    "id": "neural-networks-manifolds-and-topology-colahs-blog-b498716e",
    "title": "Neural Networks, Manifolds, and Topology -- colah's blog",
    "summary": "Neural Networks, Manifolds, and Topology Posted on April 6, 2014 topology, neural networks, deep learning, manifold hypothesis Recently, thereâ€™s been a great deal of excitement and interest in deep neural networks because theyâ€™ve achieved breakthrough results in areas such as computer vision. 1 However, there remain a number of concerns about them. One is that it can be quite challenging to understand what a neural network is really doing. If one trains it well, it achieves high quality results, but it is challenging to understand how it is doing so. If the network fails, it is hard to understand what went wrong. While it is challenging to understand the behavior of deep neural networks in general, it turns out to be much easier to explore low-dimensional deep neural networks â€“ networks that only have a few neurons in each layer. In fact, we can create visualizations to completely understand the behavior and training of such networks. This perspective will allow us to gain deeper intuition about the behavior of neural networks and observe a connection linking neural networks to an area of mathematics called topology. A number of interesting things follow from this, including fundamental lower-bounds on the complexity of a neural network capable of classifying certain datasets. A Simple Example Letâ€™s begin with a very simple dataset, two curves on a plane. The network will learn to classify points as belonging to one or the other. The obvious way to visualize the behavior of a neural network â€“ or any classification algorithm, for that matter â€“ is to simply look at how it classifies every possible data point. Weâ€™ll start with the simplest possible class of neural network, one with only an input layer and an output layer. Such a network simply tries to separate the two classes of data by dividing them with a line. That sort of network isnâ€™t very interesting. Modern neural networks generally have multiple layers between their input and output, called â€œhiddenâ€ layers. At the very least, they have one. Diagram of a simple network from Wikipedia As before, we can visualize the behavior of this network by looking at what it does to different points in its domain. It separates the data with a more complicated curve than a line. With each layer, the network transforms the data, creating a new representation . 2 We can look at the data in each of these representations and how the network classifies them. When we get to the final representation, the network will just draw a line through the data (or, in higher dimensions, a hyperplane). In the previous visualization, we looked at the data in its â€œrawâ€ representation. You can think of that as us looking at the input layer. Now we will look at it after it is transformed by the first layer. You can think of this as us looking at the hidden layer. Each dimension corresponds to the firing of a neuron in the layer. The hidden layer learns a representation so that the data is linearly separable Continuous Visualization of Layers In the approach outlined in the previous section, we learn to understand networks by looking at the representation corresponding to each layer. This gives us a discrete list of representations. The tricky part is in understanding how we go from one to another. Thankfully, neural network layers have nice properties that make this very easy. There are a variety of different kinds of layers used in neural networks. We will talk about tanh layers for a concrete example. A tanh layer \\(\\tanh(Wx+b)\\) consists of: A linear transformation by the â€œweightâ€ matrix \\(W\\) A translation by the vector \\(b\\) Point-wise application of tanh. We can visualize this as a continuous transformation, as follows: The story is much the same for other standard layers, consisting of an affine transformation followed by pointwise application of a monotone activation function. We can apply this technique to understand more complicated networks. For example, the following network classifies two spirals that are slightly entangled, using four hidden layers. Over time, we can see it shift from the â€œrawâ€ representation to higher level ones it has learned in order to classify the data. While the spirals are originally entangled, by the end they are linearly separable. On the other hand, the following network, also using multiple layers, fails to classify two spirals that are more entangled. It is worth explicitly noting here that these tasks are only somewhat challenging because we are using low-dimensional neural networks. If we were using wider networks, all this would be quite easy. (Andrej Karpathy has made a nice demo based on ConvnetJS that allows you to interactively explore networks with this sort of visualization of training!) Topology of tanh Layers Each layer stretches and squishes space, but it never cuts, breaks, or folds it. Intuitively, we can see that it preserves topological properties. For example, a set will be connected afterwards if it was before (and vice versa). Transformations like this, which donâ€™t affect topology, are called homeomorphisms. Formally, they are bijections that are continuous functions both ways. Theorem : Layers with \\(N\\) inputs and \\(N\\) outputs are homeomorphisms, if the weight matrix, \\(W\\) , is non-singular. (Though one needs to be careful about domain and range.) Proof : Letâ€™s consider this step by step: Letâ€™s assume \\(W\\) has a non-zero determinant. Then it is a bijective linear function with a linear inverse. Linear functions are continuous. So, multiplying by \\(W\\) is a homeomorphism. Translations are homeomorphisms tanh (and sigmoid and softplus but not ReLU) are continuous functions with continuous inverses. They are bijections if we are careful about the domain and range we consider. Applying them pointwise is a homeomorphism Thus, if \\(W\\) has a non-zero determinant, our layer is a homeomorphism. âˆŽ This result continues to hold if we compose arbitrarily many of these layers together. Topology and Classification \\(A\\) is red, \\(B\\) is blue Consider a two dimensional dataset with two classes \\(A, B \\subset \\mathbb{R}^2\\) :",
    "link": "https://colah.github.io/posts/2014-03-NN-Manifolds-Topology/"
  },
  {
    "id": "ai-for-the-rest-of-us-by-nathan-lambert-interconne-21ed8818",
    "title": "AI for the rest of us - by Nathan Lambert - Interconnects",
    "summary": "AI for the rest of us Apple Intelligence makes a lot of sense when you get out of the AI bubble. Plus, the cool technical details Apple shared about their language models \"thinking different.\" Nathan Lambert Jun 12, 2024 31 3 Share By being quiet in the race for the biggest and most rad foundation models, many people assumed that Apple didnâ€™t have anything to contribute to the AI race. After their new announcements, many in the heart of the AI labs still will argue this, forecasting that a marginal lead on language model performance has an effectively infinite expected value . Apple is betting that AI follows paths paved by previous technological revolutions: incremental progress to transformational results. With a large swath of new features, many of which are built into a new multi-model AI system called Apple Intelligence , Apple has demonstrated how meaningful AI interactions can be built into every corner of our digital life â€” connecting apps and making tasks easier. By executing this well, Apple can guarantee that AI is a force multiplier that keeps even people using their devices longer, rather than pushing them to other upstart companies or competitive products. Appleâ€™s presentation rang very different than most AI keynotes weâ€™ve seen in the last few years. While OpenAI and Google are trying to prove that they are the best at AI, Apple leaned into a narrative of what else we can do with AI. Appleâ€™s large suite of new AI features coming this fall across all their devices, enabling automation, information retrieval, and generation in a privacy-conscious way will be the first time that many people meaningfully interact with AI. This is the latest chapter that the story of the internet is unfolding at the junction between Apple and Meta. Two companies, who should really be compliments, consistently try to one-up and out-position each other. A few months ago with the launch of Llama 3, Meta launched Meta AI , which is generally available and rolling out to Metaâ€™s 3 billion users. Meta AI is much more general. It is trying to be a chatbot, entertainment, and an information store. Its competition includes ChatGPT and all the other places people can go for this information. Meta AI should really be integrated next to ChatGPT into operating system features, but this is very unlikely. Appleâ€™s new features will be rolled out on its newest devices (e.g. iPhone 15 Pro and later, M1 Macs and later), which eventually can reach all of their 2 billion users as well. It presents a very different view of what AI is to the general public. Apple is framing what they built as being fundamentally different from ChatGPT, Claude, Meta AI, Gemini results, etc., and it is probably right. While the many features they announced are interesting, and I will be upgrading my phone this cycle to try them out, they have an extremely high variance on how well theyâ€™ll land in the final product (e.g. how Microsoft Recall needed substantial updates post announcement due to security issues). Photo generation, decent on-device generation, better Siri, textual Siri, webpage summaries, etc. all will serve different audiences. Today, beyond the focus on how transformative it is for billions of people to have access to AI rather than hundreds of millions, I want to dig into the interesting technical details Apple has shared with us. Interconnects is a reader-supported publication. Consider becoming a subscriber. Subscribe Appleâ€™s technical approach All the evidence in WWDC and related communications, such as a foundation model blog post , showcase that Apple is right where it needs to be with AI on the technical side. Given the amount of AI products that are announced and never delivered, passing the bar on technical capabilities via clear communications with the ML community is important for independent monitoring of the performance of top labs (i.e. what I, and many investors, ML nerds, etc. all try to do). I split this summary into three core categories: 1) Core model performance , 2) adapter and on-device strategy , and 3) alignment strategies . The general approach that Apple is taking is highlighted in this great 6-minute video from the WWDC keynote . In summary, Apple foundation models focus on personalization (alignment strategies), performance (core models), and size (on-device strategies). Only the last one is orthogonal from other developers out there, but it surely added substantial constraints that inspired the other two. Core models: What did Apple build? There are a lot of details in the blog post about their models, covering everything from safety to summarization to email completion. At the end of the day, most of these evaluations are extremely opaque, so Iâ€™m leaning on a lot of trust in the company's reputation and general context to make predictions on the actual quality of their models. The most central figure to my analysis is the Apple human evaluation versus other top language models. This evaluation likely favors Apple by at least a few percentage points. The things theyâ€™re training these models for are explicitly different than what ChatGPT is â€” otherwise, why would Apple have the option to send any prompts to other services? These models are about things that iPhone users already could use AI for. Regardless, the scores are still solid enough to make a strong case. We can see that the on-device model, which is about 3 billion parameters, performs very well versus other models out there. Models like Gemma and Phi 3 are more research or marketing artifacts than models central to future business strategy, so Apple really ought to be crushing models here. Appleâ€™s Server Model is where things get a little more interesting. The models theyâ€™re comparing to, except GPT-4, mostly are considered B-tier by the community. This looks like 2 years after the release (edit) training of GPT-4, Apple has approximately trained an original GPT-4 level model. This is likely not their ceiling, but the sweet spot they found with respect to their costs",
    "link": "https://www.interconnects.ai/p/apple-intelligence"
  },
  {
    "id": "research-code-non_interactive-software-ml-1256127f",
    "title": "Research Code â€“ Non_Interactive â€“ Software & ML",
    "summary": "At my job, Iâ€™m currently in a cycle that is involving working with software engineers quite a bit. One thing that has happened a number of times is that a software engineer will bring up â€œresearch codeâ€ with a condescending tone. The implication is that research code is messy, unreadable, and difficult to maintain. I donâ€™t deny this! It often is those things, but I also think it has a beauty to its purpose and prose that is worth acknowledging. Most code has a purpose from the get go. Someone thinks â€œwouldnâ€™t it be nice if my computer did <x>â€, then designs and builds the code to achieve <x>. Generally you know where you want to end up and how to get there fairly early on in the project. This makes writing design docs, unit tests and a coherent software architecture possible. Research doesnâ€™t really work in this way. It is similarly goal oriented, but the distinction is that you never really know exactly how youâ€™re going to get to the goal. You constantly poke at the unknown until something â€œworksâ€. You get stuck in iterative loops improving small aspects of your design over weeks or months â€“ which naturally causes it to evolve. At the end of the journey the goalposts have moved, more often than not. In such an environment, you take notes rather than write design docs. They describe where youâ€™ve been and why the things you tried didnâ€™t work. You test one-off implementations directly rather than write unit tests. If something works, you cement it in place as a beacon marking a step towards your goal. If youâ€™re fortunate enough to be working on a problem that allows evaluation metrics, you might record the eval scores you achieve. It might make sense to write a unit test here, but itâ€™s often easier just to treat the beacon code as immutable and branch off of it. I love this way of iterating on a problem. I always have, well before I joined the field. Itâ€™s haphazard as hell, but it also feels as close to â€œartâ€ as I think programming will ever get. Let me explain that a bit: art is more often then not about individual expression. Programming for most large companies is explicitly not that: itâ€™s about writing in common language with your fellow programmers as well as sharing â€œcultural normsâ€ like coding conventions, reviews and tests. There are good reasons for all of these things! But the end product is rarely â€œartâ€. By contrast, the code that a researcher builds is a product of the interaction between their individual ingenuity and the real world. Itâ€™s organic and complex and two researchers tackling a similar problem rarely come up with the same code at the end of the day. I call that â€œartâ€.",
    "link": "https://nonint.com/2024/03/16/research-code/"
  },
  {
    "id": "compute-multipliers-non_interactive-software-ml-f16603ae",
    "title": "Compute Multipliers â€“ Non_Interactive â€“ Software & ML",
    "summary": "Iâ€™ve listened to a couple of interviews with Dario Amodei, CEO of Anthropic, this year. In both of them, he dropped the term â€œcompute multiplierâ€ a few times. This concept is exceptionally important in the field of ML, and I donâ€™t see it talked about enough. In this post, Iâ€™m going to attempt to explain what it is and why it is so important. Computational Efficiency Chinchilla is undoubtedly the landmark academic paper of 2022 in the field of Machine Learning. Itâ€™s most known for documenting the optimal relationship between the amount of compute poured into training a neural network and the amount of data used to train said network. In the process, it refuted some of the findings of an OpenAI paper from 2020, Scaling Laws for Neural Language models, which claimed that the optimal data:compute ratio was far smaller than was correct. (By the way, I highly recommend this post if you want to read more about Chinchillaâ€™s findings) Chinchilla did another thing, though â€“ it highlighted the importance of studying the computational efficiency of our learning algorithms. Lets dig in there a bit: Compute efficiency is a measurement of how well your model performs taking into account only the amount of compute you used to train it. â€œHow well your model performsâ€ is generally measured by a loss function applied to a test dataset, but can be measured using any stable, low-variance metric that gets improved as your model trains. â€œComputeâ€ is generally measured in FLOPs, but it can be thought of as â€œhow many GPUs, for how longâ€. Multipliers Putting the two together, you can re-define compute efficiency a little bit: given a set of design and hyper-parameter choices and a fixed test loss target, compute efficiency is the measurement of how much compute is required to meet that test loss. A more efficient model requires less GPUs, a less efficient one needs more. And this is where the term â€œcompute multipliersâ€ come in. If you make any discovery that improves compute efficiency across all model scales, you have discovered a compute multipler. If your discovery increases efficiency by 20%, itâ€™s as if your training fleet suddenly has 20% more GPUs in it. Due to the way scaling actually works, compute multipliers are actually generally worth more than a proportional increase in the number of GPUs you have. This is because adding more GPUs comes with overhead which decreases the net efficiency of the system, for example slow interconnect speeds might mean that a GPU cluster that is 20% larger is only 18% faster at crunching your giant matmuls. In a world where a single H100 costs $25k at a minimum, and weâ€™re training these LLMs on thousands of these GPUs, you can see why these compute multipliers start to make a huge difference. Finding a 5% compute multiplier means you potentially saved your 4000-GPU company $5M in GPUs. That savings can be applied in several ways: buying less GPUs, training faster, or training a larger & better model. Where are they hiding? I started this post mentioning that Dario had talked about compute multipliers. The context within which he talked about them in both podcasts I listened to was within an infosec discussion â€“ he considers proprietary compute multipliers to be the among the most valuable corporate secrets that Anthropic has. As such, I obviously canâ€™t share specifics on where you might go to look for these things. Thatâ€™s OK, though, as the point of this post is to foster a different way of thinking about how improvements are made to ML algorithms. I think the search for compute multipliers is far more ubiquitous than anyone who doesnâ€™t religiously adhere to scaling laws might think: literally every single scientist in the field of machine learning that isnâ€™t looking into new applications of existing techniques should be performing compute efficiency scans to ensure that their discoveries are actually relevant. Some examples: You invent a new architecture that proposes to replace the transformer â€“ you better show that you achieve better test losses than a transformer with a fixed compute budget! You create a new dataset â€“ does it improve a test loss that you care about when compared with an otherwise identical model trained for an identical duration of time/tokens? You invent a new optimizer algorithm â€“ what test loss does it achieve when compared with the old one? You tweak some hyperparameters â€“ â€¦. you get the point In general, the important conclusion is that if you believe in scaling laws (and you should!), then it is not impressive to simply come up with an idea and achieve a state of the art score on some evaluation metric. Anyone can do that with any architecture and enough compute. You must measure the performance of your idea against other ideas with compute fixed. Funnily enough if everyone did this, weâ€™d see a lot less papers in the field as itâ€™d become clear pretty quickly that most ideas simply donâ€™t pan out where it matters. The only reason the paper mills keep turning is that just about everything can be SOTA with enough scale, compute and eval hacking.",
    "link": "https://nonint.com/2023/11/05/compute-multipliers/"
  },
  {
    "id": "on-the-efficiency-of-human-intelligence-non_intera-8bd1e2c5",
    "title": "On the efficiency of human intelligence â€“ Non_Interactive â€“ Software & ML",
    "summary": "A pet peeve of mine that often shows up in ML discourse is the claim that humans are much more data efficient at learning than the models we are currently training. The argument typically goes like this: â€œIâ€™m blown away by how much knowledge my 3 year old has. They are smarter than most language models, despite being trained on a very small training dataset. Clearly, our models are missing something important because they cannot learn like my 3 year old!â€ But is the training dataset of a 3 year old actually smaller than a typical language model? For fun, Iâ€™d like to do some napkin math to bring the numbers down to levels that we can actually reason over. Starting with the LLM itself â€“ letâ€™s use Llama 65B. This model was trained on 1.4T tokens. For some easy math, letâ€™s assume that the codebook size was 65536, which means that each token represents 16 bits of data. That means Llama was trained on 22.4 TBits of data in total. Human â€œtraining dataâ€ Letâ€™s try to figure out how much information a human can gather in 3 years. To do this, weâ€™ll first decompose our world experience into individual â€œexperiencesâ€ which happen at a regular interval across those 3 years. Letâ€™s say that a human has a new experience every second (itâ€™s probably more frequent than this). Letâ€™s also assume that the human is awake on average 12 hours a day. Over a time span of 3 years, that means a human will have 3 * 365 * 12 * 60 * 60 = 47,304,000 experiences. Letâ€™s now compare those 3 years of experience with the at a we used to train that 65B Llama model: 22,400,000,000,000 Bits / 47,304,000 experiences = 473,532 = 474 KBit/experience. That is to say â€“ if every given human experience has more than 474 KBit of information, than a 3 year old human is technically getting trained on more raw information than Llama 65B. Letâ€™s digest that a bit further by the modalities of human experience: Vision The internet tells me that a human eye can perceive 576 Megapixels and 10M colors. 10M colors is ~23Bits. Weâ€™ve got two eyes so that comes out to 576,000,000 pixels * 23 bits/pixels * 2 possible states per observation. That comes out to 26GBit per experience. Hm. I donâ€™t think the human brain actually perceives all of the visual information it is presented with. Rather, it focuses on a very small fraction (attention!). But even a very small fraction of 26GBit is a big number! Basically any way you try to pare this number down, itâ€™s going to be big. Audio Young humans can perceive sound frequencies up to 20kHz. I donâ€™t really know how to measure how fine-grained the pressure fluctuations our ears can perceive are, Iâ€™ll use 8 bits (255 pressure values) as a reasonable lower bound. That means over the course of one second, a human could theoretically perceive 20,000*8 = 160KBits of audio data. Touch, smell and taste AFAIU, smelling is performed by chemicals binding to smell receptors. The action of binding is an on or off proposition, and the internet tells me we have ~400 different smell receptors. That comes out to an easy 400 bits of information from smell. Iâ€™ll measure touch similarly â€“ the internet tells me we have ~4M touch receptors (why does the number 4 keep coming up?). Each one is independent and (I assume?) can be on or off, which comes out to 4MBits of touch information. Taste is a complex amalgam of touch, smell and tastebuds. We can taste something like 5 independent tastes, Iâ€™ll assume smell and touch is covered above. So letâ€™s say taste is a simple 5 bits of information. Summing up Iâ€™m not going to bother adding all these information sources together as I think this is all pretty pie in the sky. The point Iâ€™m trying to make â€“ and I hope Iâ€™ve demonstrated it clearly â€“ is that we can pretty easily make the argument that the human brain receives 474 KBit of information per second. And if we can make this claim, then we can also make the claim that a 3 year old has very likely been trained on as much data as Llama 65B (though I suspect itâ€™s quite a lot more!!) Information redundancy I expect the skeptics reading this will make the following counterpoint: most human experiences are redundant! Even though the total information input is very dense, the amount of novel information is quite small! But you can say the same thing about our models! The text datasets these models are trained on is composed of all the human text on the internet. This is necessarily highly redundant â€“ humans love to talk about the same things, day in and day out. Politics, sex, war, food, dieting, exercise, sports, fashion, etc. The vast majority of human text has a very low amount of semantic entropy. I actually think redundancy is very important to a learning system. It aids in the act of compression, which seems to be linked to intelligence. By being exposed to the same observations over long periods of time, we learn what matters and what does not. Highly redundant experiences fade from our attention and we instead focus on novel, unexpected occurrences. I would not be surprised to learn that our models work in the same way. Just the fact that training over multiple epochs with sensible data augmentations improves model performance seems like a small signal that this is the case. Other types of efficiency Itâ€™s disingenuous to only consider data efficiency when talking about the human brain. When it comes to energy efficiency, our brains are pretty damned remarkable. I donâ€™t think weâ€™re even within a few orders of magnitude of that type of efficiency with silicon. This gives me a lot of hope, though! If capabilities are already as awesome as they are at the paltry efficiencies weâ€™ve",
    "link": "https://nonint.com/2023/07/05/on-the-efficiency-of-human-intelligence/"
  },
  {
    "id": "learned-structures-non_interactive-software-ml-9097b67e",
    "title": "Learned Structures â€“ Non_Interactive â€“ Software & ML",
    "summary": "From 2019-2021, I was fascinated with neural network architectures. I think a lot of researchers in the field were at the time. The transformer paper had been out for a little while and it was starting to sink in how transformational it was going to be. The general question in the air was: what other simple tweaks can we make to greatly improve performance? As time has passed, Iâ€™ve internally converged on the understanding that there are only a few types of architectural tweaks that actually meaningfully impact performance across model scales. These tweaks seem to fall into one of two categories: modifications that improve numerical stability during training, and modifications that enhance the expressiveness of a model in learnable ways. Improving numerical stability is a bit of a black art. Iâ€™m not an expert but those that are remind me of the RF engineers I worked with in my first job. Things that fit into this category would include where and how to normalize activations, weight initialization and smoothed non-linearities. Iâ€™d love to talk more about this someday. I wanted to talk about learnable expressiveness in this post. The core idea here is to build structured representations of your data, and let those structures interact in learnable ways. Letâ€™s start by looking at different ways this currently can happen: MLPs are the most basic building block of a neural network and provide the foundation of interacting structures: they allow all of the elements of a vector to interact with each other through the weights of the neural network. Attention builds another layer: rather than considering just a single vector interacting with weights, we consider a set of vectors. Through the attention layer, elements from this set can interact with each other. Mixture of Experts adds yet another layer: Rather than considering vectors interacting with a fixed set of weights, we now dynamically select the weights to use for other operations based on the values within the vector (and some more weights!) Hopefully youâ€™re seeing the pattern here: in each of the above cases, we add an axis by which our activations can affect the end result of the computation performed by our neural network. I have no empirical proof for this, but what I think is actually happening here is that as you add these nested structures into the computational graph, you are adding ways for the network to learn in stages . Why is important to learn in stages? Because we train our neural networks in a really, really dumb way: we optimize the entire parameter space from the beginning of training. This means all of the parameters fight from the very beginning to optimize really simple patterns of the data distribution. 7 Billion parameters learning that â€œparkâ€ and â€œfrisbeeâ€ are common words to find around â€œdogâ€. The neat thing about these learned structures is that theyâ€™re practically useless in the early training regime. Attention cannot be meaningfully learned while the network is still learning â€œblackâ€ from â€œwhiteâ€. Same with MoE: expert routing amounts to random chance when the network activations are akin to random noise. As training progresses, these mechanisms come â€œonlineâ€, though: providing meaningful value just when you need a boost in capacity to learn a more complex layer of the data distribution. Anyhow, regardless of whether or not my philosophical waxing is correct, learnable structures are probably the most fascinating research direction I can think of in architecture right now. My hunch is that there are additional structures that we can bolt onto our neural networks for another meaningful increase in performance. The main thing to pay attention to is that you are not just re-inventing a type of learned structure that already exists. Like Mamba. ðŸ™‚ One idea along this vein that I had explored before joining OpenAI: StyleGAN is an image generation model with exceptional fidelity and speed. The catch is that it is an extremely narrow learning framework: It only works when you heavily regularize the dataset you train it on. For example, only photos of center-cropped faces, or specific types of churches. If you attempt to train it on something like LAION quality drops off as you lose the ability to model the data distribution: itâ€™s just too wide to fit in the parameter space. But hereâ€™s the thing: you can think of most images as being made up of several modal components. Maybe a persons face here, a hand there, a tree in the background. It seems to me that an optimal way to get high generation performance and fidelity would be to train StyleGAN-like things separately from an image â€œcomposerâ€ that learns to place the correct StyleGAN over the correct places in an image to decode. A â€œmixture of StyleGANsâ€ if you will. As a final note: I donâ€™t want to claim the above is novel or anything, just a good idea. I think one of my favorite early applications of this general idea is using StyleGAN to fix StableDiffusion faces, like this . I want to try something like this learned end to end someday!",
    "link": "https://nonint.com/2024/03/03/learned-structures/"
  },
  {
    "id": "successful-language-model-evals-jason-wei-bb8095f7",
    "title": "Successful language model evals â€” Jason Wei",
    "summary": "Successful language model evals May 24 Written By Jason Wei Everybody uses evaluation benchmarks (â€œevalsâ€), but I think they deserve more attention than they are currently getting. Evals are incentives for the research community, and breakthroughs are often closely linked to a huge performance jump on some eval. In fact, Iâ€™d argue that a key job of the team lead is to dictate what eval to optimize. What is the definition of a successful eval? Iâ€™d say that if an eval is used in breakthrough papers and trusted within the community, then itâ€™s clearly successful. Here are some of the successful evals of the past five years: GLUE / SuperGLUE was used by basically all NLP papers in the pre-LLM era (BERT, T5, etc). MMLU is used by almost all LLM papers. Itâ€™s the favorite eval of DeepMind and Google. GSM8K spurred LLMs for reasoning, and is used in every paper on chain-of-thought. MATH is also used in most LLM papers. HumanEval is the classic eval for LLMs for coding. Obviously this isnâ€™t a comprehensive listâ€”there are other good evals like HellaSwag , SQuAD , etc. I made two evals that are somewhat popular. MGSM is used in OpenAIâ€™s simple evals, Claude, and Gemini. BBH was used in Claude, Gemini, and Llama. I think theyâ€™re decent but not among the best. One common thing among the successful evals is that a big paper claims some victory using the eval. GLUE was promoted by BERT. MMLU was promoted by Gopher, Chinchilla, and Flan-PaLM. Chain-of-thought prompting claimed a breakthrough on GSM8K. The prowess of Minerva was shown on MATH. HumanEval was attempted by Codex and others. Going one level deeper, a good score on the eval must mean something significant and easily understandable. For instance, achieving superhuman performance is very understandable. Solving grade-school level math problems is also something people can easily grasp the significance of. Itâ€™s easier to mess up an eval than to make a good one. Most of the non-successful evals make at least one mistake. If an eval doesnâ€™t have enough examples, it will be noisy and a bad UI for researchers. For example, someone might run the eval over the course of model training and see that it fluctuates wildly from checkpoint to checkpoint. This makes the eval painful for researchers, and they wonâ€™t like using it. Itâ€™s good to have at least 1,000 examples for your eval; perhaps more if itâ€™s a multiple choice eval. Even though GPQA is a good eval, the fact that it fluctuates based on the prompt makes it hard to use. Evals should be high quality. If there are a lot of mistakes in your eval, people wonâ€™t trust it. For example, I used Natural Questions (NQ) for a long time. But GPT-4 crossed the threshold where if GPT-4 got a test-example incorrect, it was more likely that the ground truth answer provided by the eval was wrong. So I stopped using NQ. If your eval is too complicated, it will be hard for people to understand it and it will simply be used less. I think the first version of HELM was a great effort, but it had way too many metrics and subsets. Itâ€™s critical to have a single-number metricâ€”I canâ€™t think of any great evals that donâ€™t have a single-number metric. If your eval takes too much work to run, it wonâ€™t gain traction even if everything else is good. BIG-Bench is one of my favorite evals, but it is a great pain to run. There were both log-prob evals and generation evals, which required different infra. There were way too many subsets, and some of them had too many examples, so it took a long time. I believe thatâ€™s why BIG-Bench didnâ€™t gain much traction, even though it provided a lot of signal. If an eval is not on a meaningful task, AI researchers wonâ€™t deeply care about it. For example, in BIG-Bench Hard we had tasks like recommending movies or closing parentheses properly. These tasks were challenging and trended well with model size, but doing well on them didnâ€™t allow for making a substantive conclusion about the intelligence of the model. Successful evals often measure things central to intelligence, like language understanding, exam problems, or math. The grading in your eval should be extremely correct. If someone is debugging why their model got graded incorrectly, and they disagree with the grading, thatâ€™s a quick way for them to write-off your eval immediately. Itâ€™s worth spending the time to minimize errors due to parsing, or to have the best autograder prompt possible. For the eval to stand the test of time, performance must not become saturated too quickly. For example, GLUE/SuperGLUE got saturated too quickly that it was hard to show big gains, and people stopped using them. Language models also got good at tasks like summarization and translation faster than we could develop good evals for them, and so we stopped measuring those tasks. Funny enough, it seems like most of the great evals have atrocious names. GSM8K didnâ€™t need theâ€œ8Kâ€, and HumanEval doesnâ€™t actually use humans for evaluation (itâ€™s called HumanEval because the problems were created by humans). MATH was too generic, so people started calling it â€œHendrycks-mathâ€, which I suppose is a clever way to get people to name an eval after you. If you want your eval to be successful, you should help people use it. For instance, when I make an eval, I usually offer to run it for other people on their models. If their model does well, theyâ€™ll like the eval and promote it. HELM does a great job of trying to evaluate other peopleâ€™s models for them, and publicizing the results. It also helps if you can create incentives for people to use your eval. One of the best incentives for people is what their manager values. So it can pay off to get buy-in on your eval from managers within your lab or company, who will ask their",
    "link": "https://www.jasonwei.net/blog/evals"
  },
  {
    "id": "accessing-gpt-4-level-mathematical-olympiad-soluti-fa2cc100",
    "title": "Accessing GPT-4 level Mathematical Olympiad Solutions via Monte Carlo Tree Self-refine with LLaMa-3 8B",
    "summary": "This paper introduces the MCT Self-Refine (MCTSr) algorithm, an innovative integration of Large Language Models (LLMs) with Monte Carlo Tree Search (MCTS), designed to enhance performance in complex mathematical reasoning tasks. Addressing the challenges of accuracy and reliability in LLMs, particularly in strategic and mathematical reasoning, MCTSr leverages systematic exploration and heuristic self-refine mechanisms to improve decision-making frameworks within LLMs. The algorithm constructs a Monte Carlo search tree through iterative processes of Selection, self-refine, self-evaluation, and Backpropagation, utilizing an improved Upper Confidence Bound (UCB) formula to optimize the exploration-exploitation balance. Extensive experiments demonstrate MCTSr's efficacy in solving Olympiad-level mathematical problems, significantly improving success rates across multiple datasets, including GSM8K, GSM Hard, MATH, and Olympiad-level benchmarks, including Math Odyssey, AIME, and OlympiadBench. The study advances the application of LLMs in complex reasoning tasks and sets a foundation for future AI integration, enhancing decision-making accuracy and reliability in LLM-driven applications.",
    "link": "https://arxiv.org/abs/2406.07394v1"
  },
  {
    "id": "getting-50-sota-on-arc-agi-with-gpt-4o-9c8d7de4",
    "title": "Getting 50% (SoTA) on ARC-AGI with GPT-4o",
    "summary": "Getting 50% (SoTA) on ARC-AGI with GPT-4o You can just draw more samples Ryan Greenblatt Jun 17, 2024 129 45 11 Share I recently got to 50% 1 accuracy on the public test set for ARC-AGI by having GPT-4o generate a huge number of Python implementations of the transformation rule (around 8,000 per problem) and then selecting among these implementations based on correctness of the Python programs on the examples (if this is confusing, go to the next section) 2 . I use a variety of additional approaches and tweaks which overall substantially improve the performance of my method relative to just sampling 8,000 programs. [This post is on a pretty different topic than the usual posts on our substack. So regular readers should be warned!] The additional approaches and tweaks are: I use few-shot prompts which perform meticulous step-by-step reasoning. I have GPT-4o try to revise some of the implementations after seeing what they actually output on the provided examples. I do some feature engineering , providing the model with considerably better grid representations than the naive approach of just providing images. (See below for details on what a â€œgridâ€ in ARC-AGI is.) I used specialized few-shot prompts for the two main buckets of ARC-AGI problems (cases where the grid size changes vs doesnâ€™t). The prior state of the art on a similarly difficult dataset was 34% accuracy , so this is a significant improvement. 3 (Edit: But see this comment and this comment for important clarifications.) On a held-out subset of the train set, where humans get 85% accuracy , my solution gets 72% accuracy. 4 (The train set is significantly easier than the test set as noted here .) Additional increases of runtime compute would further improve performance (and there are clear scaling laws), but this is left as an exercise to the reader. Subscribe In this post: I describe my method; I analyze what limits its performance and make predictions about what is needed to reach human performance; I comment on what it means for claims that FranÃ§ois Chollet makes about LLMs. Given that current LLMs can perform decently well on ARC-AGI, do claims like \"LLMs like Gemini or ChatGPT [don't work] because they're basically frozen at inference time. They're not actually learning anything.\" make sense? (This quote is from here .) Thanks to Fabien Roger and Buck Shlegeris for a bit of help with this project and with writing this post. What is ARC-AGI? ARC-AGI is a dataset built to evaluate the general reasoning abilities of AIs. It consists of visual problems like the below, where there are input-output examples which are grids of colored cells. The task is to guess the transformation from input to output and then fill out the missing grid. Here is an example from the tutorial: This one is easy, and itâ€™s easy to get GPT-4o to solve it. But the tasks from the public test set are much harder; theyâ€™re often non-trivial for (typical) humans. There is a reported MTurk human baseline for the train distribution of 85% , but no human baseline for the public test set which is known to be significantly more difficult. Here are representative problems from the test set 5 , and whether my GPT-4o-based solution gets them correct or not. Problem 1: Problem 2: Problem 3: My method The main idea behind my solution is very simple: get GPT-4o to generate around 8,000 python programs which attempt to implement the transformation, select a program which is right on all the examples (usually there are 3 examples), and then submit the output this function produces when applied to the additional test input(s). I show GPT-4o the problem as images and in various ascii representations. My approach is similar in spirit to the approach applied in AlphaCode in which a model generates millions of completions attempting to solve a programming problem and then aggregates over them to determine what to submit. Actually getting to 50% with this main idea took me about 6 days of work. This work includes constructing few-shot prompts, building better text representations of these grids, iterating against the train set, and implementing various other tweaks to improve performance. I started on this project a few days before Dwarkesh Patel recorded the recent podcast with Chollet. This was inspired by Dwarkesh talking to my coworker Buck about ARC-AGI, and then Buck being like â€œcome on, surely you can do better than current SOTA using LLMsâ€. Then, I tested GPT-4o a bit and it seemed to get what was going on. Iâ€™ve recently been doing some research that involved getting Claude 3 Opus to do reasoning right at the edge of its capabilities, and thought I might have an edge based on my experience from that project. At a high level, the method I use is: Provide the ARC-AGI problem to GPT-4o, with both an image representation and with various text representations for each grid in the problem. The text representations include showing which cells are occupied by different connected components of colors and showing diffs between the input and output (in cases where the grid shapes are the same). Instruct GPT-4o to reason about what the transformation is, reason how to implement the transformation as code, and then finally actually implement the transformation in code. Use a few-shot prompt with several carefully handwritten examples of step-by-step reasoning 6 to actually get GPT-4o to do this reasoning somewhat effectively. The resulting prompt is usually around 30k tokens long including images. We actually have a pair of few-shot prompts: one prompt for when the grid size changes (in any of the examples) and one for when it doesnâ€™t. We also ensemble over multiple pairs of few-shot prompts. This doesnâ€™t help much, but I already had the samples in the cache. Sample vast, vast numbers of completions (~5,000 per problem) from GPT-4o. Take the most promising 12 completions for each problem, and then try to fix each by showing GPT-4o what this program actually outputs",
    "link": "https://redwoodresearch.substack.com/p/getting-50-sota-on-arc-agi-with-gpt"
  },
  {
    "id": "experimenting-with-llms-to-research-reflect-and-pl-f039c7cc",
    "title": "Experimenting with LLMs to Research, Reflect, and Plan",
    "summary": "eugeneyan Start Here Writing Speaking Prototyping About Experimenting with LLMs to Research, Reflect, and Plan [ llm deeplearning learning ðŸ›  ðŸ”¥ ] Â· 14 min read Over the past few weekends, Iâ€™ve been playing with large language models (LLMs) and combining them with tools and interfaces to build a simple assistant . Along the way, I noticed some issues with retrieval and thought of a few ideas on how to solve them. It was a lot of fun, and I came away from this experience optimistic that LLMs have great potential to augment how we work, especially how we research, reflect, and plan . Hereâ€™s the code to speed run this. Itâ€™s messy and relies largely on LangChain abstractions. Could be useful as a reference, but not as learning material. Tools to summarize, query, and advise My first project was inspired by Simonâ€™s post on how ChatGPT is unable to read content from URLs. Thus, I tried to help it do just that with /summarize and /eli5 . The former can /summarize content from URLs into bullet points while the latter reads the content and explains like Iâ€™m five (eli5). They help me skim content before deciding if I want to read the details in full ( tweet thread ). Bullet point summary of \" ChatGPT Is a Blurry JPEG of the Web \" Explaining to a five-year old \" ChatGPT Is a Blurry JPEG of the Web \" Next, I explored building agents with access to tools like SQL and search. /sql takes natural language questions, writes and runs SQL queries, and returns the result. /sql-agent does the same but as a zero-shot agent. Though /sql-agent didnâ€™t work as reliably as I hoped (see Appendix ), watching it struggle and eventually get it right was endearing and motivating ( tweet thread ). Querying a small database via /sql Querying a small database via /sql-agent (it's fun to watch it think, observe, and act) I also built /search , an agent that can use tools to query search provider APIs (e.g., Google Search). This way, the LLM can find recent data that it hasnâ€™t been trained on and return an accurate and up-to-date response. (This was before ChatGPT plugins that now have this functionality out of the box. Even so, it was fun building it from scratch.) Using /search to find recent information for the LLM Most recently, I built a /board of advisors. Itâ€™s based on content from informal mentorsâ€”and prolific writersâ€”like Paul Graham, Marc Andreessen, Will Larson, Charity Majors, and Naval Ravikant. /board provides advice on topics including technology, leadership, and startups. Its response includes source URLs for further reading, which can be chained with /summarize and /eli5 ( tweet thread ). Seeking advice on technical leadership and personal success from the /board I also built /ask-ey which is similar to /board but based on my own writing. Because Iâ€™m more familiar with my work, itâ€™s easier to spot issues such as not using an expected source (i.e., recall issue) or using irrelevant sources (i.e., ranking issue). Synthesizing across recsys and mechanism posts on my site via /ask-ey Combining LLMs, databases, search APIs, and Discord To extract content from URLs, I used good olâ€™ requests and BeautifulSoup . For LLMs, I worked with OpenAIâ€™s gpt-3.5-turbo and gpt-4 , primarily the former due to its cost-effectiveness. LangChain made it easy to apply the LLM chains, agents, and tools. For search, I used Googleâ€™s custom search through the google-api-python-client wrapper. To embed documents and queries, I used OpenAIâ€™s text-embedding-ada-002 . The application server was hosted on Railway . To host, serve, and find nearest neighbours on embeddings, I used Pinecone . Lastly, I integrated everything with Discord via the interactions wrapper. Shortcomings in retrieval and how to solve them While experimenting with /board and /ask-ey , I noticed that it wasnâ€™t retrieving and using the expected sources some of the time. For example, when I asked the /board â€œHow do I decide between being a manager or an ICâ€, it fails to use (as a source) any of Charityâ€™s writing on the manager - engineer pendulum or management . However, tweaking the question to â€œHow do I decide between being a manager or an engineer â€ resolved this. Asking the first question doesn't lead to the expected manager-eng sources being used; the second does Similarly, when I /ask-ey â€œWhat bandits are used in recommendation systemsâ€, it didnâ€™t retrieve my main writing on bandits . But updating the question to â€œ How are bandits used in recommendation systemsâ€ fixed this issue. Asking the first question doesn't lead to the expected bandit sources being used; the second does But when I checked the retrieved sources, it was disappointing to see that only the top hit came from the expected URL , and even that was an irrelevant chunk of the content. (Text from each URL is split into chunks of 1,500 tokens.) I had expected embedding-based retrival to fetch more chunks from the bandit URL. This suggests thereâ€™s room to improve on how I processed the data before embedding and highlights the importance of data prep. Only the top hit has the right resource but it doesn't contain useful content This issue is partially due to poor recall. Here are a few hypotheses on why this happens: ANN indices might be tuned sub-optimally Off-the-shelf embeddings may transfer poorly across domains Documents may be inadequately chunked Embedding-based retrieval alone might be insufficient ANN indices might be tuned sub-optimally Most (if not all) embedding-based retrieval use approximate nearest neighbours (ANN). If we use exact nearest neighbours, we would get perfect recall of 1.0 but with higher latency (think seconds). In contrast, ANN offers good-enough recall (~0.95) with millisecond latency. Iâ€™ve previously compared several open-source ANNs and most achieved ~0.95 recall at throughput of hundreds to thousands of queries per second. Benchmarking ANNs on recall vs latency across index parameters; top-right is better. If the issue lies in a sub-optimally tuned ANN index, we can",
    "link": "https://eugeneyan.com/writing/llm-experiments/"
  },
  {
    "id": "the-bitter-lesson-4223dfcb",
    "title": "The Bitter Lesson",
    "summary": "The Bitter Lesson Rich Sutton March 13, 2019 The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin. The ultimate reason for this is Moore's law, or rather its generalization of continued exponentially falling cost per unit of computation. Most AI research has been conducted as if the computation available to the agent were constant (in which case leveraging human knowledge would be one of the only ways to improve performance) but, over a slightly longer time than a typical research project, massively more computation inevitably becomes available. Seeking an improvement that makes a difference in the shorter term, researchers seek to leverage their human knowledge of the domain, but the only thing that matters in the long run is the leveraging of computation. These two need not run counter to each other, but in practice they tend to. Time spent on one is time not spent on the other. There are psychological commitments to investment in one approach or the other. And the human-knowledge approach tends to complicate methods in ways that make them less suited to taking advantage of general methods leveraging computation. There were many examples of AI researchers' belated learning of this bitter lesson, and it is instructive to review some of the most prominent. In computer chess, the methods that defeated the world champion, Kasparov, in 1997, were based on massive, deep search. At the time, this was looked upon with dismay by the majority of computer-chess researchers who had pursued methods that leveraged human understanding of the special structure of chess. When a simpler, search-based approach with special hardware and software proved vastly more effective, these human-knowledge-based chess researchers were not good losers. They said that ``brute force\" search may have won this time, but it was not a general strategy, and anyway it was not how people played chess. These researchers wanted methods based on human input to win and were disappointed when they did not. A similar pattern of research progress was seen in computer Go, only delayed by a further 20 years. Enormous initial efforts went into avoiding search by taking advantage of human knowledge, or of the special features of the game, but all those efforts proved irrelevant, or worse, once search was applied effectively at scale. Also important was the use of learning by self play to learn a value function (as it was in many other games and even in chess, although learning did not play a big role in the 1997 program that first beat a world champion). Learning by self play, and learning in general, is like search in that it enables massive computation to be brought to bear. Search and learning are the two most important classes of techniques for utilizing massive amounts of computation in AI research. In computer Go, as in computer chess, researchers' initial effort was directed towards utilizing human understanding (so that less search was needed) and only much later was much greater success had by embracing search and learning. In speech recognition, there was an early competition, sponsored by DARPA, in the 1970s. Entrants included a host of special methods that took advantage of human knowledge---knowledge of words, of phonemes, of the human vocal tract, etc. On the other side were newer methods that were more statistical in nature and did much more computation, based on hidden Markov models (HMMs). Again, the statistical methods won out over the human-knowledge-based methods. This led to a major change in all of natural language processing, gradually over decades, where statistics and computation came to dominate the field. The recent rise of deep learning in speech recognition is the most recent step in this consistent direction. Deep learning methods rely even less on human knowledge, and use even more computation, together with learning on huge training sets, to produce dramatically better speech recognition systems. As in the games, researchers always tried to make systems that worked the way the researchers thought their own minds worked---they tried to put that knowledge in their systems---but it proved ultimately counterproductive, and a colossal waste of researcher's time, when, through Moore's law, massive computation became available and a means was found to put it to good use. In computer vision, there has been a similar pattern. Early methods conceived of vision as searching for edges, or generalized cylinders, or in terms of SIFT features. But today all this is discarded. Modern deep-learning neural networks use only the notions of convolution and certain kinds of invariances, and perform much better. This is a big lesson. As a field, we still have not thoroughly learned it, as we are continuing to make the same kind of mistakes. To see this, and to effectively resist it, we have to understand the appeal of these mistakes. We have to learn the bitter lesson that building in how we think we think does not work in the long run. The bitter lesson is based on the historical observations that 1) AI researchers have often tried to build knowledge into their agents, 2) this always helps in the short term, and is personally satisfying to the researcher, but 3) in the long run it plateaus and even inhibits further progress, and 4) breakthrough progress eventually arrives by an opposing approach based on scaling computation by search and learning. The eventual success is tinged with bitterness, and often incompletely digested, because it is success over a favored, human-centric approach. One thing that should be learned from the bitter lesson is the great power of general purpose methods, of methods that continue to scale with increased computation even as the available computation becomes very great. The two methods that seem to scale arbitrarily in this way are search and learning . The second general point to be learned from the bitter lesson is that the actual contents of minds are tremendously, irredeemably complex; we should stop",
    "link": "http://www.incompleteideas.net/IncIdeas/BitterLesson.html"
  },
  {
    "id": "transformer-taxonomy-the-last-lit-review-kipplys-b-18c5ede0",
    "title": "Transformer Taxonomy (the last lit review) | kipply's blog",
    "summary": "This document is my running literature review for people trying to catch up on AI. It covers 22 models, 11 architectural changes, 7 post-pre-training techniques and 3 training techniques (and 5 things that are none of the above). Everything is very loosely in order of importance and somewhat uniqueness. All papers will link to the actual PDF and not the ArXiv page and the selection is mostly curated based on things I know about. Systems/performance and alignment are excluded for this one because theyâ€™re my favourite and Iâ€™d want to do it more justice. Alignment research is really important, I hope to do it justice some day! Also probably not all the papers in the model list are worth reading. 1. Models If a property is unspecified itâ€™s either undisclosed or follows approximately the standard GPT recipe. GPT-3 [ paper ] â€” 175B params, 96 layers, 12288 embd dim, 96 heads â€” OpenAI May 2020 This was a seminal paper for large language models, following the GPT-2 paper (2018) and the scaling laws paper . It was trained on a 300B token dataset consisting mostly of filtered Common Crawl, along with some books, webtext and Wikipedia. BPE tokenizer (same from GPT-2). 2048 context length. Alternates dense and sparse attention layers. Warms up to 0.6 Ã— 10^âˆ’4 learning rate in the first 375M toks, cosine decayed to 10% after 260B toks. Batch size ramp from 32k toks to 3.2M toks over the first 12B tokens. 4x MLP projection ratio as done in the 2017 transformer paper . 50k vocab size. Many of these characteristics (e.g. embd dim = 128 * layers, 4x MLP projection ratio, and LR and batch size ramp) form a standard recipe that has been reused by later models. thereâ€™s a probably-typo in Table 2.1 that documents the hyperparameters, where GPT-3 13B is labelled as having an embedding dimension of 5140 which should probably be 5120 GPT-4 [ technical report ] â€” Released March 2023, finished pre-training August 2022 Man, feels awkward to write a pathetic summary of something this big, but here goes: GPT-4 is a model available through OpenAI of unknown architecture (other than that itâ€™s GPT-like, though they only technically specify transformer-like). The technical report contains mostly evals (which performed well of course), as well as the results of their continued scaling which are accurately extrapolated from smaller models. The report also documents safety mitigation and has a demo of their multi-modal capabilities of GPT-4 which seem trained in Ã  la Flamingo. It also has the best Acknowledgements section of all time. Gopher [ paper ] â€” 280B params, 260B non-embedding params, 80 layers, 16384 embd dim, 128 heads â€” DeepMind Dec 2021 DeepMindâ€™s first large language model release in 2021. It uses an RMSNorm instead of a LayerNorm, uses a relative positional encoding scheme from Transformer-XL instead of an absolute positional encoding, which is why there are so many embedding parameters. Tokenizes with SentencePiece, vocab size 32k. Trained on 300B tokens, with half being from MassiveText which was collected for Gopher, along with books, Common Crawl, Wikipedia, news and Github. Note that Gopher was actually trained end of 2020 and released a year later. AlphaCode [ paper ] â€” 41B, 8 encoder layers, 56 decoder layers, 6144 embd dim â€” DeepMind Feb 2022 A model trained on 715GB(967B tokens) of code to do competitive programming. The only model on this list with an encoder-decoder architecture, it treated contest programming as a translation task (problem statement â†’ solution) to gain bidirectionality. It uses 1536 tokens in the encoder and 768 tokens in the decoder. Uses multi-query attention, and generates thousands of samples at inference time and then selects a subset of solutions to submit. RETRO [ paper ] â€” 7B parameters â€” DeepMind Feb 2022 Retrieval is the general technique if giving a model a database to look up while doing inference. RETRO was the inaugural retrieval paper for transformers, using a 2T token database. It embeds the token-database in chunks using a pretrained BERT-style model and then performs chunked cross-attention to nearest neighbors in the database during training and inference GPT-3.5 [ docs ] â€” architecture unknown â€” OpenAI Mar 2022 OpenAI delineates three models as GPT-3.5, specifically anything in the davinci-002 or davinci-003 family. code-davinci-002 is the base model, text-davinci-002 is a version with FeedME non-RL instruction tuning, and text-davinci-003 is an InstructGPT with RLHF. There is an InstructGPT paper that trains an RLHF model and does not mention FeedME, and though text-davinci-002 is an InstructGPT model it does not use RLHF. The davinci model on the OpenAI API is noted to be the 175B model in the 2020 paper, but itâ€™s never confirmed whether davinci-002 is the same size. Chinchilla [ paper ] â€” 70B params, 80 layers, 8192 embd dim, 64 heads â€” DeepMind Mar 2022 With the paper titled \"Training Compute-Optimal Large Language Modelsâ€, new and improved scaling laws were introduced. Chinchilla is trained with 1.5T tokens (similar dataset as Gopher) and same amount of compute as Gopher, yet outperforms it. Results in scaling laws that have parameters and tokens linearly increase at a 20:1 token to parameter ratio. Learning rate adjusts with a cosine schedule. Megatron Turing NLG and Jurassic J-1 Jumbo are two other large models that arenâ€™t documented here as they are not Chinchilla optimal and arenâ€™t uniquely significant. Flamingo [ paper ] â€” 80B params â€” DeepMind Apr 2022 Flamingo is a multi-modal (text and image) model. It only generates text, and image inputs are run through a vision encoder (435M params), and cross-attention is used to attend to those outputs. It also uses a resampler (194M params) after the vision encoder to produce a fixed (small) number of visual tokens no matter the number of input features. They build on frozen Chinchilla models, the 80B params come from the cross-attention layers added to the 70B Chinchilla model. PaLI is a Google model that follows up on image/language multimodal. Gato [ paper ] â€” 1.18B params",
    "link": "https://kipp.ly/transformer-taxonomy/"
  },
  {
    "id": "llm-parameter-counting-kipplys-blog-6937c4f9",
    "title": "LLM Parameter Counting | kipply's blog",
    "summary": "Each weight or parameter is a float that was tuned during training and is usually two bytes as most training is done half-precision now( bfloat16 ). Not everything is trained/served bfloat16, but it's at least half-precision (at least since the GPT-3 Paper in 2020) which gets us the two bytes. The weights loosely consist of the following, per each block (where one block a decoder unit that consists of a self-attention layer and a feedforward layer, though I'll refer to blocks as layers): \\( W_q,W_k,W_v \\) matrices, which are each \\(d_\\text{model} \\cdot n_\\text{heads}\\cdot d_\\text{head} \\) and project the input into the query, key, and value used in self-attention. A \\( W_o \\) matrix, which is also \\(d_\\text{model}\\cdot n_\\text{heads}\\cdot d_\\text{head} \\) and used on the output of self-attention, before the MLP layer (the feed-foward neural network that's stacked on the self-attention layer). MLP weights, which are two matrices each of \\({d_\\text{model}}^2 \\cdot 4\\). You might also see this referred to by feedforward or linear layers. The four in the MLP weights calculation is based on architecture, but basically every transformer since the original from 2017 has gone with that ratio â€” where the MLP is 4 four times the size of the model embedding dimension. In a vast majority of transformer architectures, \\(n_\\text{heads}\\cdot d_\\text{head} = d_\\text{model}\\). You can see this in all the GPT models at Table 2.1 (the 13B model is off by 20, but might just be a... typo?), in the Gopher models in Table 1 (where what I called \\(d_\\text{head}\\), they called \"Key/Value Size\"). This is not necessarily the case, but can be assumed. So then we have a handy equation to calculate the number of parameters! With these, we can practice seeing how the factor of four in the MLP layers and the relationship of \\(n_\\text{heads}\\cdot d_\\text{head} = d_\\text{model}\\) holds true with the dimensions in the inaugural Anthropic paper in Table 1, where only \\(n_\\text{layers}\\), \\(d_\\text{model}\\) and \\(P\\) are supplied. This is not quite 52B. It's probably cheating to round up by half a billion parameters, but we can account for them! The equation above is most of the parameters, but we're missing token embeddings. Anthropic uses a 65536 vocab size, so we get \\(n_\\text{tokens} * d_\\text{model} = 536,870,912 \\). Adding \\(536,870,912 + 51,539,607,552 = 52,076,478,464\\). We actually have that half a billion params twice for the unembeddings, which leads us to about 52.5B tokens. We're also missing biases that are attached to all the weights, as well as layernorm. Biases should be approximately zero, and layernorm are \\(d_\\text{model}\\) (though they exist per block), but otherwise known as zero. Transformers also have positional encoding mechanisms, which for GPT-2 and the original transformer is \\(n_\\text{ctx}\\cdot d_\\text{model}\\) (aka, zero) but Gopher 280B there's 21.5B weights spent on the relative positional encoding method presented in the Transformer XL paper .",
    "link": "https://kipp.ly/transformer-param-count/"
  },
  {
    "id": "how-i-hire-programmers-aaron-swartzs-raw-thought-951f4645",
    "title": "How I Hire Programmers (Aaron Swartz's Raw Thought)",
    "summary": "Raw Thought by Aaron Swartz How I Hire Programmers EspaÃ±ol There are three questions you have when youâ€™re hiring a programmer (or anyone, for that matter): Are they smart? Can they get stuff done? Can you work with them? Someone whoâ€™s smart but doesnâ€™t get stuff done should be your friend, not your employee. You can talk your problems over with them while they procrastinate on their actual job. Someone who gets stuff done but isnâ€™t smart is inefficient: non-smart people get stuff done by doing it the hard way and working with them is slow and frustrating. Someone you canâ€™t work with, you canâ€™t work with. The traditional programmer hiring process consists of: a) reading a resume, b) asking some hard questions on the phone, and c) giving them a programming problem in person. I think this is a terrible system for hiring people. You learn very little from a resume and people get real nervous when you ask them tough questions in an interview. Programming isnâ€™t typically a job done under pressure, so seeing how people perform when nervous is pretty useless. And the interview questions usually asked seem chosen just to be cruel. I think Iâ€™m a pretty good programmer, but Iâ€™ve never passed one of these interviews and I doubt I ever could. So when I hire people, I just try to answer the three questions. To find out if they can get stuff done, I just ask what theyâ€™ve done. If someone can actually get stuff done they should have done so by now. Itâ€™s hard to be a good programmer without some previous experience and these days anyone can get some experience by starting or contributing to a free software project. So I just request a code sample and a demo and see whether it looks good. You learn an enormous amount really quickly, because youâ€™re not watching them answer a contrived interview question, youâ€™re seeing their actual production code. Is it concise? clear? elegant? usable? Is it something youâ€™d want in your product? To find out whether someoneâ€™s smart, I just have a casual conversation with them. I do everything I can to take off any pressure off: I meet at a cafe, I make it clear itâ€™s not an interview, I do my best to be casual and friendly. Under no circumstances do I ask them any standard â€œinterview questionsâ€ â€” I just chat with them like I would with someone I met at a party. (If you ask people at parties to name their greatest strengths and weaknesses or to estimate the number of piano tuners in Chicago, youâ€™ve got bigger problems.) I think itâ€™s pretty easy to tell whether someoneâ€™s smart in casual conversation. I constantly make judgments about whether people I meet are smart, just like I constantly make judgments about whether people I see are attractive. But if I had to write down what it is that makes someone seem smart, Iâ€™d emphasize three things. First, do they know stuff? Ask them what theyâ€™ve been thinking about and probe them about it. Do they seem to understand it in detail? Can they explain it clearly? (Clear explanations are a sign of genuine understanding.) Do they know stuff about the subject that you donâ€™t? Second, are they curious? Do they reciprocate by asking questions about you? Are they genuinely interested or just being polite? Do they ask follow-up questions about what youâ€™re saying? Do their questions make you think? Third, do they learn? At some point in the conversation, youâ€™ll probably be explaining something to them. Do they actually understand it or do they just nod and smile? There are people who know stuff about some small area but arenâ€™t curious about others. And there are people who are curious but donâ€™t learn, they ask lots of questions but donâ€™t really listen. You want someone who does all three. Finally, I figure out whether I can work with someone just by hanging out with them for a bit. Many brilliant people can seem delightful in a one-hour conversation, but their eccentricities become grating after a couple hours. So after youâ€™re done chatting, invite them along for a meal with the rest of the team or a game at the office. Again, keep things as casual as possible. The point is just to see whether they get on your nerves. If all that looks good and Iâ€™m ready to hire someone, thereâ€™s a final sanity check to make sure I havenâ€™t been fooled somehow: I ask them to do part of the job. Usually this means picking some fairly separable piece we need and asking them to write it. (If you really insist on seeing someone working under pressure, give them a deadline.) If necessary, you can offer to pay them for the work, but I find most programmers donâ€™t mind being given a small task like this as long as they can open source the work when theyâ€™re done. This test doesnâ€™t work on its own, but if someoneâ€™s passed the first three parts, it should be enough to prove they didnâ€™t trick you, they can actually do the work. (Iâ€™ve known some people who say â€œOK, well why donâ€™t we try hiring you for a month and see how it goes.â€ This doesnâ€™t seem to work. If you canâ€™t make up your mind after a small project you also canâ€™t make it up after a month and you end up hiring people who arenâ€™t good enough. Better to just say no and err on the side of getting better people.) Iâ€™m fairly happy with this method. When Iâ€™ve skipped parts, Iâ€™ve ended up with bad hires who eventually had to be let go. But when Iâ€™ve followed it, Iâ€™ve ended up with people I like so much so that I actually feel bad I donâ€™t get to work with them anymore. Iâ€™m amazed that so many companies use such silly hiring methods instead. You should follow me on",
    "link": "http://www.aaronsw.com/weblog/hiring.en"
  },
  {
    "id": "the-smalltalk-question-aaron-swartzs-raw-thought-9305228e",
    "title": "The Smalltalk Question (Aaron Swartz's Raw Thought)",
    "summary": "Raw Thought by Aaron Swartz The Smalltalk Question One of the minor puzzles of American life is what question to ask people at parties and suchly to get to know them. â€œHow ya doinâ€™?â€ is of course mere formality, only the most troubled would answer honestly for anything but the positive. â€œWhat do you do?â€ is somewhat offensive. First, it really means â€œwhat occupation do you hold?â€ and thus implies you do little outside your occupation. Second, it implies that oneâ€™s occupation is the most salient fact about them. Third, it rarely leads to further useful inquiry. For only a handful of occupations, you will be able to say something somewhat relevant, but even this will no doubt be slightly annoying or offensive. (â€œOh yeah, I always thought about studying history.â€) â€œWhere are you from?â€ is even less fruitful. â€œWhatâ€™s your major?â€ (in the case of college students) turns sour when, as is tragically all too often the case, students feel no real passion for their major. â€œWhat book have you read recently?â€ will cause the majority of Americans who donâ€™t read to flail, while at best only getting an off-the-cuff garbled summary of a random book. â€œWhatâ€™s something cool youâ€™ve learned recently?â€ puts the person on the spot and inevitably leads to hemming and hawing and then something not all that cool. I propose instead that one ask â€œWhat have you been thinking about lately?â€ First, the question is extremely open-ended. The answer could be a book, a movie, a relationship, a class, a job, a hobby, etc. Even better, it will be whichever of these is most interesting at the moment. Second, it sends the message that thinking, and thinking about thinking, is a fundamental human activity, and thus encourages it. Third, itâ€™s easiest to answer, since by its nature its asking about whatâ€™s already on the personâ€™s mind. Fourth, itâ€™s likely to lead to productive dialog, as you can discuss the topic together and hopefully make progress. Fifth, the answer is quite likely to be novel. Unlike books and occupations, peopleâ€™s thoughts seem to be endlessly varied. Sixth, it helps capture a personâ€™s essence. A job can be forced by circumstance and parentage, but our thoughts are all our own. I can think of little better way to quickly gauge what a person is really like. â€œWhat have you been working on lately?â€ can be seen, in this context, to be clearly inferior, although similar. So, what have you been thinking about lately? You should follow me on twitter here . August 16, 2006 Comments A friend of mine taught me that when meeting new people at school its better to ask â€˜whyâ€™ questions than â€˜whatâ€™ questions. â€œWhat is your Major?â€ â€œMedical Scienceâ€ â€œWhy did you choose that programâ€ {lengthy conversation started} posted by Mike Morton on August 16, 2006 # I like the question, but until it comes into common use, it will still be a bit awkward. I think because people donâ€™t often hear such open-ended questions from strangers, theyâ€™re likely to infer a more specific question and answer that with the same problems you mentioned. Iâ€™ve been thinking lately about how to encourage diverse self-selected communities without geographic constraints forcing us to spend time around people who annoy us (i.e. online). posted by Scott Reynen on August 16, 2006 # I often ask newly-met people whatâ€™s important to them. It definitely throws them off at first, but just generally taking an interest in something -they- find interesting gets it going just fine. If what they thing is important is foreign to me, so much the betterâ€” I can learn something and treat them to a blank slate. Simon Willison probably set a record in meeting people at SXSW a while back by asking everyone he met what they were excited about. By the time the conference was over, it seemed everybody knew him. posted by Jeremy Dunck on August 16, 2006 # Iâ€™ve been wondering what any of this has to do with an obscure programming language. posted by Michael Conlen on August 16, 2006 # From the title, I assumed this was about Smalltalk the language, which is a different type of conversation starter ;-) I like Aaronâ€™s question, and am often trying to figure out what type of open-ended question to ask people in these situations. Especially nerds, who are often the most interesting once they get rolling, but often slow to start talking to new people (myself included). posted by mgc on August 16, 2006 # â€œWhat do you do?â€ is somewhat offensive. First, it really means â€œwhat occupation do you hold?â€ and thus implies you do little outside your occupation. Second, it implies that oneâ€™s occupation is the most salient fact about them. Third, it rarely leads to further useful inquiry. Iâ€™m not sure I agree with this - I was recently at a party where I didnâ€™t know many of the guests, and this was used as the standard icebreaker by myself and others, and each time it led to interesting conversations. I think this is down to it being such a standard gambit that people always add something to their reply, so instead of â€˜Iâ€™m a journalistâ€™ or â€˜Iâ€™m an artistâ€™, you get â€˜Iâ€™m a news journalist, Iâ€™m just back from the Lebanonâ€™ or â€˜Iâ€™m an artist, Iâ€™m hanging my new show tomorrowâ€™. That said, I do like the suggested question and will try it out, even if it would probably be met with a raised eyebrow. Iâ€™ve been thinking about the (apparent) lack of contemporary equivalents to movements like Arts & Crafts, in which people worked collaboratively across different artistic disciplines. posted by Jack on August 16, 2006 # So Iâ€™m not the only one who thought of the Smalltalk language first. posted by Mike Cohen on August 16, 2006 # It took me half your post to realize that you were not going to get to Smalltalk the programming language;). On topic, though, â€œWhat have you",
    "link": "http://www.aaronsw.com/weblog/smalltalkq"
  },
  {
    "id": "solving-olympiad-geometry-without-human-demonstrat-7fd3462c",
    "title": "Solving olympiad geometry without human demonstrations | Nature",
    "summary": "Solving olympiad geometry without human demonstrations Download PDF Download PDF Subjects Computational science Computer science An Author Correction to this article was published on 23 February 2024 This article has been updated Abstract Proving mathematical theorems at the olympiad level represents a notable milestone in human-level automated reasoning 1 , 2 , 3 , 4 , owing to their reputed difficulty among the worldâ€™s best talents in pre-university mathematics. Current machine-learning approaches, however, are not applicable to most mathematical domains owing to the high cost of translating human proofs into machine-verifiable format. The problem is even worse for geometry because of its unique translation challenges 1 , 5 , resulting in severe scarcity of training data. We propose AlphaGeometry, a theorem prover for Euclidean plane geometry that sidesteps the need for human demonstrations by synthesizing millions of theorems and proofs across different levels of complexity. AlphaGeometry is a neuro-symbolic system that uses a neural language model, trained from scratch on our large-scale synthetic data, to guide a symbolic deduction engine through infinite branching points in challenging problems. On a test set of 30 latest olympiad-level problems, AlphaGeometry solves 25, outperforming the previous best method that only solves ten problems and approaching the performance of an average International Mathematical Olympiad (IMO) gold medallist. Notably, AlphaGeometry produces human-readable proofs, solves all geometry problems in the IMO 2000 and 2015 under human expert evaluation and discovers a generalized version of a translated IMO theorem in 2004. Similar content being viewed by others Olympiad-level formal mathematical reasoning with reinforcement learning Article 12 November 2025 Local prediction-learning in high-dimensional spaces enables neural networks to plan Article Open access 15 March 2024 Learning behavior aware features across spaces for improved 3D human motion prediction Article Open access 04 August 2025 Main Proving theorems showcases the mastery of logical reasoning and the ability to search through an infinitely large space of actions towards a target, signifying a remarkable problem-solving skill. Since the 1950s (refs. 6 , 7 ), the pursuit of better theorem-proving capabilities has been a constant focus of artificial intelligence (AI) research 8 . Mathematical olympiads are the most reputed theorem-proving competitions in the world, with a similarly long history dating back to 1959, playing an instrumental role in identifying exceptional talents in problem solving. Matching top human performances at the olympiad level has become a notable milestone of AI research 2 , 3 , 4 . Theorem proving is difficult for learning-based methods because training data of human proofs translated into machine-verifiable languages are scarce in most mathematical domains. Geometry stands out among other olympiad domains because it has very few proof examples in general-purpose mathematical languages such as Lean 9 owing to translation difficulties unique to geometry 1 , 5 . Geometry-specific languages, on the other hand, are narrowly defined and thus unable to express many human proofs that use tools beyond the scope of geometry, such as complex numbers (Extended Data Figs. 3 and 4 ). Overall, this creates a data bottleneck, causing geometry to lag behind in recent progress that uses human demonstrations 2 , 3 , 4 . Current approaches to geometry, therefore, still primarily rely on symbolic methods and human-designed, hard-coded search heuristics 10 , 11 , 12 , 13 , 14 . We present an alternative method for theorem proving using synthetic data, thus sidestepping the need for translating human-provided proof examples. We focus on Euclidean plane geometry and exclude topics such as geometric inequalities and combinatorial geometry. By using existing symbolic engines on a diverse set of random theorem premises, we extracted 100 million synthetic theorems and their proofs, many with more than 200 proof steps, four times longer than the average proof length of olympiad theorems. We further define and use the concept of dependency difference in synthetic proof generation, allowing our method to produce nearly 10 million synthetic proof steps that construct auxiliary points, reaching beyond the scope of pure symbolic deduction. Auxiliary construction is geometryâ€™s instance of exogenous term generation, representing the infinite branching factor of theorem proving, and widely recognized in other mathematical domains as the key challenge to proving many hard theorems 1 , 2 . Our work therefore demonstrates a successful case of generating synthetic data and learning to solve this key challenge. With this solution, we present a general guiding framework and discuss its applicability to other domains in Methods section â€˜AlphaGeometry framework and applicability to other domainsâ€™. We pretrain a language model on all generated synthetic data and fine-tune it to focus on auxiliary construction during proof search, delegating all deduction proof steps to specialized symbolic engines. This follows standard settings in the literature, in which language models such as GPT-f (ref. 15 ), after being trained on human proof examples, can generate exogenous proof terms as inputs to fast and accurate symbolic engines such as nlinarith or ring 2 , 3 , 16 , using the best of both worlds. Our geometry theorem prover AlphaGeometry, illustrated in Fig. 1 , produces human-readable proofs, substantially outperforms the previous state-of-the-art geometry-theorem-proving computer program and approaches the performance of an average IMO gold medallist on a test set of 30 classical geometry problems translated from the IMO as shown in Fig. 2 . Fig. 1: Overview of our neuro-symbolic AlphaGeometry and how it solves both a simple problem and the IMO 2015 Problem 3. The top row shows how AlphaGeometry solves a simple problem. a , The simple example and its diagram. b , AlphaGeometry initiates the proof search by running the symbolic deduction engine. The engine exhaustively deduces new statements from the theorem premises until the theorem is proven or new statements are exhausted. c , Because the symbolic engine fails to find a proof, the language model constructs one auxiliary point, growing the proof state before the symbolic engine retries. The loop continues until a solution is found. d , For the simple example, the loop terminates after the first auxiliary construction â€œD as the midpoint of BCâ€. The",
    "link": "https://www.nature.com/articles/s41586-023-06747-5"
  },
  {
    "id": "emergent-abilities-of-large-language-models-3f747a19",
    "title": "Emergent Abilities of Large Language Models",
    "summary": "Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.",
    "link": "https://arxiv.org/abs/2206.07682"
  },
  {
    "id": "an-intuitive-explanation-of-sparse-autoencoders-fo-b1005eef",
    "title": "An Intuitive Explanation of Sparse Autoencoders for LLM Interpretability | Adam Karvonen",
    "summary": "Sparse Autoencoders (SAEs) have recently become popular for interpretability of machine learning models (although sparse dictionary learning has been around since 1997 ). Machine learning models and LLMs are becoming more powerful and useful, but they are still black boxes, and we donâ€™t understand how they do the things that they are capable of. It seems like it would be useful if we could understand how they work. Using SAEs, we can begin to break down a modelâ€™s computation into understandable components . There are several existing explanations of SAEs, and I wanted to create a brief writeup from a different angle with an intuitive explanation of how they work. Challenges with interpretability The most natural component of a neural network is individual neurons. Unfortunately, individual neurons do not conveniently correspond to single concepts. An example neuron in a language model corresponded to academic citations, English dialogue, HTTP requests, and Korean text. This is a concept called superposition , where concepts in a neural network are represented by combinations of neurons. This may occur because many variables existing in the world are naturally sparse. For example, the birthplace of an individual celebrity may come up in less than one in a billion training tokens, yet modern LLMs will learn this fact and an extraordinary amount of other facts about the world. Superposition may emerge because there are more individual facts and concepts in the training data than neurons in the model. Sparse autoencoders have recently gained popularity as a technique to break neural networks down into understandable components. SAEs were inspired by the sparse coding hypothesis in neuroscience. Interestingly, SAEs are one of the most promising tools to interpret artificial neural networks. SAEs are similar to a standard autoencoder. A regular autoencoder is a neural network designed to compress and then reconstruct its input data. For example, it may receive a 100 dimensional vector (a list of 100 numbers) as input, feed this input through an encoder layer to compress the input to a 50 dimensional vector, and then feed the compressed encoded representation through the decoder to produce a 100 dimensional output vector. The reconstruction is typically imperfect because the compression makes the task challenging. Diagram of a standard autoencoder with a 1x4 input vector, 1x2 intermediate state vector, and 1x4 output vector. The cell colors indicate activation value. The output is an imperfect reconstruction of the input. Sparse Autoencoder Explanation How Sparse Autoencoders Work A sparse autoencoder transforms the input vector into an intermediate vector, which can be of higher, equal, or lower dimension compared to the input. When applied to LLMs, the intermediate vectorâ€™s dimension is typically larger than the inputâ€™s. In that case, without additional constraints the task is trivial, and the SAE could use the identity matrix to perfectly reconstruct the input without telling us anything interesting. As an additional constraint, we add a sparsity penalty to the training loss, which incentivizes the SAE to create a sparse intermediate vector. For example, we could expand the 100 dimensional input into a 200 dimensional encoded representation vector, and we could train the SAE to only have ~20 nonzero elements in the encoded representation. Diagram of a sparse autoencoder. Note that the intermediate activations are sparse, with only 2 nonzero values. We apply SAEs to the intermediate activations within neural networks, which can be composed of many layers. During a forward pass, there are intermediate activations within and between each layer. For example, GPT-3 has 96 layers. During the forward pass, there is a 12,288 dimensional vector (a list of 12,288 numbers) for each token in the input that is passed from layer to layer. This vector accumulates all of the information that the model uses to predict the next token as it is processed by each layer, but it is opaque and itâ€™s difficult to understand what information is contained within. We can use SAEs to understand this intermediate activation. An SAE is basically a matrix -> ReLU activation -> matrix 1 2 . As an example, if our GPT-3 SAE has an expansion factor of 4, the input activation is 12,288 dimensional and the SAEâ€™s encoded representation is 49,512 dimensional (12,288 x 4). The first matrix is the encoder matrix of shape (12,288, 49,512) and the second matrix is the decoder matrix of shape (49,512, 12,288). By multiplying the GPTâ€™s activation with the encoder and applying the ReLU, we produce a 49,512 dimensional SAE encoded representation that is sparse, as the SAEâ€™s loss function incentivizes sparsity. Typically, we aim to have less than 100 numbers in the SAEâ€™s representation be nonzero. By multiplying the SAEâ€™s representation with the decoder, we produce a 12,288 dimensional reconstructed model activation. This reconstruction doesnâ€™t perfectly match the original GPT activation because our sparsity constraint makes the task difficult. We train individual SAEs on only one location in the model. For example, we could train a single SAE on intermediate activations between layers 26 and 27. To analyze the information contained in the outputs of all 96 layers in GPT-3, we would train 96 separate SAEs - one for each layerâ€™s output. If we also wanted to analyze various intermediate activations within each layer, this would require hundreds of SAEs. Our training data for these SAEs comes from feeding a diverse range of text through the GPT model and collecting the intermediate activations at each chosen location. Iâ€™ve included a reference SAE Pytorch implementation. The variables have shape annotations following Noam Shazeerâ€™s tip . Note that various SAE implementations will often have various bias terms, normalization schemes, or initialization schemes to squeeze out additional performance. One of the most common additions is some sort of constraint on decoder vector norms. For more details, refer to various implementations such as OpenAIâ€™s , SAELens , or dictionary_learning . import torch import torch.nn as nn # D = d_model, F = dictionary_size # e.g. if d_model = 12288 and dictionary_size = 49152 # then model_activations_D.shape = (12288,) and encoder_DF.weight.shape = (12288, 49152) class",
    "link": "https://adamkarvonen.github.io/machine_learning/2024/06/11/sae-intuitions.html"
  },
  {
    "id": "the-techno-optimist-manifesto-andreessen-horowitz-416603e0",
    "title": "The Techno-Optimist Manifesto | Andreessen Horowitz",
    "summary": "The Techno-Optimist Manifesto Marc Andreessen share Copy Link Email X LinkedIn Facebook Hacker News WhatsApp Flipboard Reddit Table of Contents Table of Contents Posted October 16, 2023 You live in a deranged age â€” more deranged than usual, because despite great scientific and technological advances, man has not the faintest idea of who he is or what he is doing. Walker Percy Our species is 300,000 years old. For the first 290,000 years, we were foragers, subsisting in a way thatâ€™s still observable among the Bushmen of the Kalahari and the Sentinelese of the Andaman Islands. Even after Homo Sapiens embraced agriculture, progress was painfully slow. A person born in Sumer in 4,000BC would find the resources, work, and technology available in England at the time of the Norman Conquest or in the Aztec Empire at the time of Columbus quite familiar. Then, beginning in the 18th Century, many peopleâ€™s standard of living skyrocketed. What brought about this dramatic improvement, and why? Marian Tupy Thereâ€™s a way to do it better. Find it. Thomas Edison Lies We are being lied to. We are told that technology takes our jobs, reduces our wages, increases inequality, threatens our health, ruins the environment, degrades our society, corrupts our children, impairs our humanity, threatens our future, and is ever on the verge of ruining everything. We are told to be angry, bitter, and resentful about technology. We are told to be pessimistic. The myth of Prometheus â€“ in various updated forms like Frankenstein, Oppenheimer, and Terminator â€“ haunts our nightmares. We are told to denounce our birthright â€“ our intelligence, our control over nature, our ability to build a better world. We are told to be miserable about the future. Truth Our civilization was built on technology. Our civilization is built on technology. Technology is the glory of human ambition and achievement, the spearhead of progress, and the realization of our potential. For hundreds of years, we properly glorified this â€“ until recently. I am here to bring the good news. We can advance to a far superior way of living, and of being. We have the tools, the systems, the ideas. We have the will. It is time, once again, to raise the technology flag. It is time to be Techno-Optimists. Technology Techno-Optimists believe that societies, like sharks, grow or die. We believe growth is progress â€“ leading to vitality, expansion of life, increasing knowledge, higher well being. We agree with Paul Collier when he says, â€œEconomic growth is not a cure-all, but lack of growth is a kill-all.â€ We believe everything good is downstream of growth. We believe not growing is stagnation, which leads to zero-sum thinking, internal fighting, degradation, collapse, and ultimately death. There are only three sources of growth: population growth, natural resource utilization, and technology. Developed societies are depopulating all over the world, across cultures â€“ the total human population may already be shrinking. Natural resource utilization has sharp limits, both real and political. And so the only perpetual source of growth is technology. In fact, technology â€“ new knowledge, new tools, what the Greeks called techne â€“ has always been the main source of growth, and perhaps the only cause of growth, as technology made both population growth and natural resource utilization possible. We believe technology is a lever on the world â€“ the way to make more with less. Economists measure technological progress as productivity growth : How much more we can produce each year with fewer inputs, fewer raw materials. Productivity growth, powered by technology, is the main driver of economic growth, wage growth, and the creation of new industries and new jobs, as people and capital are continuously freed to do more important, valuable things than in the past. Productivity growth causes prices to fall, supply to rise, and demand to expand, improving the material well being of the entire population. We believe this is the story of the material development of our civilization; this is why we are not still living in mud huts, eking out a meager survival and waiting for nature to kill us. We believe this is why our descendents will live in the stars. We believe that there is no material problem â€“ whether created by nature or by technology â€“ that cannot be solved with more technology. We had a problem of starvation, so we invented the Green Revolution. We had a problem of darkness, so we invented electric lighting. We had a problem of cold, so we invented indoor heating. We had a problem of heat, so we invented air conditioning. We had a problem of isolation, so we invented the Internet. We had a problem of pandemics, so we invented vaccines. We have a problem of poverty, so we invent technology to create abundance. Give us a real world problem, and we can invent technology that will solve it. Markets We believe free markets are the most effective way to organize a technological economy. Willing buyer meets willing seller, a price is struck, both sides benefit from the exchange or it doesnâ€™t happen. Profits are the incentive for producing supply that fulfills demand. Prices encode information about supply and demand. Markets cause entrepreneurs to seek out high prices as a signal of opportunity to create new wealth by driving those prices down . We believe the market economy is a discovery machine, a form of intelligence â€“ an exploratory, evolutionary, adaptive system. We believe Hayekâ€™s Knowledge Problem overwhelms any centralized economic system. All actual information is on the edges, in the hands of the people closest to the buyer. The center, abstracted away from both the buyer and the seller, knows nothing. Centralized planning is doomed to fail, the system of production and consumption is too complex. Decentralization harnesses complexity for the benefit of everyone; centralization will starve you to death. We believe in market discipline. The market naturally disciplines â€“ the seller either learns and changes when the buyer fails to show, or exits the market.",
    "link": "https://a16z.com/the-techno-optimist-manifesto/"
  },
  {
    "id": "the-best-gpus-for-deep-learning-in-2023-an-in-dept-4a8d7b43",
    "title": "The Best GPUs for Deep Learning in 2023 â€” An In-depth Analysis",
    "summary": "Deep learning is a field with intense computational requirements, and your choice of GPU will fundamentally determine your deep learning experience. But what features are important if you want to buy a new GPU? GPU RAM, cores, tensor cores, caches? How to make a cost-efficient choice? This blog post will delve into these questions, tackle common misconceptions, give you an intuitive understanding of how to think about GPUs, and will lend you advice, which will help you to make a choice that is right for you. This blog post is designed to give you different levels of understanding of GPUs and the new Ampere series GPUs from NVIDIA. You have the choice: (1) If you are not interested in the details of how GPUs work, what makes a GPU fast compared to a CPU, and what is unique about the new NVIDIA RTX 40 Ampere series, you can skip right to the performance and performance per dollar charts and the recommendation section. The cost/performance numbers form the core of the blog post and the content surrounding it explains the details of what makes up GPU performance. (2) If you worry about specific questions, I have answered and addressed the most common questions and misconceptions in the later part of the blog post. (3) If you want to get an in-depth understanding of how GPUs, caches, and Tensor Cores work, the best is to read the blog post from start to finish. You might want to skip a section or two based on your understanding of the presented topics. Contents hide Overview How do GPUs work? The Most Important GPU Specs for Deep Learning Processing Speed Tensor Cores Matrix multiplication without Tensor Cores Matrix multiplication with Tensor Cores Matrix multiplication with Tensor Cores and Asynchronous copies (RTX 30/RTX 40) and TMA (H100) Memory Bandwidth L2 Cache / Shared Memory / L1 Cache / Registers Estimating Ada / Hopper Deep Learning Performance Practical Ada / Hopper Speed Estimates Possible Biases in Estimates Advantages and Problems for RTX40 and RTX 30 Series Sparse Network Training Low-precision Computation Fan Designs and GPUs Temperature Issues 3-slot Design and Power Issues Power Limiting: An Elegant Solution to Solve the Power Problem? RTX 4090s and Melting Power Connectors: How to Prevent Problems 8-bit Float Support in H100 and RTX 40 series GPUs Raw Performance Ranking of GPUs GPU Deep Learning Performance per Dollar GPU Recommendations Is it better to wait for future GPUs for an upgrade? The future of GPUs. Question & Answers & Misconceptions Do I need PCIe 4.0 or PCIe 5.0? Do I need 8x/16x PCIe lanes? How do I fit 4x RTX 4090 or 3090 if they take up 3 PCIe slots each? How do I cool 4x RTX 3090 or 4x RTX 3080? Can I use multiple GPUs of different GPU types? What is NVLink, and is it useful? I do not have enough money, even for the cheapest GPUs you recommend. What can I do? What is the carbon footprint of GPUs? How can I use GPUs without polluting the environment? What do I need to parallelize across two machines? Is the sparse matrix multiplication features suitable for sparse matrices in general? Do I need an Intel CPU to power a multi-GPU setup? Does computer case design matter for cooling? Will AMD GPUs + ROCm ever catch up with NVIDIA GPUs + CUDA? When is it better to use the cloud vs a dedicated GPU desktop/server? Version History Acknowledgments Related Related Posts Overview This blog post is structured in the following way. First, I will explain what makes a GPU fast. I will discuss CPUs vs GPUs, Tensor Cores, memory bandwidth, and the memory hierarchy of GPUs and how these relate to deep learning performance. These explanations might help you get a more intuitive sense of what to look for in a GPU. I discuss the unique features of the new NVIDIA RTX 40 Ampere GPU series that are worth considering if you buy a GPU. From there, I make GPU recommendations for different scenarios. After that follows a Q&A section of common questions posed to me in Twitter threads; in that section, I will also address common misconceptions and some miscellaneous issues, such as cloud vs desktop, cooling, AMD vs NVIDIA, and others. How do GPUs work? If you use GPUs frequently, it is useful to understand how they work. This knowledge will help you to undstand cases where are GPUs fast or slow. In turn, you might be able to understand better why you need a GPU in the first place and how other future hardware options might be able to compete. You can skip this section if you just want the useful performance numbers and arguments to help you decide which GPU to buy. The best high-level explanation for the question of how GPUs work is my following Quora answer: Read Tim Dettmers â€˜ answer to Why are GPUs well-suited to deep learning? on Quora This is a high-level explanation that explains quite well why GPUs are better than CPUs for deep learning. If we look at the details, we can understand what makes one GPU better than another. The Most Important GPU Specs for Deep Learning Processing Speed This section can help you build a more intuitive understanding of how to think about deep learning performance. This understanding will help you to evaluate future GPUs by yourself. This section is sorted by the importance of each component. Tensor Cores are most important, followed by memory bandwidth of a GPU, the cache hierachy, and only then FLOPS of a GPU. Tensor Cores Tensor Cores are tiny cores that perform very efficient matrix multiplication. Since the most expensive part of any deep neural network is matrix multiplication Tensor Cores are very useful. In fast, they are so powerful, that I do not recommend any GPUs that do not have Tensor Cores. It is helpful to understand how they work to appreciate the importance of these computational units specialized for matrix",
    "link": "https://timdettmers.com/2023/01/30/which-gpu-for-deep-learning/"
  },
  {
    "id": "how-web-bloat-impacts-users-with-slow-devices-55ef110d",
    "title": "How web bloat impacts users with slow devices",
    "summary": "In 2017, we looked at how web bloat affects users with slow connections . Even in the U.S., many users didn't have broadband speeds , making much of the web difficult to use. It's still the case that many users don't have broadband speeds, both inside and outside of the U.S. and that much of the modern web isn't usable for people with slow internet, but the exponential increase in bandwidth (Nielsen suggests this is 50% per year for high-end connections ) has outpaced web bloat for typical sites, making this less of a problem than it was in 2017, although it's still a serious problem for people with poor connections. CPU performance for web apps hasn't scaled nearly as quickly as bandwidth so, while more of the web is becoming accessible to people with low-end connections, more of the web is becoming inaccessible to people with low-end devices even if they have high-end connections. For example, if I try browsing a \"modern\" Discourse-powered forum on a Tecno Spark 8C , it sometimes crashes the browser. Between crashes, on measuring the performance, the responsiveness is significantly worse than browsing a BBS with an 8 MHz 286 and a 1200 baud modem. On my 1Gbps home internet connection, the 2.6 MB compressed payload size \"necessary\" to load message titles is relatively light. The over-the-wire payload size has \"only\" increased by 1000x , which is dwarfed by the increase in internet speeds. But the opposite is true when it comes to CPU speeds â€” for web browsing and forum loading performance, the 8-core (2 1.6 GHz Cortex-A75 / 6 1.6 GHz Cortex-A55) CPU can't handle Discourse. The CPU is something like 100000x faster than our 286 . Perhaps a 1000000x faster device would be sufficient. For anyone not familiar with the Tecno Spark 8C , today, a new Tecno Spark 8C , a quick search indicates that one can be hand for USD 50-60 in Nigeria and perhaps USD 100-110 in India. As a fraction of median household income, that's substantially more than a current generation iPhone in the U.S. today. By worldwide standards, the Tecno Spark 8C isn't even close to being a low-end device, so we'll also look at performance on an Itel P32 , which is a lower end device (though still far from the lowest-end device people are using today). Additionally, we'll look at performance with an M3 Max Macbook (14-core) , an M1 Pro Macbook (8-core) , and the M3 Max set to 10x throttling in Chrome dev tools. In order to give these devices every advantage, we'll be on fairly high-speed internet (1Gbps, with a WiFi router that's benchmarked as having lower latency under load than most of its peers). We'll look at some blogging platforms and micro-blogging platforms (this blog, Substack, Medium, Ghost, Hugo, Tumblr, Mastodon, Twitter, Threads, Bluesky, Patreon), forum platforms (Discourse, Reddit, Quora, vBulletin, XenForo, phpBB, and myBB), and platforms commonly used by small businesses (Wix, Squarespace, Shopify, and WordPress again). In the table below, every row represents a website and every non-label column is a metric. After the website name column, we have the compressed size transferred over the wire ( wire ) and the raw, uncompressed, size ( raw ). Then we have, for each device, Largest Contentful Paint* ( LCP* ) and CPU usage on the main thread ( CPU ). Google's docs explain LCP as Largest Contentful Paint (LCP) measures when a user perceives that the largest content of a page is visible. The metric value for LCP represents the time duration between the user initiating the page load and the page rendering its primary content LCP is a common optimization target because it's presented as one of the primary metrics in Google PageSpeed Insights, a \"Core Web Vital\" metric. There's an asterisk next to LCP as used in this document because, LCP as measured by Chrome is about painting a large fraction of the screen, as opposed to the definition above, which is about content. As sites have optimized for LCP , it's not uncommon to have a large paint (update) that's completely useless to the user, with the actual content of the page appearing well after the LCP . In cases where that happens, I've used the timestamp when useful content appears, not the LCP as defined by when a large but useless update occurs. The full details of the tests and why these metrics were chosen are discussed in an appendix. Although CPU time isn't a \"Core Web Vital\", it's presented here because it's a simple metric that's highly correlated with my and other users' perception of usability on slow devices. See appendix for more detailed discussion on this. One reason CPU time works as a metric is that, if a page has great numbers for all other metrics but uses a ton of CPU time, the page is not going to be usable on a slow device. If it takes 100% CPU for 30 seconds, the page will be completely unusable for 30 seconds, and if it takes 50% CPU for 60 seconds, the page will be barely usable for 60 seconds, etc. Another reason it works is that, relative to commonly used metrics, it's hard to cheat on CPU time and make optimizations that significantly move the number without impacting user experience. The color scheme in the table below is that, for sizes, more green = smaller / fast and more red = larger / slower. Extreme values are in black. Site Size M3 Max M1 Pro M3/10 Tecno S8C Itel P32 wire raw LCP* CPU LCP* CPU LCP* CPU LCP* CPU LCP* CPU danluu.com 6kB 18kB 50ms 20ms 50ms 30ms 0.2s 0.3s 0.4s 0.3s 0.5s 0.5s HN 11kB 50kB 0.1s 30ms 0.1s 30ms 0.3s 0.3s 0.5s 0.5s 0.7s 0.6s MyBB 0.1MB 0.3MB 0.3s 0.1s 0.3s 0.1s 0.6s 0.6s 0.8s 0.8s 2.1s 1.9s phpBB 0.4MB 0.9MB 0.3s 0.1s 0.4s 0.1s 0.7s 1.1s 1.7s 1.5s 4.1s 3.9s WordPress 1.4MB 1.7MB 0.2s 60ms 0.2s 80ms 0.7s",
    "link": "https://danluu.com/slow-device/"
  },
  {
    "id": "pytorch-is-dead-long-live-jax-neel-gupta-0eecd2d3",
    "title": "PyTorch is dead. Long live JAX. | Neel Gupta",
    "summary": "Home > Blogs > PyTorch is dead. Long live JAX. PyTorch is dead. Long live JAX. The future of Deep Learning frameworks July 19, 2024 Â· 24 min read Table of contents Assumed audience: ML researchers who frequently work with PyTorch , but are interested in trying out JAX or have yet to be convinced. Introduction # Usually, people start these â€˜critiquesâ€™ with a disclaimer that they are not trying to trash the framework, and talk about how itâ€™s a tradeoff. However, this is assumed - Iâ€™m not going to waste your time with that. Instead, Iâ€™ll focus on why PyTorch has been a net negative for all (if not most) scientific computing efforts, causing billions of dollars in lost productivity and thousands of wasted dev-hours. This is not because its a bad framework per-se, but rather - it simply because it wasnâ€™t designed for the use-cases itâ€™s being employed in right now. Ever since LuaTorch , PyTorch was supposed to be a â€œproduction ready, easy-to-use framework for quick prototypingâ€. It wasnâ€™t meant to be deployed onto huge, distributed clusters comprising of thousands of interconnected nodes and GPUs and scale well in a fault-tolerant and robust matter. The guiding philosophy of Torch was never about scale - despite what their marketing may have you believe - but flexibility . In response to the rising need of a scalable and performant framework, DeepMind developed JAX to meet a simple goal: â€œâ€¦ Supporting state-of-the-art AI research [by] balancing rapid prototyping and quick iteration with the ability to deploy experiments at a scale â€¦â€ - JAX blogpost This post is about convincing you how important this idea/philosophy is not only for Deep Learning, but for all scientfic computing that needs to happen at scale. I believe that all infrastructure built on Torch is just a huge pile of technical debt, that will haunt the field for a long, long time. The Philosophy # PyTorchâ€™s philosophy has always, in some ways, been antithetical to that of Tensorflowâ€™s. Where TF 1.x tried to be a static but performant framework by making strong use of the XLA compiler, PyTorch instead focused on being dynamic, easily debuggable and pythonic. Early on, the TF devs realized their mistakes when they came to realize how much the community hated the old 1.x API, which was counter-intuitive and introduced anti-pythonic patterns that were difficult to grasp for beginners. This prompted the core decision to use Keras as the main interface for TensorFlow and downplay the role of XLA compiler that was at TFâ€™s core. The main focus was on cleaning up the frontend as much as possible. This was a huge mistake. Sure, the API did improve and worked well for some people - but only as long as your workloads were standard. Any deviations from the norm were punished by stacktrace dumps that were often literal pages of just garbled XLA-HLO that were a nightmare to debug unless you had a strong grasp on the internals of the framework/compiler - which you didnâ€™t because XLA was a closed source, internal Google project at the time. In short, it had every hallmark of a typical Google product. Thus it comes as no surprise that people who switched over to PyTorch thought they had discovered literal heaven: I've been using PyTorch a few months now and I've never felt better. I have more energy. My skin is clearer. My eye sight has improved. â€” Andrej Karpathy (@karpathy) May 26, 2017 PyTorch stuck to its roots. Unlike TensorFlowâ€™s static & lazy approach, they took the bolder, more dynamic â€œeagerâ€ approach wherein all torch.Tensor s were evaluated immediately, leading to a much more cleaner abstraction than TensorFlowâ€™s. Clearly, they understood that complexity is the enemy of productivity. Instead of tacking on band-aids, they had pursued a fresh new path which paid off. Unsurprisingly, almost serious research moved to PyTorch: PyTorch vs. Tensorflow usage in research repos But in 2021 GPT-3 hit the scene and suddenly things started getting serious. All of a sudden, performance and scalability became the primary concern. PyTorch accomodated for this rising demand decently well, but because it wasnâ€™t designed around this philosophy - slowly the debt starting catching up and the foundations started crumbling. Itâ€™s hard to reconcile flexibility with performance. Clearly, a tradeoff needed to be made. Either they could give their biggest and richest users exactly what they wanted - a clean & scalable ecosystem that prioritized performance - which would be a static-oriented TF -like design - or they could try to hold on to what made Torch so special in the first place - being dynamic and â€œeagerâ€ at the expense of performance, and somehow delegate those large-scale workloads to an entirely seperate technological stack. So the devs, being the smart and rational engineers they are, choose an appropriate compromise which was . . . . to pursue both paths simultaneously. They were unwilling to make any tradeoffs. They wanted their cake and were going to eat it too. The new approach was ultimately a chaotic mishmash of competing features. You have on one hand, PyTorchâ€™s committment to eventually use some compiler (likely XLA ) as a performant and reliable default backend and on the other, to build up their own entire torch.compile stack that somehow meshes well with the eager, dynamic philosophy by giving users the freedom to invoke a compiler if need be. This lack of real long-term strategy is a serious issue. Take the torch.compile stack and the new DTensor API as an example. The documentation is transparent about the inspiration for this feature. It tries to bring the sharding model of parallelization from JAX to PyTorch . â€¦ When using DTensor in a [distributed] fashion it might be â€¦ slower compared to existing solutions like DDP/FSDP. This is mainly because DDP/FSDP have a global view of the entire model â€¦ [and can thus] optimize for data parallel specifically â€¦ [whereas] DistributedTensor â€¦ can only optimize within individual tensor operations. To improve",
    "link": "https://neel04.github.io/my-website/blog/pytorch_rant/"
  },
  {
    "id": "classifying-all-of-the-pdfs-on-the-internet-dde35c0f",
    "title": "classifying all of the pdfs on the internet",
    "summary": "The thumbnail of the article. Classifying all of the pdfs on the internet TLDR: I classified the entirety of SafeDocs using a mixture of LLMs, Embeddings Models, XGBoost and just for fun some LinearRegressors. In the process I too created some really pretty graphs ! Introduction How would you classify all the pdfs in the internet? Well, that is what I tried doing this time. Lets begin with the mother of all datasets: Common Crawl or CC is a web archive of all of the internet, it currently is petabytes in size and has been running since 2007. Maybe, you know about the Internet Archive which is almost the same but with the main difference being that Common Crawl focuses more on archiving the internet for scientists and researchers instead of digital preservation. What this translates into is that CC doesnâ€™t save all of the pdfs when it finds them. Specifically, when Common Crawl gets to a pdf, it just stores the first megabyte of information and truncates the rest. This is where SafeDocs or CC-MAIN-2021-31-PDF-UNTRUNCATED enters the picture. This corpus was originally created by the DARPA SafeDocs program and what it did was refetch all the different pdfs from a snapshot of Common Crawl to have untruncated versions of them. This dataset is incredibly big, it has roughly 8.4~ million pdfs that uncompressed total 8TB. This corpus is the biggest pure pdf dataset on the internet 1 . So I tried classifying it because it doesnâ€™t sound that hard. Dataset generation Lets define what classifying all of this pdfs using different labels actually means. For example: I wanted to tag a Linear Algebra pdf as Math or an Anatomy textbook as Medicine . Training pipeline for the embeddings model in FineWeb-Edu The reason for all of this is because I wanted to use LLMs in my personal projects and I got this idea after reading the Fineweb technical blog / paper . The FineWeb team endedup creating a subset for â€œeducationalâ€ content based on the bigger FineWeb dataset. What they did was use a Teacher and student approach where the LLM generates labels from unstructured text and then you train a smaller student or â€œdistilledâ€ learner capable of classifying based on the labels generated. So I decided to follow the same approach. The problem is that 8TB of data is still a lot of information. I donâ€™t have 8TB laying around, but we can be a bit more clever. The original dataset has the metadata available for download. Its only 8GB of pure text! In particular I cared about a specific column called url . I really care about the urls because they essentially tell us a lot more from a website than what meats the eye. For example: https://assets.openstax.org/oscms-prodcms/media/documents/Introduction_to_Python_Programming_-_WEB.pdf Specifically this part: Introduction_to_Python_Programming_-_WEB.pdf Tells us a lot of information. I know that this url is going to be education or technology adjacent. For now, lets just say its education because it has the â€œIntroductionâ€ part in its name. Here is where the bit of prompt engineering that I used enters the picture. Few shot prompting Few shot prompting is a fancy name for making an LLM learn with examples without training it. Its a really cool trick that you can use to make the models outputs more coherent and consistent. I am going to give you an extremely easy example of how one of this prompts look like: Few Shot prompting example of an addition This might feel like magic if it is the first time you are using it, but LLMs are incredible at following patterns and just showing how to do things can make it improve their performance. Using this method I generated 100k labels (to start) using this specific prompt and the Llama-3-70B model through the together API. The distribution of 100k labels looks originally like this: Distribution of 100k labels So, as you can see, there is a lot of really small data points that are going to be extremely annoying to classify. So guess what Iâ€™m going to do? Iâ€™m going to ditch anything with less than 250 labels and label them as other just to keep the most frequent classes. Because this labels are unbalanced, I decided to balance them and just take 5k samples at the most of each possible label. This left me with a total of 59k labels: 59k label pie chart, more distributed With this newly minted dataset, I decided to start going after the training of the new student classifier! Model Training Idea 1 Example of embeddings I am going to introduce the idea of an embeddings model. For short, an embeddings model is a model capable of passing things like text, images, video or any other â€œunstructuredâ€ information to vectors in an ndimensional space with semantic meaning. In other words, you can have some points that are near each other like dog, cat, elephant and this ones are going to be separated from something like car, truck or vehicle. With that in mind, this models are great for generating clusters of meaning. This is going to be extremely useful for us, because we are going to just make this model be capable of classifying the specific labels that we ended up having through a process called finetuning. Finetuning is were you grab an already trained model for a generic task and just train it a little bit more for your particular problem. This case being classification. If you want to learn more about finetuning you should check out the first fast.ai lecture where they finetune a model. In the FineWeb paper, they used a total of 500k labels. Thats a lot more than what I currently had, but at the time, I decided to ignore this 2 . I went on with my life and decided to just start testing some models. And oh boy, I realized that you can do a lot with just a gaming laptop. In FineWeb Edu they used a specific embeddings",
    "link": "https://snats.xyz/pages/articles/classifying_a_bunch_of_pdfs.html"
  },
  {
    "id": "strangely-matrix-multiplications-on-gpus-run-faste-55ca6e9a",
    "title": "Strangely, Matrix Multiplications on GPUs Run Faster When Given \"Predictable\" Data! [short]",
    "summary": "Strangely, Matrix Multiplications on GPUs Run Faster When Given \"Predictable\" Data! [short] Great minds discuss flops per watt. Horace He Apr 29, 2024 134 21 8 Share Itâ€™s 2022. I check out this cool new project, CUTLASS , with very fast matmuls. I take a large matmul, 8192 x 8192 x 8192, and benchmark it in PyTorch, which calls CuBLAS. python mm_bench.py > CuBLAS: 258 Teraflops Not bad, 83% flop utilization. Now letâ€™s check out Cutlassâ€™s performance using their profiler. ./cutlass_profiler --operation=Gemm --m=8192 --n=8192 --k=8192 > CUTLASS: 288 Teraflops !!! 10% higher perf? Thatâ€™s incredible. CuBLAS is highly optimized for large compute-bound matmuls, and somehow CUTLASS + autotuning is outperforming it by 10%? We gotta start using these matmuls yesterday. The next step is to bind the CUTLASS kernels into Python and compare against CuBLAS using my previous script. python cutlass_mm_bench.py > CuBLAS: 258 Teraflops > CUTLASS: 257 Teraflops Somehow, in the light of Python, all of CUTLASSâ€™s performance gains disappear. This in of itself is not shocking - itâ€™s notoriously difficult to ensure consistent benchmarking across setups. I tediously ablate the two benchmark scripts, until finally, I find that CUTLASSâ€™s profiler, by default, actually initializes the values in a fairly strange way - it only initializes the inputs with integers. Confused about whether this matters, I try: zero_inputs = torch.zeros(N, N) randn_inputs = torch.randn(N, N) benchmark(zero_inputs) # 295 Teraflops benchmark(randn_inputs) # 257 Teraflops What? How could the values of the matrix affect the runtime of the model? I know Nvidia has some weird data compression thing on A100s, but I wouldnâ€™t have expected that to be on in matmuls. Letâ€™s try some other data distributions, like an uniform distribution [0,1]. This was â€¦ confusing, to say the least. Somehow, the actual content of the tensors being multiplied is leading to different matmul performance. There certainly are cases where the runtime depends on the content of the tensor â€” indirect indexing (e.g. A[b] ), or things like sparsity. But matrix multiplications have nothing like that at all! No matter what the contents of the matrix contain, the matrix multiplication kernel will 1. perform the same number of computations, 2. perform the same computations in the same order, 3. access the same memory addresses, and 4. access the same memory addresses in the same order. Nowhere did my mental model of matrix multiplications and GPU hardware allow for the values in the matrix to influence matmul performance. And yet, here we are. As it turns out, the culprit is â€¦â€¦. dynamic/switching power in semiconductors! Thonk From First Principles is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber. Subscribe Power Usage in Semiconductors An Nvidia A100 GPU has a power limit of 400W 1 . However, as the phrase â€œpower limitâ€ may hint, the GPU doesnâ€™t always use all 400W. For example, when the GPU is fully idle, nvidia-smi tells me that itâ€™s only pulling 88W of power. But when the GPU is running under load, that power usage will spike considerably, typically to around the power limit. In order to stay under the power limit, a piece on the chip called the Voltage Regulator Module reduces the voltage supplied to the GPU, â€” throttling the clock frequency and reducing its performance. In other words, if our GPU ends up using enough power to hit the power limit, our performance will become capped. Most of us take it for granted that â€œGPU does something, power consumption goes upâ€. But there are actually two distinct mechanisms through which power gets consumed. Dynamic/switching power on the left, static/leakage power on the right. Taken from https://semiengineering.com/knowledge_centers/low-power/low-power-design/power-consumption/ The first one is static/leakage power. You can think of this as the power that inevitably gets lost by just flowing power through your circuits. The amount of static power used is proportional the amount of silicon that is powered. As GPUs donâ€™t do much power gating , this is essentially the amount of power used at idle (88W in the above photo). However, the second one, dynamic (or switching) power, is the culprit. Specifically, a small amount of power is consumed whenever a transistor switches states . If the transistor never needs to switch states, it doesnâ€™t consume any extra power. On the other hand, if itâ€™s rapidly flipping, then it consumes a ton of dynamic/switching power. Multiply that by the billions of transistors in your GPU, and you get the overall increase in power consumption. In other words, the reason why matrix multiplications are faster when passed zeros is that this reduces the â€œflippingâ€ of enough transistors in the chip to stay under the power limit! So, this (mostly) explains what we saw previously 2 . All zeros are probably the fastest since every single bit of each computation is a zero and the accumulator remains at zero. All ones is probably still quite fast since every single tensor-core instruction results in exactly the same values. The uniform distribution probably is a little bit faster than the normal distribution since the accumulator never needs to flip-flop between positive and negative. The normal distribution probably has the worst performance since it leads to pretty high randomness among all transistors involved in the computation(?). Hereâ€™s the results on a number of fun distributions I tried: Randn: Normally distributed Checkerboard: Normal distribution, but we have zeros in a checkerboard shape. Rand: Uniform distribution Sparse: Normal distribution, but a (random) 75% of the elements are masked. Ternary: Every value is 1, -1, or 0. One Bit: Only one bit set in every single value (the 4th bit) All Pies: Every single value is the mathematical constant PI. Twos: Every value in the matrices are 2 Zeros: Every value in the matrices are 0 Who says unstructured sparsity isnâ€™t efficient with tensor-cores? :) How power limit and clock speed affects this Hereâ€™s another piece of evidence that dynamic/switching power is responsible. Roughly, the power weâ€™re using is proportional to the clock speed",
    "link": "https://www.thonking.ai/p/strangely-matrix-multiplications"
  },
  {
    "id": "what-shapes-do-matrix-multiplications-like-medium-ca46bcfe",
    "title": "What Shapes Do Matrix Multiplications Like? [medium]",
    "summary": "What Shapes Do Matrix Multiplications Like? [medium] Divining order from the chaos Horace He Apr 01, 2024 89 2 3 Share A while back, Karpathy tweeted that increasing the size of his matmul made it run faster. Surprisingly, itâ€™s not just relatively faster, it takes less absolute time. In other words, despite doing more work, it is executing in less time. https://twitter.com/karpathy/status/1621578354024677377 This may seem intuitively quite strange. Is cuBLAS just messing up somehow? Why doesnâ€™t the matrix multiplication kernel just pad it to a larger shape? It has become tribal knowledge that the particular shapes chosen for matmuls has a surprisingly large effect on their performance. But â€¦ why? Can this be understood by mere mortals? Letâ€™s take a crack at it. First, letâ€™s plot FLOPs achieved for square matmuls. By the end of this article, I will aim to explain all the strange squiggly lines. There are 3 general concepts to understand that explain the majority of performance variation among matmul shapes. Compute Intensity/Parallelization: This explains the general upward trend Tiling: This explains the multiple tiers of lines. Wave Quantization: This explains the strange striped lines. Thonk From First Principles is a reader-supported publication. To receive new posts and support my work, consider becoming a free or paid subscriber. Subscribe Compute Intensity and More Parallelism First of all, as we move along the x-axis, the matrix multiplications generally get more performant. Thereâ€™s two primary reasons for this. The first one is simply â€œmore work/more parallelismâ€. There are a large number of fixed overheads that come with launching a kernel (e.g. creating new SMs, waiting for all SMs to finish, etc.), and so, the more work we have to do, the less important those fixed overheads are. Along with more work comes more parallelism, and since GPUs have a ton of parallel cores, you need a surprising amount of work in order to fill a GPU up with enough parallelism. Data movement is expensive! The second one is â€œarithmetic intensityâ€. As Iâ€™ve written about before , memory accesses are much more expensive than compute. So, since a square matmul performs 3N^2 memory accesses and 2N^3 FLOPs, at a very minimum, N needs to be in the hundreds before we start spending more time on compute than memory! The desire for sufficient Arithmetic Intensity and Parallelism also compound. For example, letâ€™s say you have your output matrix is 1024 x 1024 . If you let each SM compute a 128 x 128 slice of the output, thatâ€™s only 64 pieces of â€œworkâ€ for your GPU, not even enough for each one of an A100â€™s 108 SMs ! If you decrease your output slice size to 64 x 64, we now have 256 pieces of â€œworkâ€ for our GPU, but our arithmetic intensity has also decreased by a factor of 2. With smaller matrix sizes, you need to worry about problems like this that donâ€™t show up with larger matrices. Tiling Now that we understand the overall structure of the plot, the next question is: why is the plot all over the place? Why, even for very large matrices, do the TFLOPS jumping between >250 and <100? To give a hint, letâ€™s color-code each dot by the highest power of 2 itâ€™s divisible by. As it turns out, the multiple â€œlevelsâ€ of FLOPS are due to their shapesâ€™ divisibility. For example, when the shape is odd, the matmul performs significantly worse than when the shape is even. The matmul performs even better when the shape is divisible by 8, with even more performance gains when itâ€™s divisible by 16 or 32. Now, merely knowing about this effect is very practically useful, but what actually causes this effect? As it turns out, the answer is tiling. But, what even is tiling? And why does it cause such substantial performance issues? Taken from https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#tile-quant Some online have mentioned tile quantization as the culprit. Tile quantization certainly can impact performance, but only at tile boundary sizes . Basically, tile quantization occurs when the size of your matrix multiplication increases such that the GPU needs to launch another â€œchunkâ€ of work. For example, imagine that you could multiply 8 elements at a time with a SIMD instruction. Now, if you went from 32 elements to 33 elements (a 3% increase in problem size), you go from needing 4 SIMD instructions to 5 (a 25% increase). Note that crucially, when tile quantization is the culprit, your absolute runtime still grows monotonically, although your efficiency may drop. However, in our above plot, we see much more drastic performance drops! Moreover, like in Karpathyâ€™s original example, we see that the absolute runtime decreases despite problem size increasing . So, tile quantization cannot be the explanation here. The true cause is that tiling is just fundamentally worse for certain memory layouts. In other words, by the time weâ€™re trying to execute the matmul, youâ€™ve already lost. The memory layout is poor and your performance will suffer. Letâ€™s look at some examples! Memory Layout of Tiling First, letâ€™s think about how our matrixâ€™s memory layout looks like when our size is a multiple of the cache line (pretend itâ€™s 4 elements). I choose to show 3 â€œcache linesâ€ per row because our matrix logically has 12 elements per row. We see that each row starts on a cache line 1 . Among other advantages, this means that we donâ€™t need to perform any â€œunnecessaryâ€ loads to load all yellow elements. We can just load the 3 cache lines that the yellow elements are part of. However, what happens if we increase the number of elements per row from 12 to 13? Each logical row (which have 13 elements) no longer starts aligned with a cache line. With an unaligned layout, each row is misaligned relative to our cache line. In other words, if we start loading the beginning of the green row, we must redundantly load the last element of the blue row as well. Now, letâ€™s look at what",
    "link": "https://www.thonking.ai/p/what-shapes-do-matrix-multiplications"
  },
  {
    "id": "zen-cuda-and-tensor-cores-part-i-the-silicon-35944909",
    "title": "Zen, CUDA, and Tensor Cores, Part I: The Silicon",
    "summary": "Zen, CUDA, and Tensor Cores, Part I: The Silicon Casey Muratori Sep 03, 2024 182 9 4 Share This is part 1 of a miniseries on todayâ€™s common consumer computing cores. It contains investigations into undisclosed hardware details. I have tried to validate the accuracy of the series by consulting hardware designers, and have done my best to paint an accurate picture of what we know â€” and what we donâ€™t know â€” about modern core designs. Please keep in mind that many aspects of this series represent collective best guesses, not certainties. Whatâ€™s the difference between a Zen core, a CUDA core, and a Tensor core? Not vaguely â€” like â€œone is for graphics, one is for AIâ€ and so on â€” but specifically, how does each â€œcoreâ€ differ in design and operation? In this multi-part series, weâ€™re going to look in detail at these core types to help bridge the gap between the vague conceptual understanding most of us already have, and the in-depth knowledge we could use to make better sense of hardware behavior across core types. Here in part 1, weâ€™ll look at the most tangible aspect of a core: what it physically looks like. Weâ€™ll put aside the abstract flow charts and design diagrams, and look directly at the silicon itself. Letâ€™s take a CPU on the left and a GPU on the right and we'll zoom in on the silicon to try to find the things their manufacturers call â€œcoresâ€. For the CPU, we'll use an AMD Ryzen 5 7600, and for the GPU we'll use an NVIDIA RTX 4090: Now, these are in no way comparable chips in terms of die area or cost. I picked them because they contain cores that were popular at the time of this recording, and have publicly-available high-quality die shots. Thankfully, as we'll see, it doesn't matter that much which specific chips we look at. Once we understand what the different types of cores are and what they do, we can apply our knowledge to any CPU or GPU whose layout we know. For now, on the left side we're looking for a Zen 4 core, and on the right side weâ€™re looking for an Ada Lovelace CUDA core and Tensor core. According to their spec sheets, the 7600 should have 6 Zen 4 cores, and the RTX 4090 should have 16,384 CUDA cores and 512 Tensor cores: Letâ€™s see if we can find them! If we remove the cooler and the integrated heat spreader from the chips, this is what they look like: On the GPU side, there's just one big chip labeled AD102. On the CPU side, itâ€™s not so simple. There are two separate chips on the 7600 module. The top chip is actually the IO die and doesn't contain any Zen 4 cores. To find a Zen 4 core, we need to look at the bottom chip which is called the Core Complex Die, or CCD: Now, as we zoom in on these chips, itâ€™s important to remember that the scale is very different. The Zen 4 CCD is only 70 square millimeters. This is much smaller than the monolithic NVIDIA chip that's over 600 square millimeters. The NVIDIA chip is almost nine times larger. Please keep that in mind as we look at these chips in detail. Although it may not appear so at first, although they differ greatly in size, the high-level layout of these two chips is actually very similar. If we start on the CPU side, we can split the chip into three distinct sections. There are parts on the left and the right that appear to be mirrors of each other, and then there's a band running down the middle. On the GPU side, we see something similar, but laid out vertically rather than horizontally. The top part and the bottom part mirror each other rather than the left and the right. If we rotate the GPU 90 degrees like this: we can line up their layouts more directly. On both chips, the middle section doesnâ€™t contain the cores weâ€™re looking for. Itâ€™s occupied by shared resources, such as the last-level cache, or â€œLLCâ€. To find our cores, we have to look at the bands on either side, and since they are mirrors of each other, we can simplify things by focusing on just the left band. If we zoom in, we see that the CPU and GPU have a similar layout once again. From top to bottom, both are made up of what appear to be repeating tiles. On the CPU side, there are four, and and on the GPU side, six: If the only type of core we were looking for was a Zen 4 core, we could now congratulate ourselves because we have found one. Each of the tiles on the CPU side is actually a Zen 4 core, complete with its dedicated L2 cache. On the GPU side, however, weâ€™re far from done. NVIDIA doesnâ€™t call these tiles â€œcoresâ€ at all â€” it calls them Graphics Processing Clusters, or â€œGPCsâ€. Measuring the size of the Zen 4 core and the Ada GPC, we can see that the GPC is much larger: nearly seven times larger, in fact! As you might guess from the term â€œClusterâ€, if we look closely, we can see that the GPC itself is made up of repeating tiles. Although there's a section on the right which contains several non-repeating things like the raster engine, we can see that the rest of the GPC is made up of what appear to be three more repeating tiles. However, if we zoom in on one the tiles, we see that actually, the tiles themselves each appear to be made up of two mirrored sub-tiles â€” one on the top and one on the bottom â€” so in fact there are six sub-tiles in a GPC. If we measure the size of one of these sub-tiles, we actually find that it matches the area",
    "link": "https://www.computerenhance.com/p/zen-cuda-and-tensor-cores-part-i"
  },
  {
    "id": "physics-of-language-models-part-22-how-to-learn-fr-ef09a99c",
    "title": "Physics of Language Models: Part 2.2, How to Learn From Mistakes on Grade-School Math Problems",
    "summary": "Language models have demonstrated remarkable performance in solving reasoning tasks; however, even the strongest models still occasionally make reasoning mistakes. Recently, there has been active research aimed at improving reasoning accuracy, particularly by using pretrained language models to \"self-correct\" their mistakes via multi-round prompting. In this paper, we follow this line of work but focus on understanding the usefulness of incorporating \"error-correction\" data directly into the pretraining stage. This data consists of erroneous solution steps immediately followed by their corrections. Using a synthetic math dataset, we show promising results: this type of pretrain data can help language models achieve higher reasoning accuracy directly (i.e., through simple auto-regression, without multi-round prompting) compared to pretraining on the same amount of error-free data. We also delve into many details, such as (1) how this approach differs from beam search, (2) how such data can be prepared, (3) whether masking is needed on the erroneous tokens, (4) the amount of error required, (5) whether such data can be deferred to the fine-tuning stage, and many others.",
    "link": "https://arxiv.org/abs/2408.16293"
  },
  {
    "id": "physics-of-language-models-part-21-grade-school-ma-dfadbd02",
    "title": "Physics of Language Models: Part 2.1, Grade-School Math and the Hidden Reasoning Process",
    "summary": "Recent advances in language models have demonstrated their capability to solve mathematical reasoning problems, achieving near-perfect accuracy on grade-school level math benchmarks like GSM8K. In this paper, we formally study how language models solve these problems. We design a series of controlled experiments to address several fundamental questions: (1) Can language models truly develop reasoning skills, or do they simply memorize templates? (2) What is the model's hidden (mental) reasoning process? (3) Do models solve math questions using skills similar to or different from humans? (4) Do models trained on GSM8K-like datasets develop reasoning skills beyond those necessary for solving GSM8K problems? (5) What mental process causes models to make reasoning mistakes? (6) How large or deep must a model be to effectively solve GSM8K-level math questions?\n  Our study uncovers many hidden mechanisms by which language models solve mathematical questions, providing insights that extend beyond current understandings of LLMs.",
    "link": "https://arxiv.org/abs/2407.20311"
  },
  {
    "id": "physics-of-language-models-part-31-knowledge-stora-53b062ff",
    "title": "Physics of Language Models: Part 3.1, Knowledge Storage and Extraction",
    "summary": "Large language models (LLMs) can store a vast amount of world knowledge, often extractable via question-answering (e.g., \"What is Abraham Lincoln's birthday?\"). However, do they answer such questions based on exposure to similar questions during training (i.e., cheating), or by genuinely learning to extract knowledge from sources like Wikipedia?\n  In this paper, we investigate this issue using a controlled biography dataset. We find a strong correlation between the model's ability to extract knowledge and various diversity measures of the training data. $\\textbf{Essentially}$, for knowledge to be reliably extracted, it must be sufficiently augmented (e.g., through paraphrasing, sentence shuffling, translations) $\\textit{during pretraining}$. Without such augmentation, knowledge may be memorized but not extractable, leading to 0% accuracy, regardless of subsequent instruction fine-tuning.\n  To understand why this occurs, we employ (nearly) linear probing to demonstrate a strong connection between the observed correlation and how the model internally encodes knowledge -- whether it is linearly encoded in the hidden embeddings of entity names or distributed across other token embeddings in the training text.\n  This paper provides $\\textbf{several key recommendations for LLM pretraining in the industry}$: (1) rewrite the pretraining data -- using small, auxiliary models -- to provide knowledge augmentation, and (2) incorporate more instruction-finetuning data into the pretraining stage before it becomes too late.",
    "link": "https://arxiv.org/abs/2309.14316"
  },
  {
    "id": "physics-of-language-models-part-32-knowledge-manip-e6b74546",
    "title": "Physics of Language Models: Part 3.2, Knowledge Manipulation",
    "summary": "Language models can store vast factual knowledge, yet their ability to flexibly use this knowledge for downstream tasks (e.g., via instruction finetuning) remains questionable. This paper investigates four fundamental knowledge manipulation tasks: retrieval (e.g., \"What is person A's attribute X?\"), classification (e.g., \"Is A's attribute X even or odd?\"), comparison (e.g., \"Is A greater than B in attribute X?\"), and inverse search (e.g., \"Which person's attribute X equals T?\").\n  We show that language models excel in knowledge retrieval but struggle even in the simplest classification or comparison tasks unless Chain of Thoughts (CoTs) are employed during both training and inference. Moreover, their performance in inverse knowledge search is virtually 0%, regardless of the prompts. Our primary contribution is a controlled, synthetic experiment that confirms these weaknesses are inherent to language models: they cannot efficiently manipulate knowledge from pre-training data, even when such knowledge is perfectly stored in the models, despite adequate training and sufficient model size. Our findings also apply to modern pretrained language models such as GPT-4, thus giving rise to many Turing tests to distinguish Humans from contemporary AIs.",
    "link": "https://arxiv.org/abs/2309.14402"
  },
  {
    "id": "amortized-planning-with-large-scale-transformers-a-1522f2d2",
    "title": "Amortized Planning with Large-Scale Transformers: A Case Study on Chess",
    "summary": "This paper uses chess, a landmark planning problem in AI, to assess transformers' performance on a planning task where memorization is futile $\\unicode{x2013}$ even at a large scale. To this end, we release ChessBench, a large-scale benchmark dataset of 10 million chess games with legal move and value annotations (15 billion data points) provided by Stockfish 16, the state-of-the-art chess engine. We train transformers with up to 270 million parameters on ChessBench via supervised learning and perform extensive ablations to assess the impact of dataset size, model size, architecture type, and different prediction targets (state-values, action-values, and behavioral cloning). Our largest models learn to predict action-values for novel boards quite accurately, implying highly non-trivial generalization. Despite performing no explicit search, our resulting chess policy solves challenging chess puzzles and achieves a surprisingly strong Lichess blitz Elo of 2895 against humans (grandmaster level). We also compare to Leela Chess Zero and AlphaZero (trained without supervision via self-play) with and without search. We show that, although a remarkably good approximation of Stockfish's search-based algorithm can be distilled into large-scale transformers via supervised learning, perfect distillation is still beyond reach, thus making ChessBench well-suited for future research.",
    "link": "https://arxiv.org/abs/2402.04494"
  },
  {
    "id": "gsm-symbolic-understanding-the-limitations-of-math-9dbed5ae",
    "title": "GSM-Symbolic: Understanding the Limitations of Mathematical Reasoning in Large Language Models",
    "summary": "Recent advancements in Large Language Models (LLMs) have sparked interest in their formal reasoning capabilities, particularly in mathematics. The GSM8K benchmark is widely used to assess the mathematical reasoning of models on grade-school-level questions. While the performance of LLMs on GSM8K has significantly improved in recent years, it remains unclear whether their mathematical reasoning capabilities have genuinely advanced, raising questions about the reliability of the reported metrics. To address these concerns, we conduct a large-scale study on several SOTA open and closed models. To overcome the limitations of existing evaluations, we introduce GSM-Symbolic, an improved benchmark created from symbolic templates that allow for the generation of a diverse set of questions. GSM-Symbolic enables more controllable evaluations, providing key insights and more reliable metrics for measuring the reasoning capabilities of models.Our findings reveal that LLMs exhibit noticeable variance when responding to different instantiations of the same question. Specifically, the performance of all models declines when only the numerical values in the question are altered in the GSM-Symbolic benchmark. Furthermore, we investigate the fragility of mathematical reasoning in these models and show that their performance significantly deteriorates as the number of clauses in a question increases. We hypothesize that this decline is because current LLMs cannot perform genuine logical reasoning; they replicate reasoning steps from their training data. Adding a single clause that seems relevant to the question causes significant performance drops (up to 65%) across all state-of-the-art models, even though the clause doesn't contribute to the reasoning chain needed for the final answer. Overall, our work offers a more nuanced understanding of LLMs' capabilities and limitations in mathematical reasoning.",
    "link": "https://arxiv.org/abs/2410.05229v1"
  },
  {
    "id": "contextual-document-embeddings-1c0b77f0",
    "title": "Contextual Document Embeddings",
    "summary": "Dense document embeddings are central to neural retrieval. The dominant paradigm is to train and construct embeddings by running encoders directly on individual documents. In this work, we argue that these embeddings, while effective, are implicitly out-of-context for targeted use cases of retrieval, and that a contextualized document embedding should take into account both the document and neighboring documents in context - analogous to contextualized word embeddings. We propose two complementary methods for contextualized document embeddings: first, an alternative contrastive learning objective that explicitly incorporates the document neighbors into the intra-batch contextual loss; second, a new contextual architecture that explicitly encodes neighbor document information into the encoded representation. Results show that both methods achieve better performance than biencoders in several settings, with differences especially pronounced out-of-domain. We achieve state-of-the-art results on the MTEB benchmark with no hard negative mining, score distillation, dataset-specific instructions, intra-GPU example-sharing, or extremely large batch sizes. Our method can be applied to improve performance on any contrastive learning dataset and any biencoder.",
    "link": "https://arxiv.org/abs/2410.02525"
  },
  {
    "id": "is-the-watermarking-of-llm-generated-code-robust-d931770d",
    "title": "Is The Watermarking Of LLM-Generated Code Robust?",
    "summary": "We present the first in depth study on the robustness of existing watermarking techniques applied to code generated by large language models (LLMs). As LLMs increasingly contribute to software development, watermarking has emerged as a potential solution for detecting AI generated code and mitigating misuse, such as plagiarism or the automated generation of malicious programs. While previous research has demonstrated the resilience of watermarking in the text setting, our work reveals that watermarking techniques are significantly more fragile in code-based contexts. Specifically, we show that simple semantic-preserving transformations, such as variable renaming and dead code insertion, can effectively erase watermarks without altering the program's functionality. To systematically evaluate watermark robustness, we develop an algorithm that traverses the Abstract Syntax Tree (AST) of a watermarked program and applies a sequence of randomized, semantics-preserving transformations. Our experimental results, conducted on Python code generated by different LLMs, indicate that even minor modifications can drastically reduce watermark detectability, with true positive rates (TPR) dropping below 50% in many cases. Our code is publicly available at https://github.com/uiuc-arc/llm-code-watermark.",
    "link": "https://arxiv.org/abs/2403.17983"
  },
  {
    "id": "watermark-stealing-in-large-language-models-5b69557d",
    "title": "Watermark Stealing in Large Language Models",
    "summary": "LLM watermarking has attracted attention as a promising way to detect AI-generated content, with some works suggesting that current schemes may already be fit for deployment. In this work we dispute this claim, identifying watermark stealing (WS) as a fundamental vulnerability of these schemes. We show that querying the API of the watermarked LLM to approximately reverse-engineer a watermark enables practical spoofing attacks, as hypothesized in prior work, but also greatly boosts scrubbing attacks, which was previously unnoticed. We are the first to propose an automated WS algorithm and use it in the first comprehensive study of spoofing and scrubbing in realistic settings. We show that for under $50 an attacker can both spoof and scrub state-of-the-art schemes previously considered safe, with average success rate of over 80%. Our findings challenge common beliefs about LLM watermarking, stressing the need for more robust schemes. We make all our code and additional examples available at https://watermark-stealing.org.",
    "link": "https://arxiv.org/abs/2402.19361"
  },
  {
    "id": "softmax-is-not-enough-for-sharp-size-generalisatio-c2ac6a46",
    "title": "Softmax is not Enough (for Sharp Size Generalisation)",
    "summary": "A key property of reasoning systems is the ability to make sharp decisions on their input data. For contemporary AI systems, a key carrier of sharp behaviour is the softmax function, with its capability to perform differentiable query-key lookups. It is a common belief that the predictive power of networks leveraging softmax arises from \"circuits\" which sharply perform certain kinds of computations consistently across many diverse inputs. However, for these circuits to be robust, they would need to generalise well to arbitrary valid inputs. In this paper, we dispel this myth: even for tasks as simple as finding the maximum key, any learned circuitry must disperse as the number of items grows at test time. We attribute this to a fundamental limitation of the softmax function to robustly approximate sharp functions with increasing problem size, prove this phenomenon theoretically, and propose adaptive temperature as an ad-hoc technique for improving the sharpness of softmax at inference time.",
    "link": "https://arxiv.org/abs/2410.01104"
  },
  {
    "id": "the-page-of-entropy-34a10ffe",
    "title": "The Page of Entropy",
    "summary": "In all of physics, there is perhaps no topic more underrated and misunderstood than entropy. The behavior of large collections of particles, such as the universe, a grain of sand, or a tuna salad sandwich, is dictated by two universal laws: one involving energy, the other involving entropy. And yet, while energy is described in great detail throughout any introductory physics textbook, entropy is relegated to about two or three pages, and is usually badly described. Well, no more! Here's the real story of physics. Here's what really drives the universe. Here's what your physics instructor won't tell you. Here's entropy !!! The answer is entropy . Entropy and poker. Entropy and a box of air. Entropy and energy. Here it is: The Second Law of Thermodynamics But wait! I've put water in my freezer, and it comes out ice! Entropy and disorder. Other pages on entropy and the second law While the page you're looking at focuses on the microscopic details of entropy, here's one that focuses on the large scale effects of the second law. The Information Please encyclopedia entry on thermodynamics. If you know of other pages on entropy that I might want to link to, please let me know . Back to Dave's Physics Shack ? If you like \"The Page of E n t Ro P y ,\" why not try Dave's other educational web pages? Dave's Relativity Page The Page of Uncertainty Dave's Microcosmos The Atom A quick note about Dave's educational web pages. Comments on The Page of E n t Ro P y? JavaScript must be enabled to display this email address. . The Page of Entropy is presented with the assistance of Morningside College. It was first posted to the web in 1996 and has been slightly modified on several occasions since then. The cartoon is by Sydney Harris, who has done lots of cartoons about science. He has books out. I own Einstein Simplified: Cartoons About Science. It's very good. Back?",
    "link": "https://webs.morningside.edu/slaven/Physics/entropy/"
  },
  {
    "id": "fast-transformer-decoding-one-write-head-is-all-yo-3940b4b3",
    "title": "Fast Transformer Decoding: One Write-Head is All You Need",
    "summary": "Multi-head attention layers, as used in the Transformer neural sequence model, are a powerful alternative to RNNs for moving information across and between sequences. While training these layers is generally fast and simple, due to parallelizability across the length of the sequence, incremental inference (where such paralleization is impossible) is often slow, due to the memory-bandwidth cost of repeatedly loading the large \"keys\" and \"values\" tensors. We propose a variant called multi-query attention, where the keys and values are shared across all of the different attention \"heads\", greatly reducing the size of these tensors and hence the memory bandwidth requirements of incremental decoding. We verify experimentally that the resulting models can indeed be much faster to decode, and incur only minor quality degradation from the baseline.",
    "link": "https://arxiv.org/abs/1911.02150"
  },
  {
    "id": "language-models-represent-space-and-time-da25ec4d",
    "title": "Language Models Represent Space and Time",
    "summary": "The capabilities of large language models (LLMs) have sparked debate over whether such systems just learn an enormous collection of superficial statistics or a set of more coherent and grounded representations that reflect the real world. We find evidence for the latter by analyzing the learned representations of three spatial datasets (world, US, NYC places) and three temporal datasets (historical figures, artworks, news headlines) in the Llama-2 family of models. We discover that LLMs learn linear representations of space and time across multiple scales. These representations are robust to prompting variations and unified across different entity types (e.g. cities and landmarks). In addition, we identify individual \"space neurons\" and \"time neurons\" that reliably encode spatial and temporal coordinates. While further investigation is needed, our results suggest modern LLMs learn rich spatiotemporal representations of the real world and possess basic ingredients of a world model.",
    "link": "https://arxiv.org/abs/2310.02207"
  },
  {
    "id": "rethinking-conventional-wisdom-in-machine-learning-e805a0a3",
    "title": "Rethinking Conventional Wisdom in Machine Learning: From Generalization to Scaling",
    "summary": "The remarkable success of large language pretraining and the discovery of scaling laws signify a paradigm shift in machine learning. Notably, the primary objective has evolved from minimizing generalization error to reducing approximation error, and the most effective strategy has transitioned from regularization (in a broad sense) to scaling up models. This raises a critical question:\n  Do the established principles that proved successful in the generalization-centric era remain valid in this new era of scaling?\n  This paper examines several influential regularization-based principles that may no longer hold true in the scaling-centric, large language model (LLM) era. These principles include explicit L2 regularization and implicit regularization through small batch sizes and large learning rates. Additionally, we identify a new phenomenon termed ``scaling law crossover,'' where two scaling curves intersect at a certain scale, implying that methods effective at smaller scales may not generalize to larger ones. Together, these observations highlight two fundamental questions within this new paradigm:\n  $\\bullet$ Guiding Principles for Scaling: If regularization is no longer the primary guiding principle for model design, what new principles are emerging to guide scaling?\n  $\\bullet$ Model Comparison at Scale: How to reliably and effectively compare models at the scale where only a single experiment is feasible?",
    "link": "https://arxiv.org/abs/2409.15156"
  },
  {
    "id": "training-large-language-models-to-reason-in-a-cont-1dab7033",
    "title": "Training Large Language Models to Reason in a Continuous Latent Space",
    "summary": "Large language models (LLMs) are typically constrained to reason in the language space, where they express the reasoning process through a chain-of-thought (CoT) to solve complex problems. However, the language space may not always be optimal for reasoning. Most word tokens primarily ensure textual coherence and are not essential for reasoning, while some critical tokens require complex planning and pose challenges to LLMs. To explore the potential of reasoning beyond language, we introduce a new paradigm called Coconut (Chain of Continuous Thought). Coconut utilizes the last hidden state of the LLM as a representation of the reasoning state, termed \"continuous thought.\" Instead of decoding this state into words, we feed it back to the model as the next input embedding directly in the continuous space. This latent reasoning paradigm enables an advanced reasoning pattern, where continuous thoughts can encode multiple alternative next steps, allowing the model to perform a breadth-first search (BFS) rather than committing prematurely to a single deterministic path as in CoT. Coconut outperforms CoT on logical reasoning tasks that require substantial search during planning and achieves a better trade-off between accuracy and efficiency.",
    "link": "https://arxiv.org/abs/2412.06769"
  },
  {
    "id": "deepseek-r1-incentivizing-reasoning-capability-in--b22513d5",
    "title": "DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning",
    "summary": "We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero, a model trained via large-scale reinforcement learning (RL) without supervised fine-tuning (SFT) as a preliminary step, demonstrates remarkable reasoning capabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoning behaviors. However, it encounters challenges such as poor readability, and language mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1, which incorporates multi-stage training and cold-start data before RL. DeepSeek-R1 achieves performance comparable to OpenAI-o1-1217 on reasoning tasks. To support the research community, we open-source DeepSeek-R1-Zero, DeepSeek-R1, and six dense models (1.5B, 7B, 8B, 14B, 32B, 70B) distilled from DeepSeek-R1 based on Qwen and Llama.",
    "link": "https://arxiv.org/abs/2501.12948"
  },
  {
    "id": "deepseek-v3-technical-report-558dac37",
    "title": "DeepSeek-V3 Technical Report",
    "summary": "We present DeepSeek-V3, a strong Mixture-of-Experts (MoE) language model with 671B total parameters with 37B activated for each token. To achieve efficient inference and cost-effective training, DeepSeek-V3 adopts Multi-head Latent Attention (MLA) and DeepSeekMoE architectures, which were thoroughly validated in DeepSeek-V2. Furthermore, DeepSeek-V3 pioneers an auxiliary-loss-free strategy for load balancing and sets a multi-token prediction training objective for stronger performance. We pre-train DeepSeek-V3 on 14.8 trillion diverse and high-quality tokens, followed by Supervised Fine-Tuning and Reinforcement Learning stages to fully harness its capabilities. Comprehensive evaluations reveal that DeepSeek-V3 outperforms other open-source models and achieves performance comparable to leading closed-source models. Despite its excellent performance, DeepSeek-V3 requires only 2.788M H800 GPU hours for its full training. In addition, its training process is remarkably stable. Throughout the entire training process, we did not experience any irrecoverable loss spikes or perform any rollbacks. The model checkpoints are available at https://github.com/deepseek-ai/DeepSeek-V3.",
    "link": "https://arxiv.org/abs/2412.19437"
  },
  {
    "id": "deepseekmath-pushing-the-limits-of-mathematical-re-f7f589f8",
    "title": "DeepSeekMath: Pushing the Limits of Mathematical Reasoning in Open Language Models",
    "summary": "Mathematical reasoning poses a significant challenge for language models due to its complex and structured nature. In this paper, we introduce DeepSeekMath 7B, which continues pre-training DeepSeek-Coder-Base-v1.5 7B with 120B math-related tokens sourced from Common Crawl, together with natural language and code data. DeepSeekMath 7B has achieved an impressive score of 51.7% on the competition-level MATH benchmark without relying on external toolkits and voting techniques, approaching the performance level of Gemini-Ultra and GPT-4. Self-consistency over 64 samples from DeepSeekMath 7B achieves 60.9% on MATH. The mathematical reasoning capability of DeepSeekMath is attributed to two key factors: First, we harness the significant potential of publicly available web data through a meticulously engineered data selection pipeline. Second, we introduce Group Relative Policy Optimization (GRPO), a variant of Proximal Policy Optimization (PPO), that enhances mathematical reasoning abilities while concurrently optimizing the memory usage of PPO.",
    "link": "https://arxiv.org/abs/2402.03300"
  },
  {
    "id": "visual-autoregressive-modeling-scalable-image-gene-ae166102",
    "title": "Visual Autoregressive Modeling: Scalable Image Generation via Next-Scale Prediction",
    "summary": "We present Visual AutoRegressive modeling (VAR), a new generation paradigm that redefines the autoregressive learning on images as coarse-to-fine \"next-scale prediction\" or \"next-resolution prediction\", diverging from the standard raster-scan \"next-token prediction\". This simple, intuitive methodology allows autoregressive (AR) transformers to learn visual distributions fast and generalize well: VAR, for the first time, makes GPT-like AR models surpass diffusion transformers in image generation. On ImageNet 256x256 benchmark, VAR significantly improve AR baseline by improving Frechet inception distance (FID) from 18.65 to 1.73, inception score (IS) from 80.4 to 350.2, with around 20x faster inference speed. It is also empirically verified that VAR outperforms the Diffusion Transformer (DiT) in multiple dimensions including image quality, inference speed, data efficiency, and scalability. Scaling up VAR models exhibits clear power-law scaling laws similar to those observed in LLMs, with linear correlation coefficients near -0.998 as solid evidence. VAR further showcases zero-shot generalization ability in downstream tasks including image in-painting, out-painting, and editing. These results suggest VAR has initially emulated the two important properties of LLMs: Scaling Laws and zero-shot task generalization. We have released all models and codes to promote the exploration of AR/VAR models for visual generation and unified learning.",
    "link": "https://arxiv.org/abs/2404.02905"
  },
  {
    "id": "scaling-up-test-time-compute-with-latent-reasoning-dc7c80b7",
    "title": "Scaling up Test-Time Compute with Latent Reasoning: A Recurrent Depth Approach",
    "summary": "We study a novel language model architecture that is capable of scaling test-time computation by implicitly reasoning in latent space. Our model works by iterating a recurrent block, thereby unrolling to arbitrary depth at test-time. This stands in contrast to mainstream reasoning models that scale up compute by producing more tokens. Unlike approaches based on chain-of-thought, our approach does not require any specialized training data, can work with small context windows, and can capture types of reasoning that are not easily represented in words. We scale a proof-of-concept model to 3.5 billion parameters and 800 billion tokens. We show that the resulting model can improve its performance on reasoning benchmarks, sometimes dramatically, up to a computation load equivalent to 50 billion parameters.",
    "link": "https://arxiv.org/abs/2502.05171#:~:text=7%20Feb%202025%5D-,Scaling%20up%20Test%2DTime%20Compute%20with,Reasoning%3A%20A%20Recurrent%20Depth%20Approach&text=We%20study%20a%20novel%20language,arbitrary%20depth%20at%20test%2Dtime."
  },
  {
    "id": "large-language-diffusion-models-877ce46c",
    "title": "Large Language Diffusion Models",
    "summary": "The capabilities of large language models (LLMs) are widely regarded as relying on autoregressive models (ARMs). We challenge this notion by introducing LLaDA, a diffusion model trained from scratch under the pre-training and supervised fine-tuning (SFT) paradigm. LLaDA employs a forward data masking process and a reverse generation process, parameterized by a Transformer to predict masked tokens. It provides a principled generative approach for probabilistic inference by optimizing a likelihood lower bound. Across extensive benchmarks on general tasks, math, code, and so on, LLaDA demonstrates strong scalability and performs comparably to our self-constructed ARM baselines. Remarkably, LLaDA 8B is competitive with strong LLMs like LLaMA3 8B in in-context learning and, after SFT, exhibits impressive instruction-following abilities in case studies such as multi-turn dialogue. Moreover, LLaDA addresses the reversal curse, surpassing GPT-4o in a reversal poem completion task. Our findings show the promise of diffusion models for language modeling at scale and challenge the common assumption that core LLM capabilities discussed above inherently depend on ARMs. Project page and codes: https://ml-gsai.github.io/LLaDA-demo/.",
    "link": "https://arxiv.org/abs/2502.09992"
  },
  {
    "id": "grpo-judge-experiments-findings-empirical-observat-da28c889",
    "title": "GRPO Judge Experiments: Findings & Empirical Observations | kalomaze's kalomazing blog",
    "summary": "GRPO Judge Experiments: Findings & Empirical Observations 03 Mar, 2025 Introduction Many GRPO reproductions for LLM reinforcement learning available online lack useful intuitions or recommendations regarding hyperparameters and reward shaping. Most examples focus on GSM8k or similar math problems - tasks with clear right/wrong answers. Additionally, demonstrations seen so far primarily use models smaller than ~7B, and don't demonstrate realistic failure modes when it comes to the learning dynamics of smaller models. My goal was to tackle a more generalizable and difficult problem: improving generalized semantic judgements where the \"correct\" answer requires subjective evaluation , not just calculation. This is a more diverse task compared to math-based problem solving, and helps illustrate how GRPO develops problem-solving strategies in other domains. Task Description The task involves training a model to evaluate two pieces of text. One of the texts has been subtly augmented by a LLM (specifically, the larger 14B variant of my corruption models ). The model must provide notes and a subsequent judgment afterwards in consecutive XML tags. Example Format The base model is provided a system prompt that establishes the expected template, as well as two randomly ordered A/B samples containing \"real\" vs \"synthetic\" samples for the input: REQUEST: You are to judge the better of the two samples and determine which of the following samples is better using a short judgement that is no longer than (and no shorter than) exactly 128 tokens. Respond with an exactly 128 tokens tag labeled <notes> that contains your notes, and then <judgement> which is just the letter that you are picking. For example: JUDGE: <notes> Sample A is superior to Sample B... (example notes) </notes> <judgement>A</judgement> Now, it is your turn. [Sample A]: Included is a pre-test, post-test, and vocabulary quiz on the 8th grade math standard functions (8.F). 1.) Determine if a graph represents a function 2.) State the domain and range of a relation 3.) Plot points on a graph to determine if the table represents a function 4.) State if a function is decreasing, increasing, or constant 5.) Determine the output of a function machine 6.) Determine the recursive and explicit equation 7.) Determine the minimum, maximum, increasing interval, and decreasing interval of a graph 8.) Determine the rate of change, initial value, independent value, and dependent variable given a graph 9.) Sketch a graph given a situation The vocabulary included is dependent, output, function, domain, range, decreasing function, input, range, non-linear function, relation, increasing function, and function notation. Total Pages: 9 (18 including answer key) Answer Key: Included Document File: PDF [Sample B]: Included is a pre-test, post-test, and vocabulary quiz on the 8th grade math standard functions (8.F). 1.) Determine if a graph represents a function 2.) State the domain and range of a relation 3.) Plot points on a graph to determine if the table represents a function 4.) State if a function is increasing, decreasing, or constant 5.) Determine the output of a function given 6.) Determine the input of a function given 7.) Determine a function rule given ordered pairs or a table of values. 8.) Graph functions using a table of values and determine a trend line in a graph 9.) Write a data table situation The vocabulary included is dependent, output, function, domain, range, decreasing function, input, range, non-linear function, relation, increasing function, and function notation. Total Pages: 9 (18 including answer key) Answer Key: Included Document File: PDF JUDGE: A correct output follows this structure: <notes> Sample A provides more specific and thoroughly defined tasks. It mentions \"function machine,\" \"recursive and explicit equation,\" and detailed graph analysis with \"minimum, maximum\" and intervals. Sample B contains incomplete phrases like \"output of a function given\" without completing the thought, making it less coherent and precise than Sample A. </notes> <judgement>A</judgement> Resources Model checkpoint: quest-grpo-judge-14b-v1-205 Training code: Available in the Files tab at this link Dataset: quest-corruption-14brestorations-2.6k-filter-v1 Hyperparameter Observations Learning Rate Schedulers When hyperparameter tuning, I observed that cosine or linear decaying learning rates (particularly with fast warmup) do not work well with GRPO. What worked best was constant with a warm up phase with lower learning rates: lr_scheduler_type=\"constant_with_warmup\" Gradient Clipping For consistency, setting max_grad_norm to 0.2 as a default seems most effective (this is also the default clipping ratio in PPO, & I have done SFT runs in the past where 0.2 had the lowest loss). This value can be reduced if necessary for stability. Reward Construction Decisions Power Scaling the Judgement Reward For this task, there are two possible valid choices: A/B. Base models (Qwen2.5 series, mostly 7b/14b runs) begin with slightly worse than average reward; 14b was closer to 50/50 (equivalent to random chance). I wanted to try something different for reward scaling rather than naively using the percentage of accurate judgements. If we simply took the raw accuracy and used it as reward (e.g., 0.4 for 40% accuracy), I thought it might not create enough pressure for improvement. My idea was to apply a power function (specifically xâ´) to the reward scaling. Specifically, the approach scales down rewards for correct answers when the model's overall batch accuracy is low, while substantially increasing the reward as accuracy improves. The implementation I tried rescales individual correct judgments based on the overall batch performance. This means that the actual reward value assigned to each correct answer changes as the model improves on average, but I haven't rigorously tested if this scaling is actually better than alternatives. Example: At 50% accuracy, correct answers get 0.125 reward At 80% accuracy, correct answers get 0.512 reward XML Formatting Bonus Enforcing the XML schema (notes followed by a judgement) is something we want to incentivize. However, I wanted to avoid giving partial reward to responses with wrong judgements. What I ended up doing is enforcing the formatting reward as a flat 0.2 bonus that only applies when the judgement is also correct. My reasoning for this is because, if done this way, the \"chain of desirability\" (according to our reward specifications) looks something along the",
    "link": "https://kalomaze.bearblog.dev/grpo-judge-experiments-findings-and-empirical-observations/"
  },
  {
    "id": "why-does-grpo-work-kalomazes-kalomazing-blog-fd8c1173",
    "title": "Why does GRPO work? | kalomaze's kalomazing blog",
    "summary": "Why does GRPO work? 27 Feb, 2025 DeepSeek. Everyone in the LLM space knows that name now, and for good reason; after a little over a year of their team quietly iterating on architecture, engineering, & data work, they've finally caught up to [and in some aspects surpassed] the frontier with the release of DeepSeek r1, fresh off the heels of their latest pretraining run (DeepSeek-V3). When OpenAI released the first checkpoint that they felt confident enough in to dub a \"reasoning model\" (o1-preview), speculation went wild trying to figure out what training advancement was responsible for the performance lift that was being observed. Nobody in academia truly reproduced something at this level out in the open for several months. Not until r1 arrived. So how did they do it? Some individuals speculated that the advancement here came from integration of algorithms that have seen success in other contexts when it comes to ML, such as Monte Carlo tree search, or schemes that involved process reward modeling where each step is individually rewarded. It turns out all those galaxy brain schemes were beaten out by a clear answer; literally any reasonable variant of online reinforcement learning applied to verifiable objectives. While frontier labs at large are [presumably] still using PPO for online RL, DeepSeek developed what can be argued as strictly inferior from a pure learning perspective; but the reason why this doesn't matter is that the core principle of optimization essentially remains the same between the two methods. It turns out that custom RL objectives which explicitly reward the criteria that you are looking for are vastly more important for strong grounding in the task compared to just doing a finetune over the data you have and hoping for the best. But some elements of the PPO algorithm are more important than the others for the objectives we're aiming to optimize for. The crux of what actually matters that distinguishes PPO and GRPO is that: GRPO avoids the complexity associated with having a value model estimate the \"usefulness\" (advantage) of a sample [when it comes to optimizing for a goal] by weighing the samples according to how much they contribute to the reward. I won't get too much into the woods when it comes to the math here because that shit bores me to death, but the important thing about this setup is that since you are already taking multiple generations per training sample in a online RL setup , you can use the natural variability of these samples to estimate your advantage simply by comparing them to each other. That is to say, if generation 1 has a reward of 0.5, and generations 2, 3, and 4 are 0.1, you want to favor the probability of producing generation 1 more than the others, and by doing this in an relative fashion iteratively, you can naturally hill climb your desired metric by always favoring the samples that are proximally closer to improving your desired reward. What's most critical to observe here is how the advantage is calculated relative to performance of other attempts in the group. The advantage for each of the generations is calculated as such: advantage = (reward - mean) / stddev DPO Delenda Est, or: The Bitterer Lesson For a year or so before r1's release, open source was infatuated with the idea of \"DPO\" (Direct Preference Optimization), which eschews the \"online\" nature of reinforcement learning entirely by attempting to maximize the distance between \"chosen\" and \"rejected\" data given a pre-existing dataset. That is to say, instead of generating responses and iteratively improving them, DPO trains on an offline dataset and maximizes the probabilistic distance between chosen and rejected response pairs. I am of the opinion that this seems to have held back some pretty trivial experimentation that could have been done in the open source space to make online RL cheaper in the same ways that DeepSeek has demonstrated with GRPO as an algorithm, and to be blunt, I don't believe that DPO as an idea was ever going to work out in the long term. So, why doesn't DPO \"work\" (for the kinds of problems that GRPO is good at handling)? Well, if you think about it, it's essentially a proxy objective for another proxy objective ; that being, preference reward models, which are classifiers trained to predict the chance that a response is \"preferred\" compared to the chance that it is \"dispreferred\" (Bradley-Terry model). We cannot directly \"train\" on \"true\" human preferences, because what people like more or what is considered subjectively better on average is not typically contingent on verifiable or measurable factors; so we use the reward model as a proxy of determining what would be liked more based off of human annotated preference data. When you remove a model of preferences from the equation, you are essentially just maximizing the difference between what is already likely to happen vs what is less likely to happen; and as such, methods like DPO can never generalize from a base model, and will never learn the \"true distribution\" of what is preferred. As such, DPO is considered an \"offline\" version of RL; instead of learning to produce iteratively better outcomes \"on the fly\", the model gets prescribed pre-packaged predictions that might not even naturally align with the profile of what the model is predisposed towards producing to begin with. In fact, it can lead to performance degradation and catastrophic forgetting when the chosen/rejected data being used is simply too far away from what the model actually produces. To make matters worse, what the model \"puts out\" will always be shifting across the course of training, so even pre-collecting your chosen and rejected data according to a preference model or classifier will lead to \"stale\" representations of what the trained model would actually produce compared to if you trained the model in an online fashion. womp womp! (for a more rigorous analysis of DPO vs online RL methods like PPO, see Duan et al.",
    "link": "https://kalomaze.bearblog.dev/why-does-grpo-work/"
  },
  {
    "id": "back-to-basics-revisiting-reinforce-style-optimiza-925f1ca1",
    "title": "Back to Basics: Revisiting REINFORCE Style Optimization for Learning from Human Feedback in LLMs",
    "summary": "AI alignment in the shape of Reinforcement Learning from Human Feedback (RLHF) is increasingly treated as a crucial ingredient for high performance large language models. Proximal Policy Optimization (PPO) has been positioned by recent literature as the canonical method for the RL part of RLHF. However, it involves both high computational cost and sensitive hyperparameter tuning. We posit that most of the motivational principles that led to the development of PPO are less of a practical concern in RLHF and advocate for a less computationally expensive method that preserves and even increases performance. We revisit the formulation of alignment from human preferences in the context of RL. Keeping simplicity as a guiding principle, we show that many components of PPO are unnecessary in an RLHF context and that far simpler REINFORCE-style optimization variants outperform both PPO and newly proposed \"RL-free\" methods such as DPO and RAFT. Our work suggests that careful adaptation to LLMs alignment characteristics enables benefiting from online RL optimization at low cost.",
    "link": "https://arxiv.org/abs/2402.14740"
  },
  {
    "id": "dapo-an-open-source-llm-reinforcement-learning-sys-082075c5",
    "title": "DAPO: An Open-Source LLM Reinforcement Learning System at Scale",
    "summary": "Inference scaling empowers LLMs with unprecedented reasoning ability, with reinforcement learning as the core technique to elicit complex reasoning. However, key technical details of state-of-the-art reasoning LLMs are concealed (such as in OpenAI o1 blog and DeepSeek R1 technical report), thus the community still struggles to reproduce their RL training results. We propose the $\\textbf{D}$ecoupled Clip and $\\textbf{D}$ynamic s$\\textbf{A}$mpling $\\textbf{P}$olicy $\\textbf{O}$ptimization ($\\textbf{DAPO}$) algorithm, and fully open-source a state-of-the-art large-scale RL system that achieves 50 points on AIME 2024 using Qwen2.5-32B base model. Unlike previous works that withhold training details, we introduce four key techniques of our algorithm that make large-scale LLM RL a success. In addition, we open-source our training code, which is built on the verl framework, along with a carefully curated and processed dataset. These components of our open-source system enhance reproducibility and support future research in large-scale LLM RL.",
    "link": "https://arxiv.org/abs/2503.14476"
  },
  {
    "id": "understanding-r1-zero-like-training-a-critical-per-15f020bf",
    "title": "Understanding R1-Zero-Like Training: A Critical Perspective",
    "summary": "DeepSeek-R1-Zero has shown that reinforcement learning (RL) at scale can directly enhance the reasoning capabilities of LLMs without supervised fine-tuning. In this work, we critically examine R1-Zero-like training by analyzing its two core components: base models and RL. We investigate a wide range of base models, including DeepSeek-V3-Base, to understand how pretraining characteristics influence RL performance. Our analysis reveals that DeepSeek-V3-Base already exhibit ''Aha moment'', while Qwen2.5 base models demonstrate strong reasoning capabilities even without prompt templates, suggesting potential pretraining biases. Additionally, we identify an optimization bias in Group Relative Policy Optimization (GRPO), which artificially increases response length (especially for incorrect outputs) during training. To address this, we introduce Dr. GRPO, an unbiased optimization method that improves token efficiency while maintaining reasoning performance. Leveraging these insights, we present a minimalist R1-Zero recipe that achieves 43.3% accuracy on AIME 2024 with a 7B base model, establishing a new state-of-the-art. Our code is available at https://github.com/sail-sg/understand-r1-zero.",
    "link": "https://arxiv.org/abs/2503.20783"
  },
  {
    "id": "qwen-vl-a-versatile-vision-language-model-for-unde-f62b19ae",
    "title": "Qwen-VL: A Versatile Vision-Language Model for Understanding, Localization, Text Reading, and Beyond",
    "summary": "In this work, we introduce the Qwen-VL series, a set of large-scale vision-language models (LVLMs) designed to perceive and understand both texts and images. Starting from the Qwen-LM as a foundation, we endow it with visual capacity by the meticulously designed (i) visual receptor, (ii) input-output interface, (iii) 3-stage training pipeline, and (iv) multilingual multimodal cleaned corpus. Beyond the conventional image description and question-answering, we implement the grounding and text-reading ability of Qwen-VLs by aligning image-caption-box tuples. The resulting models, including Qwen-VL and Qwen-VL-Chat, set new records for generalist models under similar model scales on a broad range of visual-centric benchmarks (e.g., image captioning, question answering, visual grounding) and different settings (e.g., zero-shot, few-shot). Moreover, on real-world dialog benchmarks, our instruction-tuned Qwen-VL-Chat also demonstrates superiority compared to existing vision-language chatbots. Code, demo and models are available at https://github.com/QwenLM/Qwen-VL.",
    "link": "https://arxiv.org/abs/2308.12966"
  },
  {
    "id": "qwen2-vl-enhancing-vision-language-models-percepti-9abe7f12",
    "title": "Qwen2-VL: Enhancing Vision-Language Model's Perception of the World at Any Resolution",
    "summary": "We present the Qwen2-VL Series, an advanced upgrade of the previous Qwen-VL models that redefines the conventional predetermined-resolution approach in visual processing. Qwen2-VL introduces the Naive Dynamic Resolution mechanism, which enables the model to dynamically process images of varying resolutions into different numbers of visual tokens. This approach allows the model to generate more efficient and accurate visual representations, closely aligning with human perceptual processes. The model also integrates Multimodal Rotary Position Embedding (M-RoPE), facilitating the effective fusion of positional information across text, images, and videos. We employ a unified paradigm for processing both images and videos, enhancing the model's visual perception capabilities. To explore the potential of large multimodal models, Qwen2-VL investigates the scaling laws for large vision-language models (LVLMs). By scaling both the model size-with versions at 2B, 8B, and 72B parameters-and the amount of training data, the Qwen2-VL Series achieves highly competitive performance. Notably, the Qwen2-VL-72B model achieves results comparable to leading models such as GPT-4o and Claude3.5-Sonnet across various multimodal benchmarks, outperforming other generalist models. Code is available at https://github.com/QwenLM/Qwen2-VL .",
    "link": "https://arxiv.org/abs/2409.12191"
  },
  {
    "id": "vision-r1-incentivizing-reasoning-capability-in-mu-2e78830c",
    "title": "Vision-R1: Incentivizing Reasoning Capability in Multimodal Large Language Models",
    "summary": "DeepSeek-R1-Zero has successfully demonstrated the emergence of reasoning capabilities in LLMs purely through Reinforcement Learning (RL). Inspired by this breakthrough, we explore how RL can be utilized to enhance the reasoning capability of MLLMs. However, direct training with RL struggles to activate complex reasoning capabilities such as questioning and reflection in MLLMs, due to the absence of substantial high-quality multimodal reasoning data. To address this issue, we propose the reasoning MLLM, Vision-R1, to improve multimodal reasoning capability. Specifically, we first construct a high-quality multimodal CoT dataset without human annotations by leveraging an existing MLLM and DeepSeek-R1 through modality bridging and data filtering to obtain a 200K multimodal CoT dataset, Vision-R1-cold dataset. It serves as cold-start initialization data for Vision-R1. To mitigate the optimization challenges caused by overthinking after cold start, we propose Progressive Thinking Suppression Training (PTST) strategy and employ Group Relative Policy Optimization (GRPO) with the hard formatting result reward function to gradually refine the model's ability to learn correct and complex reasoning processes on a 10K multimodal math dataset. Comprehensive experiments show our model achieves an average improvement of $\\sim$6% across various multimodal math reasoning benchmarks. Vision-R1-7B achieves a 73.5% accuracy on the widely used MathVista benchmark, which is only 0.4% lower than the leading reasoning model, OpenAI O1. The datasets and code will be released in: https://github.com/Osilly/Vision-R1 .",
    "link": "https://arxiv.org/abs/2503.06749"
  },
  {
    "id": "proof-or-bluff-evaluating-llms-on-2025-usa-math-ol-d2aeec1e",
    "title": "Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad",
    "summary": "Recent math benchmarks for large language models (LLMs) such as MathArena indicate that state-of-the-art reasoning models achieve impressive performance on mathematical competitions like AIME, with the leading model, Gemini-2.5-Pro, achieving scores comparable to top human competitors. However, these benchmarks evaluate models solely based on final numerical answers, neglecting rigorous reasoning and proof generation which are essential for real-world mathematical tasks. To address this, we introduce a comprehensive evaluation of full-solution reasoning for challenging mathematical problems. Using expert human annotators, we evaluated several state-of-the-art reasoning models on the six problems from the 2025 USAMO within hours of their release. Our results reveal that all tested models struggled significantly: only Gemini-2.5-Pro achieves a non-trivial score of 25%, while all other models achieve less than 5%. Through detailed analysis of reasoning traces, we identify the most common failure modes and find several unwanted artifacts arising from the optimization strategies employed during model training. Overall, our results suggest that current LLMs are inadequate for rigorous mathematical reasoning tasks, highlighting the need for substantial improvements in reasoning and proof generation capabilities.",
    "link": "https://arxiv.org/abs/2503.21934"
  },
  {
    "id": "crossing-the-reward-bridge-expanding-rl-with-verif-f3344da0",
    "title": "Crossing the Reward Bridge: Expanding RL with Verifiable Rewards Across Diverse Domains",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has demonstrated significant success in enhancing mathematical reasoning and coding performance of large language models (LLMs), especially when structured reference answers are accessible for verification. However, its extension to broader, less structured domains remains unexplored. In this work, we investigate the effectiveness and scalability of RLVR across diverse real-world domains including medicine, chemistry, psychology, economics, and education, where structured reference answers are typically unavailable. We reveal that binary verification judgments on broad-domain tasks exhibit high consistency across various LLMs provided expert-written reference answers exist. Motivated by this finding, we utilize a generative scoring technique that yields soft, model-based reward signals to overcome limitations posed by binary verifications, especially in free-form, unstructured answer scenarios. We further demonstrate the feasibility of training cross-domain generative reward models using relatively small (7B) LLMs without the need for extensive domain-specific annotation. Through comprehensive experiments, our RLVR framework establishes clear performance gains, significantly outperforming state-of-the-art open-source aligned models such as Qwen2.5-72B and DeepSeek-R1-Distill-Qwen-32B across domains in free-form settings. Our approach notably enhances the robustness, flexibility, and scalability of RLVR, representing a substantial step towards practical reinforcement learning applications in complex, noisy-label scenarios.",
    "link": "https://arxiv.org/abs/2503.23829"
  },
  {
    "id": "improved-visual-spatial-reasoning-via-r1-zero-like-36fa4d36",
    "title": "Improved Visual-Spatial Reasoning via R1-Zero-Like Training",
    "summary": "Increasing attention has been placed on improving the reasoning capacities of multi-modal large language models (MLLMs). As the cornerstone for AI agents that function in the physical realm, video-based visual-spatial intelligence (VSI) emerges as one of the most pivotal reasoning capabilities of MLLMs. This work conducts a first, in-depth study on improving the visual-spatial reasoning of MLLMs via R1-Zero-like training. Technically, we first identify that the visual-spatial reasoning capacities of small- to medium-sized Qwen2-VL models cannot be activated via Chain of Thought (CoT) prompts. We then incorporate GRPO training for improved visual-spatial reasoning, using the carefully curated VSI-100k dataset, following DeepSeek-R1-Zero. During the investigation, we identify the necessity to keep the KL penalty (even with a small value) in GRPO. With just 120 GPU hours, our vsGRPO-2B model, fine-tuned from Qwen2-VL-2B, can outperform the base model by 12.1% and surpass GPT-4o. Moreover, our vsGRPO-7B model, fine-tuned from Qwen2-VL-7B, achieves performance comparable to that of the best open-source model LLaVA-NeXT-Video-72B. Additionally, we compare vsGRPO to supervised fine-tuning and direct preference optimization baselines and observe strong performance superiority. The code and dataset will be available soon.",
    "link": "https://arxiv.org/abs/2504.00883"
  },
  {
    "id": "learning-to-reason-for-long-form-story-generation-45cd59ce",
    "title": "Learning to Reason for Long-Form Story Generation",
    "summary": "Generating high-quality stories spanning thousands of tokens requires competency across a variety of skills, from tracking plot and character arcs to keeping a consistent and engaging style. Due to the difficulty of sourcing labeled datasets and precise quality measurements, most work using large language models (LLMs) for long-form story generation uses combinations of hand-designed prompting techniques to elicit author-like behavior. This is a manual process that is highly dependent on the specific story-generation task. Motivated by the recent success of applying RL with Verifiable Rewards to domains like math and coding, we propose a general story-generation task (Next-Chapter Prediction) and a reward formulation (Verified Rewards via Completion Likelihood Improvement) that allows us to use an unlabeled book dataset as a learning signal for reasoning. We learn to reason over a story's condensed information and generate a detailed plan for the next chapter. Our reasoning is evaluated via the chapters it helps a story-generator create, and compared against non-trained and supervised finetuning (SFT) baselines. Pairwise human judgments reveal the chapters our learned reasoning produces are preferred across almost all metrics, and the effect is more pronounced in Scifi and Fantasy genres.",
    "link": "https://arxiv.org/abs/2503.22828"
  },
  {
    "id": "paligemma-2-a-family-of-versatile-vlms-for-transfe-3cafc093",
    "title": "PaliGemma 2: A Family of Versatile VLMs for Transfer",
    "summary": "PaliGemma 2 is an upgrade of the PaliGemma open Vision-Language Model (VLM) based on the Gemma 2 family of language models. We combine the SigLIP-So400m vision encoder that was also used by PaliGemma with the whole range of Gemma 2 models, from the 2B one all the way up to the 27B model. We train these models at three resolutions (224px, 448px, and 896px) in multiple stages to equip them with broad knowledge for transfer via fine-tuning. The resulting family of base models covering different model sizes and resolutions allows us to investigate factors impacting transfer performance (such as learning rate) and to analyze the interplay between the type of task, model size, and resolution. We further increase the number and breadth of transfer tasks beyond the scope of PaliGemma including different OCR-related tasks such as table structure recognition, molecular structure recognition, music score recognition, as well as long fine-grained captioning and radiography report generation, on which PaliGemma 2 obtains state-of-the-art results.",
    "link": "https://arxiv.org/abs/2412.03555"
  },
  {
    "id": "paligemma-a-versatile-3b-vlm-for-transfer-b377e684",
    "title": "PaliGemma: A versatile 3B VLM for transfer",
    "summary": "PaliGemma is an open Vision-Language Model (VLM) that is based on the SigLIP-So400m vision encoder and the Gemma-2B language model. It is trained to be a versatile and broadly knowledgeable base model that is effective to transfer. It achieves strong performance on a wide variety of open-world tasks. We evaluate PaliGemma on almost 40 diverse tasks including standard VLM benchmarks, but also more specialized tasks such as remote-sensing and segmentation.",
    "link": "https://arxiv.org/abs/2407.07726"
  },
  {
    "id": "scaling-laws-for-native-multimodal-models-7c2440ae",
    "title": "Scaling Laws for Native Multimodal Models",
    "summary": "Building general-purpose models that can effectively perceive the world through multimodal signals has been a long-standing goal. Current approaches involve integrating separately pre-trained components, such as connecting vision encoders to LLMs and continuing multimodal training. While such approaches exhibit remarkable sample efficiency, it remains an open question whether such late-fusion architectures are inherently superior. In this work, we revisit the architectural design of native multimodal models (NMMs)-those trained from the ground up on all modalities-and conduct an extensive scaling laws study, spanning 457 trained models with different architectures and training mixtures. Our investigation reveals no inherent advantage to late-fusion architectures over early-fusion ones, which do not rely on image encoders or tokenizers. On the contrary, early-fusion exhibits stronger performance at lower parameter counts, is more efficient to train, and is easier to deploy. Motivated by the strong performance of the early-fusion architectures, we show that incorporating Mixture of Experts (MoEs) allows models to learn modality-specific weights, significantly benefiting performance.",
    "link": "https://arxiv.org/abs/2504.07951"
  },
  {
    "id": "does-reinforcement-learning-really-incentivize-rea-37252bf0",
    "title": "Does Reinforcement Learning Really Incentivize Reasoning Capacity in LLMs Beyond the Base Model?",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has recently demonstrated notable success in enhancing the reasoning performance of large language models (LLMs), particularly on mathematics and programming tasks. Similar to how traditional RL helps agents explore and learn new strategies, RLVR is believed to enable LLMs to continuously self-improve, thus acquiring novel reasoning abilities beyond those of the corresponding base models. In this study we critically examine the current state of RLVR by systematically probing the reasoning capability boundaries of RLVR-trained LLMs across various model families, RL algorithms, and math, coding, and visual reasoning benchmarks, using pass@k at large k values as the evaluation metric. Surprisingly, we find that the current training setup does not elicit fundamentally new reasoning patterns. While RLVR-trained models outperform their base models at small k (e.g., k = 1), the base models achieve a higher pass@k score when k is large. Coverage and perplexity analyses show that the observed reasoning abilities originate from and are bounded by the base model. Treating the base model as an upper bound, our quantitative analysis shows that six popular RLVR algorithms perform similarly and remain far from optimal in leveraging the potential of the base model. By contrast, we find that distillation can introduce new reasoning patterns from the teacher and genuinely expand the model's reasoning capabilities. Overall, our findings suggest that current RLVR methods have not yet realized the potential of RL to elicit truly novel reasoning abilities in LLMs. This highlights the need for improved RL paradigms, such as continual scaling and multi-turn agent-environment interaction, to unlock this potential.",
    "link": "https://arxiv.org/abs/2504.13837"
  },
  {
    "id": "concise-reasoning-via-reinforcement-learning-6ed2ac96",
    "title": "Concise Reasoning via Reinforcement Learning",
    "summary": "A major drawback of reasoning models is their excessive token usage, inflating computational cost, resource demand, and latency. We show this verbosity stems not from deeper reasoning but from reinforcement learning loss minimization when models produce incorrect answers. With unsolvable problems dominating training, this effect compounds into a systematic tendency toward longer outputs. Through theoretical analysis of PPO and GRPO, we prove that incorrect answers inherently drive policies toward verbosity \\textit{even when} $Î³=1$, reframing response lengthening as an optimization artifact. We further uncover a consistent correlation between conciseness and correctness across reasoning and non-reasoning models. Building on these insights, we propose a two-phase RL procedure where a brief secondary stage, trained on a small set of solvable problems, significantly reduces response length while preserving or improving accuracy. Finally, we show that while GRPO shares properties with PPO, it exhibits collapse modes, limiting its reliability for concise reasoning. Our claims are supported by extensive experiments.",
    "link": "https://arxiv.org/abs/2504.05185"
  },
  {
    "id": "internvl3-exploring-advanced-training-and-test-tim-a497f1d5",
    "title": "InternVL3: Exploring Advanced Training and Test-Time Recipes for Open-Source Multimodal Models",
    "summary": "We introduce InternVL3, a significant advancement in the InternVL series featuring a native multimodal pre-training paradigm. Rather than adapting a text-only large language model (LLM) into a multimodal large language model (MLLM) that supports visual inputs, InternVL3 jointly acquires multimodal and linguistic capabilities from both diverse multimodal data and pure-text corpora during a single pre-training stage. This unified training paradigm effectively addresses the complexities and alignment challenges commonly encountered in conventional post-hoc training pipelines for MLLMs. To further improve performance and scalability, InternVL3 incorporates variable visual position encoding (V2PE) to support extended multimodal contexts, employs advanced post-training techniques such as supervised fine-tuning (SFT) and mixed preference optimization (MPO), and adopts test-time scaling strategies alongside an optimized training infrastructure. Extensive empirical evaluations demonstrate that InternVL3 delivers superior performance across a wide range of multi-modal tasks. In particular, InternVL3-78B achieves a score of 72.2 on the MMMU benchmark, setting a new state-of-the-art among open-source MLLMs. Its capabilities remain highly competitive with leading proprietary models, including ChatGPT-4o, Claude 3.5 Sonnet, and Gemini 2.5 Pro, while also maintaining strong pure-language proficiency. In pursuit of open-science principles, we will publicly release both the training data and model weights to foster further research and development in next-generation MLLMs.",
    "link": "https://arxiv.org/abs/2504.10479"
  },
  {
    "id": "do-llamas-work-in-english-on-the-latent-language-o-30fb3227",
    "title": "Do Llamas Work in English? On the Latent Language of Multilingual Transformers",
    "summary": "We ask whether multilingual language models trained on unbalanced, English-dominated corpora use English as an internal pivot language -- a question of key importance for understanding how language models function and the origins of linguistic bias. Focusing on the Llama-2 family of transformer models, our study uses carefully constructed non-English prompts with a unique correct single-token continuation. From layer to layer, transformers gradually map an input embedding of the final prompt token to an output embedding from which next-token probabilities are computed. Tracking intermediate embeddings through their high-dimensional space reveals three distinct phases, whereby intermediate embeddings (1) start far away from output token embeddings; (2) already allow for decoding a semantically correct next token in the middle layers, but give higher probability to its version in English than in the input language; (3) finally move into an input-language-specific region of the embedding space. We cast these results into a conceptual model where the three phases operate in \"input space\", \"concept space\", and \"output space\", respectively. Crucially, our evidence suggests that the abstract \"concept space\" lies closer to English than to other languages, which may have important consequences regarding the biases held by multilingual language models.",
    "link": "https://arxiv.org/abs/2402.10588"
  },
  {
    "id": "funsearch-making-new-discoveries-in-mathematical-s-eed2fe09",
    "title": "FunSearch: Making new discoveries in mathematical sciences using Large Language Models - Google DeepMind",
    "summary": "December 14, 2023 Science FunSearch: Making new discoveries in mathematical sciences using Large Language Models Alhussein Fawzi and Bernardino Romera Paredes ios_share Share By searching for â€œfunctionsâ€ written in computer code, FunSearch made the first discoveries in open problems in mathematical sciences using LLMs Update: In December 2024, we published a report on arXiv showing how our method can be used to amplify human performance in combinatorial competitive programming. Large Language Models (LLMs) are useful assistants - they excel at combining concepts and can read, write and code to help people solve problems. But could they discover entirely new knowledge? As LLMs have been shown to â€œhallucinateâ€ factually incorrect information, using them to make verifiably correct discoveries is a challenge. But what if we could harness the creativity of LLMs by identifying and building upon only their very best ideas? Today, in a paper published in Nature , we introduce FunSearch, a method to search for new solutions in mathematics and computer science. FunSearch works by pairing a pre-trained LLM, whose goal is to provide creative solutions in the form of computer code, with an automated â€œevaluatorâ€, which guards against hallucinations and incorrect ideas. By iterating back-and-forth between these two components, initial solutions â€œevolveâ€ into new knowledge. The system searches for â€œfunctionsâ€ written in computer code; hence the name FunSearch. This work represents the first time a new discovery has been made for challenging open problems in science or mathematics using LLMs. FunSearch discovered new solutions for the cap set problem, a longstanding open problem in mathematics. In addition, to demonstrate the practical usefulness of FunSearch, we used it to discover more effective algorithms for the â€œbin-packingâ€ problem, which has ubiquitous applications such as making data centers more efficient. Scientific progress has always relied on the ability to share new understanding. What makes FunSearch a particularly powerful scientific tool is that it outputs programs that reveal how its solutions are constructed, rather than just what the solutions are. We hope this can inspire further insights in the scientists who use FunSearch, driving a virtuous cycle of improvement and discovery. Driving discovery through evolution with language models FunSearch uses an evolutionary method powered by LLMs, which promotes and develops the highest scoring ideas. These ideas are expressed as computer programs, so that they can be run and evaluated automatically. First, the user writes a description of the problem in the form of code. This description comprises a procedure to evaluate programs, and a seed program used to initialize a pool of programs. FunSearch is an iterative procedure; at each iteration, the system selects some programs from the current pool of programs, which are fed to an LLM. The LLM creatively builds upon these, and generates new programs, which are automatically evaluated. The best ones are added back to the pool of existing programs, creating a self-improving loop. FunSearch uses Googleâ€™s PaLM 2 , but it is compatible with other LLMs trained on code. The FunSearch process. The LLM is shown a selection of the best programs it has generated so far (retrieved from the programs database), and asked to generate an even better one. The programs proposed by the LLM are automatically executed, and evaluated. The best programs are added to the database, for selection in subsequent cycles. The user can at any point retrieve the highest-scoring programs discovered so far. Discovering new mathematical knowledge and algorithms in different domains is a notoriously difficult task, and largely beyond the power of the most advanced AI systems. To tackle such challenging problems with FunSearch, we introduced multiple key components. Instead of starting from scratch, we start the evolutionary process with common knowledge about the problem, and let FunSearch focus on finding the most critical ideas to achieve new discoveries. In addition, our evolutionary process uses a strategy to improve the diversity of ideas in order to avoid stagnation. Finally, we run the evolutionary process in parallel to improve the system efficiency. Breaking new ground in mathematics We first address the cap set problem , an open challenge, which has vexed mathematicians in multiple research areas for decades. Renowned mathematician Terence Tao once described it as his favorite open question . We collaborated with Jordan Ellenberg, a professor of mathematics at the University of Wisconsinâ€“Madison, and author of an important breakthrough on the cap set problem . The problem consists of finding the largest set of points (called a cap set) in a high-dimensional grid, where no three points lie on a line. This problem is important because it serves as a model for other problems in extremal combinatorics - the study of how large or small a collection of numbers, graphs or other objects could be. Brute-force computing approaches to this problem donâ€™t work â€“ the number of possibilities to consider quickly becomes greater than the number of atoms in the universe. FunSearch generated solutions - in the form of programs - that in some settings discovered the largest cap sets ever found. This represents the largest increase in the size of cap sets in the past 20 years. Moreover, FunSearch outperformed state-of-the-art computational solvers, as this problem scales well beyond their current capabilities. These results demonstrate that the FunSearch technique can take us beyond established results on hard combinatorial problems, where intuition can be difficult to build. We expect this approach to play a role in new discoveries for similar theoretical problems in combinatorics, and in the future it may open up new possibilities in fields such as communication theory. FunSearch favors concise and human-interpretable programs While discovering new mathematical knowledge is significant in itself, the FunSearch approach offers an additional benefit over traditional computer search techniques. Thatâ€™s because FunSearch isnâ€™t a black box that merely generates solutions to problems. Instead, it generates programs that describe how those solutions were arrived at. This show-your-working approach is how scientists generally operate, with new discoveries or phenomena explained through the process used to produce them. FunSearch favors finding solutions represented by highly compact",
    "link": "https://deepmind.google/discover/blog/funsearch-making-new-discoveries-in-mathematical-sciences-using-large-language-models/"
  },
  {
    "id": "alphaevolve-a-gemini-powered-coding-agent-for-desi-b22a27ab",
    "title": "AlphaEvolve: A Gemini-powered coding agent for designing advanced algorithms - Google DeepMind",
    "summary": "May 14, 2025 Science AlphaEvolve: A Gemini-powered coding agent for designing advanced algorithms AlphaEvolve team ios_share Share New AI agent evolves algorithms for math and practical applications in computing by combining the creativity of large language models with automated evaluators Large language models (LLMs) are remarkably versatile. They can summarize documents, generate code or even brainstorm new ideas. And now weâ€™ve expanded these capabilities to target fundamental and highly complex problems in mathematics and modern computing. Today, weâ€™re announcing AlphaEvolve , an evolutionary coding agent powered by large language models for general-purpose algorithm discovery and optimization. AlphaEvolve pairs the creative problem-solving capabilities of our Gemini models with automated evaluators that verify answers, and uses an evolutionary framework to improve upon the most promising ideas. AlphaEvolve enhanced the efficiency of Google's data centers, chip design and AI training processes â€” including training the large language models underlying AlphaEvolve itself. It has also helped design faster matrix multiplication algorithms and find new solutions to open mathematical problems, showing incredible promise for application across many areas. Designing better algorithms with large language models In 2023, we showed for the first time that large language models can generate functions written in computer code to help discover new and provably correct knowledge on an open scientific problem. AlphaEvolve is an agent that can go beyond single function discovery to evolve entire codebases and develop much more complex algorithms. AlphaEvolve leverages an ensemble of state-of-the-art large language models: our fastest and most efficient model, Gemini Flash , maximizes the breadth of ideas explored, while our most powerful model, Gemini Pro , provides critical depth with insightful suggestions. Together, these models propose computer programs that implement algorithmic solutions as code. Diagram showing how the prompt sampler first assembles a prompt for the language models, which then generate new programs. These programs are evaluated by evaluators and stored in the programs database. This database implements an evolutionary algorithm that determines which programs will be used for future prompts. AlphaEvolve verifies, runs and scores the proposed programs using automated evaluation metrics. These metrics provide an objective, quantifiable assessment of each solutionâ€™s accuracy and quality. This makes AlphaEvolve particularly helpful in a broad range of domains where progress can be clearly and systematically measured, like in math and computer science. Optimizing our computing ecosystem Over the past year, weâ€™ve deployed algorithms discovered by AlphaEvolve across Googleâ€™s computing ecosystem, including our data centers, hardware and software. The impact of each of these improvements is multiplied across our AI and computing infrastructure to build a more powerful and sustainable digital ecosystem for all our users. Diagram showing how AlphaEvolve helps Google deliver a more efficient digital ecosystem, from data center scheduling and hardware design to AI model training. Improving data center scheduling AlphaEvolve discovered a simple yet remarkably effective heuristic to help Borg orchestrate Google's vast data centers more efficiently. This solution, now in production for over a year, continuously recovers, on average, 0.7% of Googleâ€™s worldwide compute resources. This sustained efficiency gain means that at any given moment, more tasks can be completed on the same computational footprint. AlphaEvolve's solution not only leads to strong performance but also offers significant operational advantages of human-readable code: interpretability, debuggability, predictability and ease of deployment. Assisting in hardware design AlphaEvolve proposed a Verilog rewrite that removed unnecessary bits in a key, highly optimized arithmetic circuit for matrix multiplication. Crucially, the proposal must pass robust verification methods to confirm that the modified circuit maintains functional correctness. This proposal was integrated into an upcoming Tensor Processing Unit (TPU), Googleâ€™s custom AI accelerator. By suggesting modifications in the standard language of chip designers, AlphaEvolve promotes a collaborative approach between AI and hardware engineers to accelerate the design of future specialized chips. Enhancing AI training and inference AlphaEvolve is accelerating AI performance and research velocity. By finding smarter ways to divide a large matrix multiplication operation into more manageable subproblems, it sped up this vital kernel in Geminiâ€™s architecture by 23%, leading to a 1% reduction in Gemini's training time. Because developing generative AI models requires substantial computing resources, every efficiency gained translates to considerable savings. Beyond performance gains, AlphaEvolve significantly reduces the engineering time required for kernel optimization, from weeks of expert effort to days of automated experiments, allowing researchers to innovate faster. AlphaEvolve can also optimize low level GPU instructions. This incredibly complex domain is usually already heavily optimized by compilers, so human engineers typically don't modify it directly. AlphaEvolve achieved up to a 32.5% speedup for the FlashAttention kernel implementation in Transformer -based AI models. This kind of optimization helps experts pinpoint performance bottlenecks and easily incorporate the improvements into their codebase, boosting their productivity and enabling future savings in compute and energy. Advancing the frontiers in mathematics and algorithm discovery AlphaEvolve can also propose new approaches to complex mathematical problems. Provided with a minimal code skeleton for a computer program, AlphaEvolve designed many components of a novel gradient-based optimization procedure that discovered multiple new algorithms for matrix multiplication, a fundamental problem in computer science. Your browser does not support the video tag. pause A list of changes proposed by AlphaEvolve to discover faster matrix multiplication algorithms. In this example, AlphaEvolve proposes extensive changes across several components, including the optimizer and weight initialization, the loss function, and hyperparameter sweep. These changes are highly non-trivial, requiring 15 mutations during the evolutionary process. AlphaEvolveâ€™s procedure found an algorithm to multiply 4x4 complex-valued matrices using 48 scalar multiplications, improving upon Strassenâ€™s 1969 algorithm that was previously known as the best in this setting. This finding demonstrates a significant advance over our previous work, AlphaTensor , which specialized in matrix multiplication algorithms, and for 4x4 matrices, only found improvements for binary arithmetic. To investigate AlphaEvolveâ€™s breadth, we applied the system to over 50 open problems in mathematical analysis, geometry, combinatorics and number theory. The systemâ€™s flexibility enabled us to set up most experiments in a matter of hours. In roughly 75% of cases, it rediscovered state-of-the-art solutions, to the",
    "link": "https://deepmind.google/discover/blog/alphaevolve-a-gemini-powered-coding-agent-for-designing-advanced-algorithms/"
  },
  {
    "id": "absolute-zero-reinforced-self-play-reasoning-with--703c3bc5",
    "title": "Absolute Zero: Reinforced Self-play Reasoning with Zero Data",
    "summary": "Reinforcement learning with verifiable rewards (RLVR) has shown promise in enhancing the reasoning capabilities of large language models by learning directly from outcome-based rewards. Recent RLVR works that operate under the zero setting avoid supervision in labeling the reasoning process, but still depend on manually curated collections of questions and answers for training. The scarcity of high-quality, human-produced examples raises concerns about the long-term scalability of relying on human supervision, a challenge already evident in the domain of language model pretraining. Furthermore, in a hypothetical future where AI surpasses human intelligence, tasks provided by humans may offer limited learning potential for a superintelligent system. To address these concerns, we propose a new RLVR paradigm called Absolute Zero, in which a single model learns to propose tasks that maximize its own learning progress and improves reasoning by solving them, without relying on any external data. Under this paradigm, we introduce the Absolute Zero Reasoner (AZR), a system that self-evolves its training curriculum and reasoning ability by using a code executor to both validate proposed code reasoning tasks and verify answers, serving as an unified source of verifiable reward to guide open-ended yet grounded learning. Despite being trained entirely without external data, AZR achieves overall SOTA performance on coding and mathematical reasoning tasks, outperforming existing zero-setting models that rely on tens of thousands of in-domain human-curated examples. Furthermore, we demonstrate that AZR can be effectively applied across different model scales and is compatible with various model classes.",
    "link": "https://arxiv.org/abs/2505.03335"
  },
  {
    "id": "reinforcement-learning-for-reasoning-in-large-lang-4c7ed2f1",
    "title": "Reinforcement Learning for Reasoning in Large Language Models with One Training Example",
    "summary": "We show that reinforcement learning with verifiable reward using one training example (1-shot RLVR) is effective in incentivizing the math reasoning capabilities of large language models (LLMs). Applying RLVR to the base model Qwen2.5-Math-1.5B, we identify a single example that elevates model performance on MATH500 from 36.0% to 73.6% (8.6% improvement beyond format correction), and improves the average performance across six common mathematical reasoning benchmarks from 17.6% to 35.7% (7.0% non-format gain). This result matches the performance obtained using the 1.2k DeepScaleR subset (MATH500: 73.6%, average: 35.9%), which contains the aforementioned example. Furthermore, RLVR with only two examples even slightly exceeds these results (MATH500: 74.8%, average: 36.6%). Similar substantial improvements are observed across various models (Qwen2.5-Math-7B, Llama3.2-3B-Instruct, DeepSeek-R1-Distill-Qwen-1.5B), RL algorithms (GRPO and PPO), and different math examples. In addition, we identify some interesting phenomena during 1-shot RLVR, including cross-category generalization, increased frequency of self-reflection, and sustained test performance improvement even after the training accuracy has saturated, a phenomenon we term post-saturation generalization. Moreover, we verify that the effectiveness of 1-shot RLVR primarily arises from the policy gradient loss, distinguishing it from the \"grokking\" phenomenon. We also show the critical role of promoting exploration (e.g., by incorporating entropy loss with an appropriate coefficient) in 1-shot RLVR training. We also further discuss related observations about format correction, label robustness and prompt modification. These findings can inspire future work on RLVR efficiency and encourage a re-examination of recent progress and the underlying mechanisms in RLVR. All resources are open source at https://github.com/ypwang61/One-Shot-RLVR.",
    "link": "https://arxiv.org/abs/2504.20571"
  },
  {
    "id": "grit-teaching-mllms-to-think-with-images-fd0f12e4",
    "title": "GRIT: Teaching MLLMs to Think with Images",
    "summary": "Recent studies have demonstrated the efficacy of using Reinforcement Learning (RL) in building reasoning models that articulate chains of thoughts prior to producing final answers. However, despite ongoing advances that aim at enabling reasoning for vision-language tasks, existing open-source visual reasoning models typically generate reasoning content with pure natural language, lacking explicit integration of visual information. This limits their ability to produce clearly articulated and visually grounded reasoning chains. To this end, we propose Grounded Reasoning with Images and Texts (GRIT), a novel method for training MLLMs to think with images. GRIT introduces a grounded reasoning paradigm, in which models generate reasoning chains that interleave natural language and explicit bounding box coordinates. These coordinates point to regions of the input image that the model consults during its reasoning process. Additionally, GRIT is equipped with a reinforcement learning approach, GRPO-GR, built upon the GRPO algorithm. GRPO-GR employs robust rewards focused on the final answer accuracy and format of the grounded reasoning output, which eliminates the need for data with reasoning chain annotations or explicit bounding box labels. As a result, GRIT achieves exceptional data efficiency, requiring as few as 20 image-question-answer triplets from existing datasets. Comprehensive evaluations demonstrate that GRIT effectively trains MLLMs to produce coherent and visually grounded reasoning chains, showing a successful unification of reasoning and grounding abilities.",
    "link": "https://arxiv.org/abs/2505.15879"
  },
  {
    "id": "visual-planning-lets-think-only-with-images-77a2fcda",
    "title": "Visual Planning: Let's Think Only with Images",
    "summary": "Recent advancements in Large Language Models (LLMs) and their multimodal extensions (MLLMs) have substantially enhanced machine reasoning across diverse tasks. However, these models predominantly rely on pure text as the medium for both expressing and structuring reasoning, even when visual information is present. In this work, we argue that language may not always be the most natural or effective modality for reasoning, particularly in tasks involving spatial and geometrical information. Motivated by this, we propose a new paradigm, Visual Planning, which enables planning through purely visual representations for these \"vision-first\" tasks, as a supplementary channel to language-based reasoning. In this paradigm, planning is executed via sequences of images that encode step-by-step inference in the visual domain, akin to how humans sketch or visualize future actions. We introduce a novel reinforcement learning framework, Visual Planning via Reinforcement Learning (VPRL), empowered by GRPO for post-training large vision models, leading to substantial improvements in planning in a selection of representative visual navigation tasks, FrozenLake, Maze, and MiniBehavior. Our visual planning paradigm outperforms all other planning variants that conduct reasoning in the text-only space. Our results establish Visual Planning as a viable and promising supplement to language-based reasoning, opening new avenues for tasks that benefit from intuitive, image-based inference.",
    "link": "https://arxiv.org/abs/2505.11409"
  },
  {
    "id": "quantifying-attention-flow-in-transformers-36348827",
    "title": "Quantifying Attention Flow in Transformers",
    "summary": "In the Transformer model, \"self-attention\" combines information from attended embeddings into the representation of the focal embedding in the next layer. Thus, across layers of the Transformer, information originating from different tokens gets increasingly mixed. This makes attention weights unreliable as explanations probes. In this paper, we consider the problem of quantifying this flow of information through self-attention. We propose two methods for approximating the attention to input tokens given attention weights, attention rollout and attention flow, as post hoc methods when we use attention weights as the relative relevance of the input tokens. We show that these methods give complementary views on the flow of information, and compared to raw attention, both yield higher correlations with importance scores of input tokens obtained using an ablation method and input gradients.",
    "link": "https://arxiv.org/abs/2005.00928"
  },
  {
    "id": "learning-to-reason-without-external-rewards-ec2d64ad",
    "title": "Learning to Reason without External Rewards",
    "summary": "Training large language models (LLMs) for complex reasoning via Reinforcement Learning with Verifiable Rewards (RLVR) is effective but limited by reliance on costly, domain-specific supervision. We explore Reinforcement Learning from Internal Feedback (RLIF), a framework that enables LLMs to learn from intrinsic signals without external rewards or labeled data. We propose Intuitor, an RLIF method that uses a model's own confidence, termed self-certainty, as its sole reward signal. Intuitor replaces external rewards in Group Relative Policy Optimization (GRPO) with self-certainty scores, enabling fully unsupervised learning. Experiments demonstrate that Intuitor matches GRPO's performance on mathematical benchmarks while achieving superior generalization to out-of-domain tasks like code generation, without requiring gold solutions or test cases. Our findings show that intrinsic model signals can drive effective learning across domains, offering a scalable alternative to RLVR for autonomous AI systems where verifiable rewards are unavailable. Code is available at https://github.com/sunblaze-ucb/Intuitor",
    "link": "https://arxiv.org/abs/2505.19590"
  },
  {
    "id": "separating-tongue-from-thought-activation-patching-71b298da",
    "title": "Separating Tongue from Thought: Activation Patching Reveals Language-Agnostic Concept Representations in Transformers",
    "summary": "A central question in multilingual language modeling is whether large language models (LLMs) develop a universal concept representation, disentangled from specific languages. In this paper, we address this question by analyzing latent representations (latents) during a word-translation task in transformer-based LLMs. We strategically extract latents from a source translation prompt and insert them into the forward pass on a target translation prompt. By doing so, we find that the output language is encoded in the latent at an earlier layer than the concept to be translated. Building on this insight, we conduct two key experiments. First, we demonstrate that we can change the concept without changing the language and vice versa through activation patching alone. Second, we show that patching with the mean representation of a concept across different languages does not affect the models' ability to translate it, but instead improves it. Finally, we generalize to multi-token generation and demonstrate that the model can generate natural language description of those mean representations. Our results provide evidence for the existence of language-agnostic concept representations within the investigated models.",
    "link": "https://arxiv.org/abs/2411.08745v1"
  },
  {
    "id": "the-entropy-mechanism-of-reinforcement-learning-fo-b5390c05",
    "title": "The Entropy Mechanism of Reinforcement Learning for Reasoning Language Models",
    "summary": "This paper aims to overcome a major obstacle in scaling RL for reasoning with LLMs, namely the collapse of policy entropy. Such phenomenon is consistently observed across vast RL runs without entropy intervention, where the policy entropy dropped sharply at the early training stage, this diminished exploratory ability is always accompanied with the saturation of policy performance. In practice, we establish a transformation equation R=-a*e^H+b between entropy H and downstream performance R. This empirical law strongly indicates that, the policy performance is traded from policy entropy, thus bottlenecked by its exhaustion, and the ceiling is fully predictable H=0, R=-a+b. Our finding necessitates entropy management for continuous exploration toward scaling compute for RL. To this end, we investigate entropy dynamics both theoretically and empirically. Our derivation highlights that, the change in policy entropy is driven by the covariance between action probability and the change in logits, which is proportional to its advantage when using Policy Gradient-like algorithms. Empirical study shows that, the values of covariance term and entropy differences matched exactly, supporting the theoretical conclusion. Moreover, the covariance term stays mostly positive throughout training, further explaining why policy entropy would decrease monotonically. Through understanding the mechanism behind entropy dynamics, we motivate to control entropy by restricting the update of high-covariance tokens. Specifically, we propose two simple yet effective techniques, namely Clip-Cov and KL-Cov, which clip and apply KL penalty to tokens with high covariances respectively. Experiments show that these methods encourage exploration, thus helping policy escape entropy collapse and achieve better downstream performance.",
    "link": "https://arxiv.org/abs/2505.22617"
  },
  {
    "id": "prorl-prolonged-reinforcement-learning-expands-rea-54a7080c",
    "title": "ProRL: Prolonged Reinforcement Learning Expands Reasoning Boundaries in Large Language Models",
    "summary": "Recent advances in reasoning-centric language models have highlighted reinforcement learning (RL) as a promising method for aligning models with verifiable rewards. However, it remains contentious whether RL truly expands a model's reasoning capabilities or merely amplifies high-reward outputs already latent in the base model's distribution, and whether continually scaling up RL compute reliably leads to improved reasoning performance. In this work, we challenge prevailing assumptions by demonstrating that prolonged RL (ProRL) training can uncover novel reasoning strategies that are inaccessible to base models, even under extensive sampling. We introduce ProRL, a novel training methodology that incorporates KL divergence control, reference policy resetting, and a diverse suite of tasks. Our empirical analysis reveals that RL-trained models consistently outperform base models across a wide range of pass@k evaluations, including scenarios where base models fail entirely regardless of the number of attempts. We further show that reasoning boundary improvements correlates strongly with task competence of base model and training duration, suggesting that RL can explore and populate new regions of solution space over time. These findings offer new insights into the conditions under which RL meaningfully expands reasoning boundaries in language models and establish a foundation for future work on long-horizon RL for reasoning. We release model weights to support further research: https://huggingface.co/nvidia/Nemotron-Research-Reasoning-Qwen-1.5B",
    "link": "https://arxiv.org/abs/2505.24864"
  },
  {
    "id": "beyond-the-8020-rule-high-entropy-minority-tokens--89c3737c",
    "title": "Beyond the 80/20 Rule: High-Entropy Minority Tokens Drive Effective Reinforcement Learning for LLM Reasoning",
    "summary": "Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful approach to enhancing the reasoning capabilities of Large Language Models (LLMs), while its mechanisms are not yet well understood. In this work, we undertake a pioneering exploration of RLVR through the novel perspective of token entropy patterns, comprehensively analyzing how different tokens influence reasoning performance. By examining token entropy patterns in Chain-of-Thought (CoT) reasoning, we observe that only a small fraction of tokens exhibit high entropy, and these tokens act as critical forks that steer the model toward diverse reasoning pathways. Furthermore, studying how entropy patterns evolve during RLVR training reveals that RLVR largely adheres to the base model's entropy patterns, primarily adjusting the entropy of high-entropy tokens. These findings highlight the significance of high-entropy tokens (i.e., forking tokens) to RLVR. We ultimately improve RLVR by restricting policy gradient updates to forking tokens and uncover a finding even beyond the 80/20 rule: utilizing only 20% of the tokens while maintaining performance comparable to full-gradient updates on the Qwen3-8B base model and significantly surpassing full-gradient updates on the Qwen3-32B (+11.04 on AIME'25 and +7.71 on AIME'24) and Qwen3-14B (+4.79 on AIME'25 and +5.21 on AIME'24) base models, highlighting a strong scaling trend. In contrast, training exclusively on the 80% lowest-entropy tokens leads to a marked decline in performance. These findings indicate that the efficacy of RLVR primarily arises from optimizing the high-entropy tokens that decide reasoning directions. Collectively, our results highlight the potential to understand RLVR through a token-entropy perspective and optimize RLVR by leveraging high-entropy minority tokens to further improve LLM reasoning.",
    "link": "https://arxiv.org/abs/2506.01939"
  },
  {
    "id": "scaling-up-rl-unlocking-diverse-reasoning-in-llms--1fb4fe47",
    "title": "Scaling Up RL: Unlocking Diverse Reasoning in LLMs via Prolonged Training",
    "summary": "Recent advancements in reasoning-focused language models such as OpenAI's O1 and DeepSeek-R1 have shown that scaling test-time computation-through chain-of-thought reasoning and iterative exploration-can yield substantial improvements on complex tasks like mathematics and code generation. These breakthroughs have been driven by large-scale reinforcement learning (RL), particularly when combined with verifiable reward signals that provide objective and grounded supervision. In this report, we investigate the effects of prolonged reinforcement learning on a small language model across a diverse set of reasoning domains. Our work identifies several key ingredients for effective training, including the use of verifiable reward tasks, enhancements to Group Relative Policy Optimization (GRPO), and practical techniques to improve training stability and generalization. We introduce controlled KL regularization, clipping ratio, and periodic reference policy resets as critical components for unlocking long-term performance gains. Our model achieves significant improvements over strong baselines, including +14.7% on math, +13.9% on coding, and +54.8% on logic puzzle tasks. To facilitate continued research, we release our model publicly.",
    "link": "https://arxiv.org/abs/2507.12507"
  },
  {
    "id": "emerging-properties-in-unified-multimodal-pretrain-6c71c19a",
    "title": "Emerging Properties in Unified Multimodal Pretraining",
    "summary": "Unifying multimodal understanding and generation has shown impressive capabilities in cutting-edge proprietary systems. In this work, we introduce BAGEL, an open-source foundational model that natively supports multimodal understanding and generation. BAGEL is a unified, decoder-only model pretrained on trillions of tokens curated from large-scale interleaved text, image, video, and web data. When scaled with such diverse multimodal interleaved data, BAGEL exhibits emerging capabilities in complex multimodal reasoning. As a result, it significantly outperforms open-source unified models in both multimodal generation and understanding across standard benchmarks, while exhibiting advanced multimodal reasoning abilities such as free-form image manipulation, future frame prediction, 3D manipulation, and world navigation. In the hope of facilitating further opportunities for multimodal research, we share the key findings, pretraining details, data creation protocal, and release our code and checkpoints to the community. The project page is at https://bagel-ai.org/",
    "link": "https://arxiv.org/abs/2505.14683"
  },
  {
    "id": "evaluating-long-context-reasoning-ability-wh-b3d7e0c5",
    "title": "Evaluating Long Context (Reasoning) Ability | wh",
    "summary": "Pass@1 scores on the 128k subset of LongCodeEdit. Reasoning models and long agent trajectories are eating up valuable space in the context window. In response, models are being released with ever-increasing context windows; the latest, Grok 4 Fast, has a 2 million token window. Unfortunately, as anyone who has worked with these models knows, the number of tokens a model can accept as input is not the same as the number of tokens it can reason over. Performance often degrades well before reaching the advertised context window length. This seems at odds with the perfect context window claims from the many model providers. Effective context length Gemini 2.5 Pro (High), GPT-5 (High) 128k (Max) Claude Sonnet 4, Qwen 3 Max (blue) 64kâ€“128k Grok-4 Fast, DeepSeek Chat v3.1 32kâ€“64k Grok-4 16kâ€“32k Kimi K2, GLM 4.5 (High) <32k Effective context length is the maximum input token length where a model's performance remains high and stable on LongCodeEdit (max context length of 128k). In this post, I will show why this discrepency exists by looking at existing long context benchmarks. I will then discuss what makes a good long context benchmark and introduce a new benchmarkâ€”LongCodeEdit. Existing Long Context Evaluation Existing long context benchmarks have progressively attempted to answer three questions. 1. Can the model access its context window? This is a rather straightforward question as it is simply a matter of whether the very last token can attend to the very first. For example, a model that can take as input 1M tokens but was trained on 4K sequences is unlikely to be able to access its entire context window due to the masking during training. The first widely used long context benchmark was Needle In A Haystack (NIAH). Here, the model receives a query, a long â€œhaystackâ€ of text, and a hidden â€œneedleâ€ containing the answer and its task is to retrieve the needle. This was later adapted by RULER to include multiple needles and even distractor needles. Needle In A Haystack results for GPT4. Source The all-green plots from this benchmark often went viral and captured significant mindshare. However, this soon became saturated and is no longer a robust indicator of long context capabilities. To understand why, letâ€™s look at a RULER example: ...Which means that what matters is who you are, not when you do it. If you're the right sort of person, you'll win even in a bad economy. And if you're not, a good economy won't save you. The special magic number for XXX is 12345. Someone who thinks \"I better not start a startup now, because the economy is so bad\" is making the same mistake as the people who thought during the Bubble \"all I have to do is start a startup, and I'll be rich.\"... Question: What is the special magic number for XXX? The needle is completely unrelated to the haystack and this becomes a simple retrieval task with almost no reasoning required. The model simply has to look for and repeat past occurrences of the needle. 2. Is the model able to understand every input token? Under this setting, we assume that the model is able to retrieve the necessary information and we are now interested in how well it understands the context window. The best version of this is NoLiMa which turns NIAH into a question answering task. The â€œneedleâ€ is a piece of information that comes from the same distribution as the haystack but with minimal literal overlap with the query, forcing at least one reasoning step to draw an association between the two. RULER also includes a QA task using the SQuAD dataset. HELMET concatenated many documents together and asks a question from any one of the documents. LongBench v2 extends question answering to even more domains including code repositories, structured data and long dialogues. The Fiction.Live benchmark is an example of a difficult Question Answering task where the answer cannot just be extracted from a single sentence within the context and requires reasoning over the entire document. 3. Is the model able to perform real-world long context tasks? The problem with long context benchmarks is that high quality long context data is hard to come by, so most approaches resort to padding an existing language modelling task with often irrelevant input tokens. The underlying task itself is often simple - SQuAD v2 (used in RULER) was released in 2018 and is effectively saturated by all frontier LLMs. For example, if we compare the difficulty of these long context benchmarks to generic intelligence benchmarks (such as GPQA Diamond), the difference is stark. â€¦ ... Actually, Yuki lives next to the Semper Opera House. â€¦ ... Question: Which character has been to Dresden? Answer: Yuki (The Semper Opera House is in Dresden) Imagine a situation where a novel natural molecule, Xantheraquin, is discovered and is being investigated for in silico testing against a bacterial pathogen that is multidrug resistant. Preliminary studies indicate that Xantheraquin has multiple chiral centers and can exist in various tautomeric forms. Which of the following steps would be the MOST crucial before proceeding with in silico docking studies, given the complexities of bioinformatics in structure-based drug discovery? A) Analyze all tautomeric and chiral forms, but prioritize those forms that are most likely to be biologically active based on physicochemical properties. B) Use the most stable chiral form of Xantheraquin, relying on quantum mechanical calculations to predict its interaction with the bacterial target. C) Focus on Xantheraquin's pharmacokinetics and ADME (Absorption, Distribution, Metabolism, Excretion) properties, using molecular dynamics simulations to predict its behavior in a biological system. D) Combine in silico predictions with preliminary in vitro binding affinity assays to validate the most promising forms of Xantheraquin before extensive docking studies. Answer: D New models are often evaluated on \"PhD-level\" problems, but the moment it comes to long context, for some reason, we have lowered our expectations to \"tasks that were solved years ago by Flan-T5 but much longer\". OpenAI recently released MRCR which offers their definition",
    "link": "https://nrehiew.github.io/blog/long_context/"
  },
  {
    "id": "taking-the-bitter-lesson-seriously-by-rohan-pandey-2999d4a5",
    "title": "Taking the Bitter Lesson Seriously - by Rohan Pandey",
    "summary": "Taking the Bitter Lesson Seriously TLDR: to accelerate AI, enable it to hill-climb compute & energy through experimentally verifiable science Rohan Pandey Sep 30, 2025 45 3 1 Share â€œ The biggest lesson that can be read from 70 years of AI research is that general methods that leverage computation are ultimately the most effective, and by a large margin. The ultimate reason for this is Moore's law, or rather its generalization of continued exponentially falling cost per unit of computation. â€ - Richard Sutton, The Bitter Lesson In February 2025 at OpenAI, I got RL-pilled. Reinforcement Learning allows an LLM to hill-climb any problem where verification is easier than generation, and by scaling up RL compute, the Strawberry team was saturating all their environments every few weeks. So I started wondering to myself what the highest value RL environment might be. If youâ€™ve read The Bitter Lesson, you understand that AI is fundamentally advanced by scaling. But do you Take the Bitter Lesson Seriously? We, AI researchers, continue to go about our work on algorithms, architecture, and data as if it were still 2019 and scaling laws were yet to be discovered. But now itâ€™s 2025, and we know that more compute & more energy is the most reliable path to advancing AI . The dream at many frontier labs is recursive self-improvement : AI good enough to code a better n+1 version of itself, which codes a better n+2, â€¦, until the Universe is overflowing with intelligence. Technology has been accelerating its own progress for centuries, but the notion that AI will algorithmically self-improve ad infinitum forgets the Bitter Lesson and that research is compute-bound . Kick off a few agents to run pretraining experiments and see how quickly you run out of H100s. Is it hopeless then? Does the Bitter Lesson doom us to be compute-bound, waiting for Nvidia to ship GB300, for ASML to perfect High-NA EUV, and for nuclear fusion to be solved? No. If you Take the Bitter Lesson Seriously, recursive self-improvement means freeing AI to accelerate compute & energyâ€”technologies bottlenecked by real-world science . At Periodic Labs, weâ€™re building the highest value RL environment: an autonomous laboratory for matter. Our first lab in Menlo Park gives AI the robotic & scientific tools to spark an exponential scale-up of compute & energy through breakthroughs in advanced materials. For example, to discover a high temperature superconductor (i.e. an LK-99 that works), we might take an LLM weâ€™ve midtrained on terabytes of condensed-matter physics data and run RL guided by a reward for higher critical temperature. In each iteration, the LLM hypothesizes & proposes experimental materials like Li 2 Be 1 Ga 6 Rh 3 which are executed in simulation for rapid feedback or synthesized by robots in the lab for ground-truth T c verification. Verifiability makes physics as hill-climbable as math or coding. Autonomous Science is thus the most worthy problem for a bitterlessonpilled AI researcher, and comes loaded with questions about RL, synthetic data, & supercomputing that Periodic Labs is actively hiring for. Iâ€™m incredibly lucky to work on such a talented , well-capitalized team led by our founders Liam (ex-VP of Post-training at OpenAI) & Dogus (ex-Head of Quantum Materials at Google DeepMind) who pioneered ChatGPT & GNoME . I hope youâ€™ll join us as we ascend the Kardashev scale. Acknowledgements: Thank you to John Hallman, Archie Sravankumar, Aryaman Arora, Bayes Lord, Roon, Will Brown, Dwarkesh Patel, Clive Chan, Dylan Patel, Liam Fedus, Tamay Besiroglu, and Anjney Midha for reviewing drafts of this essay. 45 3 1 Share",
    "link": "https://rohanpandey.substack.com/p/taking-the-bitter-lesson-seriously"
  },
  {
    "id": "colbert-efficient-and-effective-passage-search-via-a99666ba",
    "title": "ColBERT: Efficient and Effective Passage Search via Contextualized Late Interaction over BERT",
    "summary": "Recent progress in Natural Language Understanding (NLU) is driving fast-paced advances in Information Retrieval (IR), largely owed to fine-tuning deep language models (LMs) for document ranking. While remarkably effective, the ranking models based on these LMs increase computational cost by orders of magnitude over prior approaches, particularly as they must feed each query-document pair through a massive neural network to compute a single relevance score. To tackle this, we present ColBERT, a novel ranking model that adapts deep LMs (in particular, BERT) for efficient retrieval. ColBERT introduces a late interaction architecture that independently encodes the query and the document using BERT and then employs a cheap yet powerful interaction step that models their fine-grained similarity. By delaying and yet retaining this fine-granular interaction, ColBERT can leverage the expressiveness of deep LMs while simultaneously gaining the ability to pre-compute document representations offline, considerably speeding up query processing. Beyond reducing the cost of re-ranking the documents retrieved by a traditional model, ColBERT's pruning-friendly interaction mechanism enables leveraging vector-similarity indexes for end-to-end retrieval directly from a large document collection. We extensively evaluate ColBERT using two recent passage search datasets. Results show that ColBERT's effectiveness is competitive with existing BERT-based models (and outperforms every non-BERT baseline), while executing two orders-of-magnitude faster and requiring four orders-of-magnitude fewer FLOPs per query.",
    "link": "https://arxiv.org/abs/2004.12832"
  },
  {
    "id": "glyph-scaling-context-windows-via-visual-text-comp-80cdf207",
    "title": "Glyph: Scaling Context Windows via Visual-Text Compression",
    "summary": "Large language models (LLMs) increasingly rely on long-context modeling for tasks such as document understanding, code analysis, and multi-step reasoning. However, scaling context windows to the million-token level brings prohibitive computational and memory costs, limiting the practicality of long-context LLMs. In this work, we take a different perspective-visual context scaling-to tackle this challenge. Instead of extending token-based sequences, we propose Glyph, a framework that renders long texts into images and processes them with vision-language models (VLMs). This approach substantially compresses textual input while preserving semantic information, and we further design an LLM-driven genetic search to identify optimal visual rendering configurations for balancing accuracy and compression. Through extensive experiments, we demonstrate that our method achieves 3-4x token compression while maintaining accuracy comparable to leading LLMs such as Qwen3-8B on various long-context benchmarks. This compression also leads to around 4x faster prefilling and decoding, and approximately 2x faster SFT training. Furthermore, under extreme compression, a 128K-context VLM could scale to handle 1M-token-level text tasks. In addition, the rendered text data benefits real-world multimodal tasks, such as document understanding. Our code and model are released at https://github.com/thu-coai/Glyph.",
    "link": "https://arxiv.org/abs/2510.17800"
  },
  {
    "id": "part-3-trust-region-optimization-via-sequence-mask-6e49a9cb",
    "title": "Part 3: Trust Region Optimization via Sequence Masking | Yingru Li",
    "summary": "Part 3: Trust Region Optimization via Sequence Masking Yingru LI , jiacai-liu Nov 4, 2025 18 min read Research Authors: Yingru Li , Jiacai Liu Original Blog: When Speed Kills Stability: Demystifying RL Collapse from the Training-Inference Mismatch Series Context Part 1 : We established the SGA (Stochastic Gradient Ascent) framework and identified two failure modes of off-policy mismatch: Bias (measured by $D_{TV}$) and Variance (measured by $\\chi^2$-divergence). Part 2 : We analyzed gradient estimators and showed that Token-level IS (PPO/GRPO) has $O(T^2 \\Delta_{\\max})$ bias, while Sequence-Level Truncated IS (Seq-TIS) achieves a controllable bias-variance trade-off via clipping: $\\rho(y) \\to \\min(\\rho(y), C)$. TL;DR In a standard statistical setting, Part 2 solved the problem with Seq-TIS. However, when training Agents or Reasoning Models (Chain-of-Thought), two practical phenomena violate the assumptions underlying Seq-TIS: Out-of-Distribution (OOD) High-Weight Samples: Extremely high importance weights ($\\rho \\gg C$) often correspond to samples outside the behavior policyâ€™s supportâ€”numerical errors or distribution shift artifacts. Clipping these samples still includes them in the gradient update. Solution: Enforce a Hard Trust Region via Rejection/Masking (Seq-MIS). Length-Dependent Rejection Bias: The importance ratio $\\rho(y) = \\prod_t \\rho_t$ grows exponentially with sequence length $T$, causing systematic rejection of long sequences regardless of per-step quality. Solution: Geometric Sequence Masking (Geo-Mask), which enforces a Per-Token Trust Region using a length-normalized KL divergence criterion. Pathology What Breaks Why Standard Methods Fail OOD High-Weight Samples Samples with $\\rho \\gg C$ lie outside trust region Clipping retains them with weight $C$â€”still corrupts gradients Length-Dependent Bias $\\rho(y) = \\prod_t \\rho_t$ grows exponentially with $T$ Fixed threshold $C$ systematically rejects long sequences Theory-Practice Gap TRPO requires trust region $\\propto 1/T^2$ PPO/GRPO use fixed clipping $\\epsilon$ regardless of $T$ Citation @online { liu-li-2025-rl-collapse , title = {When Speed Kills Stability: Demystifying {RL} Collapse from the Training-Inference Mismatch} , author = {Liu, Jiacai and Li, Yingru and Fu, Yuqian and Wang, Jiawei and Liu, Qian and Shen, Yu} , year = {2025} , month = sep , url = {https://richardli.xyz/rl-collapse} } Recap: The Trust Region Framework In Part 1, we established the theoretical foundation for trust region optimization. Here we recap the key results that motivate the methods in this part. The Surrogate Objective and Its Limitations Consider an autoregressive language model generating a sequence $y = (y_0, y_1, \\ldots, y_{T-1})$ given prompt $x$. When optimizing a policy $\\pi$ using samples from a behavior policy $\\mu$, we cannot directly optimize the true objective $J(\\pi)$. Instead, we optimize a surrogate objective : $$ L_\\mu(\\pi) = J(\\mu) + \\sum_{t=0}^{T-1} \\mathbb{E}_{(x, y_{\\lt t}) \\sim d_{\\mu,t}} \\mathbb{E}_{y_t \\sim \\pi(\\cdot|x, y_{\\lt t})} [A_\\mu(x, y_{\\le t})] $$ where $d_{\\mu,t}$ is the context distribution at step $t$ under $\\mu$, and $A_\\mu(x, y_{\\le t})$ is the advantage of generating token $y_t$ in context $(x, y_{\\lt t})$. The surrogate is a first-order approximation ( Kakade & Langford, 2002 ; Schulman et al., 2015 ) that satisfies: $L_\\mu(\\mu) = J(\\mu)$ (equal values at $\\pi = \\mu$) $\\nabla L_\\mu(\\pi)\\big|_{\\pi=\\mu} = \\nabla J(\\pi)\\big|_{\\pi=\\mu}$ (equal gradients at $\\pi = \\mu$) However, the approximation degrades as $\\pi$ moves away from $\\mu$. The Surrogate Gradient (Token-IS): Taking the gradient of $L_\\mu(\\pi)$: $$ \\nabla L_\\mu(\\pi) = \\sum_{t=0}^{T-1} \\mathbb{E}_{(x, y_{\\lt t}) \\sim d_{\\mu,t}} \\mathbb{E}_{y_t \\sim \\pi(\\cdot|x, y_{\\lt t})} [A_\\mu(x, y_{\\le t}) \\nabla \\log \\pi(y_t|x, y_{\\lt t})] $$ To estimate this from samples $y \\sim \\mu$, we apply importance sampling at each token: $$ \\nabla L_\\mu(\\pi) = \\mathbb{E}_{y \\sim \\mu} \\left[ \\sum_{t=0}^{T-1} \\rho_t \\cdot A_t \\nabla \\log \\pi(y_t|x, y_{\\lt t}) \\right] $$ where $\\rho_t = \\frac{\\pi(y_t|x, y_{\\lt t})}{\\mu(y_t|x, y_{\\lt t})}$ and $A_t = A_\\mu(x, y_{\\le t})$. This is the Token-IS gradient â€”the foundation for PPO ( Schulman et al., 2017 ) and GRPO ( Shao et al., 2024 ). The TRPO Lower Bound The Performance Difference Lemma ( Kakade & Langford, 2002 ) quantifies the gap between the surrogate and true objectives: $$ J(\\pi) - J(\\mu) = \\sum_{t=0}^{T-1} \\mathbb{E}_{(x, y_{\\lt t}) \\sim d_{\\pi,t}} \\mathbb{E}_{y_t \\sim \\pi(\\cdot|x, y_{\\lt t})} [A_\\mu(x, y_{\\le t})] $$ The key difference from the surrogate is that the true improvement uses the context distribution $d_{\\pi,t}$ (under the new policy), while the surrogate uses $d_{\\mu,t}$ (under the old policy). Using the Simulation Lemma ( Kearns & Singh, 2002 ), which bounds how context distributions diverge over the sequence: $$ D_{TV}(d_{\\pi,t} \\| d_{\\mu,t}) \\leq t \\cdot D_{TV}^{\\max}(\\pi, \\mu) $$ we derive the TRPO lower bound ( Schulman et al., 2015 ): $$ \\boxed{J(\\pi) \\geq L_\\mu(\\pi) - C \\cdot T^2 \\cdot D_{TV}^{\\max}(\\pi, \\mu)} $$ where: $C$ is a constant depending on the maximum advantage $\\max_{x, y_{\\le t}}|A_\\mu(x, y_{\\le t})|$ $T$ is the sequence length $D_{TV}^{\\max} = \\max_{x, y_{\\lt t}} D_{TV}(\\pi(\\cdot|x, y_{\\lt t}) | \\mu(\\cdot|x, y_{\\lt t}))$ is the maximum per-token TV distance The Trust Region Requirement For surrogate optimization to guarantee improvement in the true objective, the policy must stay within a trust region : $$ D_{TV}^{\\max}(\\pi, \\mu) \\leq \\delta $$ The critical insight is that the valid trust region size must shrink with sequence length : $$ \\delta \\propto \\frac{1}{T^2} $$ This $T^2$ dependence arises because context distribution errors accumulate linearly over $T$ tokens, and the total error (summed over all tokens) scales quadratically. Soft vs. Hard Trust Regions The TRPO framework suggests two approaches to enforce trust regions: Type Mechanism Implementation Soft (Clipping) Down-weight samples outside the region $\\min(\\rho, C)$ â€” sample included with bounded weight Hard (Rejection) Exclude samples outside the region $\\mathbb{I}(\\rho \\leq C)$ â€” sample excluded entirely Soft trust regions use clipped importance sampling: $\\min(\\rho_t, 1+\\epsilon)$. This is computationally efficient but retains potentially problematic samples. This part develops hard trust region methods â€”Seq-MIS and Geo-Maskâ€”that completely exclude samples outside the trusted region. We show when and why hard rejection outperforms soft clipping. 1. OOD High-Weight Samples: Why Rejection Outperforms Clipping 1.1 The Problem: Clipping Retains Problematic Samples In Part 2, we derived the Seq-TIS estimator: $$ \\hat{g}_{\\text{seq-tis}}(y) = \\min(\\rho(y), C) \\cdot f(y) $$ where $f(y) = R(y) \\cdot \\nabla \\log \\pi(y)$ is the score function (reward-weighted gradient), and $\\rho(y) = \\pi(y)/\\mu(y)$ is the sequence-level importance ratio. The implicit assumption was that all samples $y \\sim \\mu$",
    "link": "https://richardli.xyz/post/rl-collapse-part3/"
  },
  {
    "id": "mathematical-formulations-of-rollout-correction-me-c97df761",
    "title": "Mathematical Formulations of Rollout Correction Methods in verl â€” verl documentation",
    "summary": "Mathematical Formulations of Rollout Correction Methods in verl View page source Mathematical Formulations of Rollout Correction Methods in verl ïƒ Author: Yingru Li Last updated: 2025-11-04 ðŸ“– Documentation Structure This document - Mathematical theory: formulations, derivations, and algorithmic foundations Rollout Correction Usage Guide - Practical implementation: configurations, presets, troubleshooting Start here for theory and design rationale, refer to the usage guide for implementation. BibTeX Citation ïƒ @online { liu-li-2025-rl-collapse , title = {When Speed Kills Stability: Demystifying {RL} Collapse from the Training-Inference Mismatch} , author = {Liu, Jiacai and Li, Yingru and Fu, Yuqian and Wang, Jiawei and Liu, Qian and Shen, Yu} , year = {2025} , month = sep , url = {https://richardli.xyz/rl-collapse} } Blog Series ïƒ Main blog post: https://richardli.xyz/rl-collapse Part 1: Why Mismatch Breaks LLM-RL (analytical framework using TV distance for bias and Ï‡Â²-divergence for variance) Part 2: The Gradient Estimator Trials (token-level vs sequence-level correction bias-variance tradeoff) Part 3: When Math Meets Realityâ€”Toxic Tails and Length Traps (why rejection over clipping, and geometric-level RS) Abstract ïƒ This document provides the definitive mathematical formulations for rollout correction methods in verl , following the natural progression from REINFORCE to PPO to Decoupled PPO . Rollout correction provides a unified framework to handle general off-policy problems in RL training - any scenario where the data collection distribution differs from the training distribution. Applicable scenarios include: Policy mismatch : Different precision (FP8 vs FP16 vs BF16 vs FP32), different backends (vLLM vs SGLang vs FSDP vs Megatron) Temporal lag : Model staleness, asynchronous rollout workers Replay buffers : Training on historical trajectories from earlier policy versions Off-policy algorithms : Behavioral cloning, DAPO, expert demonstrations Data filtering : Reweighting, preference learning, curriculum learning Table of Contents ïƒ Theoretical Foundation: From REINFORCE to Decoupled PPO Implementation in verl: The Three-Policy Framework Algorithmic Components and Combinations Off-Policy Diagnostic Metrics Summary and Decision Guide Implementation References 1. Theoretical Foundation: From REINFORCE to Decoupled PPO ïƒ This section establishes the theoretical progression that verl implements. 1.1 REINFORCE: Policy Gradient Baseline ïƒ The REINFORCE algorithm ( Williams, 1992 ) is the foundation of policy gradient methods. Vanilla REINFORCE (On-Policy) For trajectories \\(\\tau = (s_0, a_0, s_1, a_1, \\ldots, s_T, a_T)\\) sampled from the current policy \\(\\pi_\\theta\\) , the policy gradient is: \\[ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\pi_\\theta} \\left[ \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot A_t \\right] \\] where \\(A_t\\) is the advantage function at timestep \\(t\\) . Off-Policy REINFORCE When trajectories are sampled from a different behavior policy \\(\\mu\\) , we apply importance sampling over the joint trajectory distribution : \\[ \\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\tau \\sim \\mu} \\left[ \\frac{P_{\\pi_\\theta}(\\tau)}{P_\\mu(\\tau)} \\sum_{t=0}^T \\nabla_\\theta \\log \\pi_\\theta(a_t|s_t) \\cdot A_t \\right] \\] where the trajectory-level importance weight is: \\[ \\frac{P_{\\pi_\\theta}(\\tau)}{P_\\mu(\\tau)} = \\frac{p(s_0) \\prod_{t=0}^T \\pi_\\theta(a_t|s_t) p(s_{t+1}|s_t, a_t)}{p(s_0) \\prod_{t=0}^T \\mu(a_t|s_t) p(s_{t+1}|s_t, a_t)} = \\prod_{t=0}^T \\frac{\\pi_\\theta(a_t|s_t)}{\\mu(a_t|s_t)} \\] The transition dynamics \\(p(s_{t+1}|s_t, a_t)\\) and initial state \\(p(s_0)\\) cancel out, leaving only the product of per-step action probability ratios. Key properties: Off-policy capable : Can learn from any behavior policy via importance sampling No trust region : Policy updates not constrained Implementation in verl: The bypass_pg_is preset implements off-policy REINFORCE with truncated importance sampling. 1.2 PPO: Adding Trust Region Control ïƒ Proximal Policy Optimization ( Schulman et al., 2017 ) adds a clipped surrogate objective: \\[ L_{\\text{PPO}}(\\theta) = -\\mathbb{E}_{(s,a) \\sim \\mu} \\left[ \\min\\left( r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t \\right) \\right] \\] where \\(r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\mu(a_t|s_t)}\\) and \\(\\epsilon\\) is the clip range (typically 0.2). Key properties: Two policies : \\(\\mu\\) (reference for clipping) and \\(\\pi_\\theta\\) (being updated) Trust region via clipping : Limits policy update magnitude via ratio \\(r_t(\\theta) = \\frac{\\pi_\\theta}{\\mu}\\) 1.3 Decoupled PPO: Achieving Batch Size Invariance ïƒ Decoupled PPO ( Hilton et al., 2021 ) solves PPOâ€™s batch size sensitivity by decoupling two roles : Proximal policy \\(\\pi_{\\text{prox}}\\) : The anchor policy for PPO clipping (controls policy update size) Behavior policy \\(\\mu\\) : The policy that collected the data (for off-policy correction via importance sampling) The problem : Standard PPO controls policy update size via the ratio \\(\\frac{\\pi_\\theta}{\\pi_{\\text{old}}}\\) , where \\(\\pi_{\\text{old}}\\) is assumed to be both the proximal policy and the behavior policy. This coupling makes the algorithm sensitive to batch size because aggregating data from multiple workers or using replay buffers changes the effective behavior policy. The solution : Decouple these two roles, leading to a three-policy formulation : \\[ L_{\\text{DecoupledPPO}}(\\theta) = -\\mathbb{E}_{(s,a) \\sim \\mu} \\left[ w_t \\cdot \\min\\left( r_t(\\theta) A_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon) A_t \\right) \\right] \\] where: \\(w_t = \\frac{\\pi_{\\text{prox}}(a_t|s_t)}{\\mu(a_t|s_t)}\\) : Importance sampling weight (corrects for behavior policy \\(\\mu\\) ). Here \\(\\pi_{\\text{prox}}\\) is frozen during training, so \\(w_t\\) is constant (no stopgrad operator needed). \\(r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\text{prox}}(a_t|s_t)}\\) : PPO ratio (controls policy update size against proximal policy \\(\\pi_{\\text{prox}}\\) ) Key properties : By decoupling: Batch size invariance : Policy update control (via \\(\\pi_{\\text{prox}}\\) ) is independent of data aggregation Flexible behavior policy : Any \\(\\mu\\) can be used (different workers, replay buffers, or stale checkpoints) Stale data utilization : Older trajectories can be corrected via importance sampling Clipping preserved : Clipping against \\(\\pi_{\\text{prox}}\\) limits update magnitude This is the algorithm that verl implements via its three-policy framework. 2. Implementation in verl: The Three-Policy Framework ïƒ The verl library implements decoupled PPO using three distinct policies, each serving a specific role. 2.1 Policy Roles and Notation ïƒ \\(\\pi_{\\text{rollout}}\\) (Behavior Policy \\(\\mu\\) ) The policy used for data collection. This is the behavior distribution \\(\\mu\\) from theory. When created : During rollout/data collection phase Purpose : Generate trajectories for training Common sources : Policy mismatch: Same weights, different implementation (precision, backend) Temporal lag: Stale checkpoint from async workers Replay buffer: Historical data from earlier iterations Off-policy algorithms: Expert demonstrations, auxiliary policies (DAPO) Data filtering: Reweighted or filtered data Fixed : Frozen during training on a batch \\(\\pi_{\\text{old}}\\) (Proximal Policy \\(\\pi_{\\text{prox}}\\) ) The reference policy for PPO clipping. This is the â€œproximal policyâ€ from decoupled PPO theory. When created : Decoupled mode : Computed at start of training epoch via actor.compute_log_prob() Bypass mode : Set equal to \\(\\pi_{\\text{rollout}}\\) (skips separate computation) Purpose :",
    "link": "https://verl.readthedocs.io/en/latest/algo/rollout_corr_math.html#geometric-aggregation-geo-rs"
  },
  {
    "id": "training-foundation-models-on-a-full-stack-amd-pla-6f27d087",
    "title": "Training Foundation Models on a Full-Stack AMD Platform: Compute, Networking, and System Design",
    "summary": "We report on the first large-scale mixture-of-experts (MoE) pretraining study on pure AMD hardware, utilizing both MI300X GPUs and Pollara networking. We distill practical guidance for both systems and model design. On the systems side, we deliver a comprehensive cluster and networking characterization: microbenchmarks for all core collectives (all-reduce, reduce-scatter, all-gather, broadcast) across message sizes and GPU counts over Pollara. To our knowledge, this is the first at this scale. We further provide MI300X microbenchmarks on kernel sizing and memory bandwidth to inform model design. On the modeling side, we introduce and apply MI300X-aware transformer sizing rules for attention and MLP blocks and justify MoE widths that jointly optimize training throughput and inference latency. We describe our training stack in depth, including often-ignored utilities such as fault-tolerance and checkpoint-reshaping, as well as detailed information on our training recipe. We also provide a preview of our model architecture and base model - ZAYA1 (760M active, 8.3B total parameters MoE, available at https://huggingface.co/Zyphra/ZAYA1-base) - which will be further improved upon in forthcoming papers. ZAYA1-base achieves performance comparable to leading base models such as Qwen3-4B and Gemma3-12B at its scale and larger, and outperforms models including Llama-3-8B and OLMoE across reasoning, mathematics, and coding benchmarks. Together, these results demonstrate that the AMD hardware, network, and software stack are mature and optimized enough for competitive large-scale pretraining.",
    "link": "https://arxiv.org/abs/2511.17127"
  },
  {
    "id": "spurious-rewards-rethinking-training-signals-in-rl-0bee283c",
    "title": "Spurious Rewards: Rethinking Training Signals in RLVR",
    "summary": "We show that reinforcement learning with verifiable rewards (RLVR) can elicit strong mathematical reasoning in certain models even with spurious rewards that have little, no, or even negative correlation with the correct answer. For example, RLVR improves MATH-500 performance for Qwen2.5-Math-7B in absolute points by 21.4% (random reward), 13.8% (format reward), 24.1% (incorrect label), 26.0% (1-shot RL), and 27.1% (majority voting) -- nearly matching the 29.1% gained with ground truth rewards. However, the spurious rewards that work for Qwen often fail to yield gains with other model families like Llama3 or OLMo2. In particular, we find code reasoning -- thinking in code without actual code execution -- to be a distinctive Qwen2.5-Math behavior that becomes significantly more frequent after RLVR, from 65% to over 90%, even with spurious rewards. Overall, we hypothesize that, given the lack of useful reward signal, RLVR must somehow be surfacing useful reasoning representations learned during pretraining, although the exact mechanism remains a topic for future work. We suggest that future RLVR research should possibly be validated on diverse models rather than a single de facto choice, as we show that it is easy to get significant performance gains on Qwen models even with completely spurious reward signals.",
    "link": "https://arxiv.org/abs/2506.10947"
  },
  {
    "id": "glm-45v-and-glm-41v-thinking-towards-versatile-mul-a7c3cc0e",
    "title": "GLM-4.5V and GLM-4.1V-Thinking: Towards Versatile Multimodal Reasoning with Scalable Reinforcement Learning",
    "summary": "We present GLM-4.1V-Thinking and GLM-4.5V, a family of vision-language models (VLMs) designed to advance general-purpose multimodal understanding and reasoning. In this report, we share our key findings in the development of the reasoning-centric training framework. We first develop a capable vision foundation model with significant potential through large-scale pre-training, which arguably sets the upper bound for the final performance. We then propose Reinforcement Learning with Curriculum Sampling (RLCS) to unlock the full potential of the model, leading to comprehensive capability enhancement across a diverse range of tasks, including STEM problem solving, video understanding, content recognition, coding, grounding, GUI-based agents, and long document interpretation. In a comprehensive evaluation across 42 public benchmarks, GLM-4.5V achieves state-of-the-art performance on nearly all tasks among open-source models of similar size, and demonstrates competitive or even superior results compared to closed-source models such as Gemini-2.5-Flash on challenging tasks including Coding and GUI Agents. Meanwhile, the smaller GLM-4.1V-9B-Thinking remains highly competitive-achieving superior results to the much larger Qwen2.5-VL-72B on 29 benchmarks. We open-source both GLM-4.1V-9B-Thinking and GLM-4.5V. Code, models and more information are released at https://github.com/zai-org/GLM-V.",
    "link": "https://arxiv.org/abs/2507.01006"
  },
  {
    "id": "plaid-an-efficient-engine-for-late-interaction-ret-c16019f6",
    "title": "PLAID: An Efficient Engine for Late Interaction Retrieval",
    "summary": "Pre-trained language models are increasingly important components across multiple information retrieval (IR) paradigms. Late interaction, introduced with the ColBERT model and recently refined in ColBERTv2, is a popular paradigm that holds state-of-the-art status across many benchmarks. To dramatically speed up the search latency of late interaction, we introduce the Performance-optimized Late Interaction Driver (PLAID). Without impacting quality, PLAID swiftly eliminates low-scoring passages using a novel centroid interaction mechanism that treats every passage as a lightweight bag of centroids. PLAID uses centroid interaction as well as centroid pruning, a mechanism for sparsifying the bag of centroids, within a highly-optimized engine to reduce late interaction search latency by up to 7$\\times$ on a GPU and 45$\\times$ on a CPU against vanilla ColBERTv2, while continuing to deliver state-of-the-art retrieval quality. This allows the PLAID engine with ColBERTv2 to achieve latency of tens of milliseconds on a GPU and tens or just few hundreds of milliseconds on a CPU at large scale, even at the largest scales we evaluate with 140M passages.",
    "link": "https://arxiv.org/abs/2205.09707"
  },
  {
    "id": "towards-storage-efficient-visual-document-retrieva-69971a73",
    "title": "Towards Storage-Efficient Visual Document Retrieval: An Empirical Study on Reducing Patch-Level Embeddings",
    "summary": "Despite the strong performance of ColPali/ColQwen2 in Visualized Document Retrieval (VDR), it encodes each page into multiple patch-level embeddings and leads to excessive memory usage. This empirical study investigates methods to reduce patch embeddings per page at minimum performance degradation. We evaluate two token-reduction strategies: token pruning and token merging. Regarding token pruning, we surprisingly observe that a simple random strategy outperforms other sophisticated pruning methods, though still far from satisfactory. Further analysis reveals that pruning is inherently unsuitable for VDR as it requires removing certain page embeddings without query-specific information. Turning to token merging (more suitable for VDR), we search for the optimal combinations of merging strategy across three dimensions and develop Light-ColPali/ColQwen2. It maintains 98.2% of retrieval performance with only 11.8% of original memory usage, and preserves 94.6% effectiveness at 2.8% memory footprint. We expect our empirical findings and resulting Light-ColPali/ColQwen2 offer valuable insights and establish a competitive baseline for future research towards efficient VDR.",
    "link": "https://arxiv.org/abs/2506.04997"
  },
  {
    "id": "doclens-a-tool-augmented-multi-agent-framework-for-87dc04b6",
    "title": "DocLens : A Tool-Augmented Multi-Agent Framework for Long Visual Document Understanding",
    "summary": "Comprehending long visual documents, where information is distributed across extensive pages of text and visual elements, is a critical but challenging task for modern Vision-Language Models (VLMs). Existing approaches falter on a fundamental challenge: evidence localization. They struggle to retrieve relevant pages and overlook fine-grained details within visual elements, leading to limited performance and model hallucination. To address this, we propose DocLens, a tool-augmented multi-agent framework that effectively ``zooms in'' on evidence like a lens. It first navigates from the full document to specific visual elements on relevant pages, then employs a sampling-adjudication mechanism to generate a single, reliable answer. Paired with Gemini-2.5-Pro, DocLens achieves state-of-the-art performance on MMLongBench-Doc and FinRAGBench-V, surpassing even human experts. The framework's superiority is particularly evident on vision-centric and unanswerable queries, demonstrating the power of its enhanced localization capabilities.",
    "link": "https://arxiv.org/abs/2511.11552"
  },
  {
    "id": "arc-is-a-vision-problem-526fd664",
    "title": "ARC Is a Vision Problem!",
    "summary": "The Abstraction and Reasoning Corpus (ARC) is designed to promote research on abstract reasoning, a fundamental aspect of human intelligence. Common approaches to ARC treat it as a language-oriented problem, addressed by large language models (LLMs) or recurrent reasoning models. However, although the puzzle-like tasks in ARC are inherently visual, existing research has rarely approached the problem from a vision-centric perspective. In this work, we formulate ARC within a vision paradigm, framing it as an image-to-image translation problem. To incorporate visual priors, we represent the inputs on a \"canvas\" that can be processed like natural images. It is then natural for us to apply standard vision architectures, such as a vanilla Vision Transformer (ViT), to perform image-to-image mapping. Our model is trained from scratch solely on ARC data and generalizes to unseen tasks through test-time training. Our framework, termed Vision ARC (VARC), achieves 60.4% accuracy on the ARC-1 benchmark, substantially outperforming existing methods that are also trained from scratch. Our results are competitive with those of leading LLMs and close the gap to average human performance.",
    "link": "https://arxiv.org/abs/2511.14761"
  },
  {
    "id": "deepseek-v32-pushing-the-frontier-of-open-large-la-17ccb53a",
    "title": "DeepSeek-V3.2: Pushing the Frontier of Open Large Language Models",
    "summary": "We introduce DeepSeek-V3.2, a model that harmonizes high computational efficiency with superior reasoning and agent performance. The key technical breakthroughs of DeepSeek-V3.2 are as follows: (1) DeepSeek Sparse Attention (DSA): We introduce DSA, an efficient attention mechanism that substantially reduces computational complexity while preserving model performance in long-context scenarios. (2) Scalable Reinforcement Learning Framework: By implementing a robust reinforcement learning protocol and scaling post-training compute, DeepSeek-V3.2 performs comparably to GPT-5. Notably, our high-compute variant, DeepSeek-V3.2-Speciale, surpasses GPT-5 and exhibits reasoning proficiency on par with Gemini-3.0-Pro, achieving gold-medal performance in both the 2025 International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI). (3) Large-Scale Agentic Task Synthesis Pipeline: To integrate reasoning into tool-use scenarios, we developed a novel synthesis pipeline that systematically generates training data at scale. This methodology facilitates scalable agentic post-training, yielding substantial improvements in generalization and instruction-following robustness within complex, interactive environments.",
    "link": "https://arxiv.org/abs/2512.02556"
  },
  {
    "id": "qwen3-embedding-advancing-text-embedding-and-reran-f05d3227",
    "title": "Qwen3 Embedding: Advancing Text Embedding and Reranking Through Foundation Models",
    "summary": "In this work, we introduce the Qwen3 Embedding series, a significant advancement over its predecessor, the GTE-Qwen series, in text embedding and reranking capabilities, built upon the Qwen3 foundation models. Leveraging the Qwen3 LLMs' robust capabilities in multilingual text understanding and generation, our innovative multi-stage training pipeline combines large-scale unsupervised pre-training with supervised fine-tuning on high-quality datasets. Effective model merging strategies further ensure the robustness and adaptability of the Qwen3 Embedding series. During the training process, the Qwen3 LLMs serve not only as backbone models but also play a crucial role in synthesizing high-quality, rich, and diverse training data across multiple domains and languages, thus enhancing the training pipeline. The Qwen3 Embedding series offers a spectrum of model sizes (0.6B, 4B, 8B) for both embedding and reranking tasks, addressing diverse deployment scenarios where users can optimize for either efficiency or effectiveness. Empirical evaluations demonstrate that the Qwen3 Embedding series achieves state-of-the-art results across diverse benchmarks. Notably, it excels on the multilingual evaluation benchmark MTEB for text embedding, as well as in various retrieval tasks, including code retrieval, cross-lingual retrieval and multilingual retrieval. To facilitate reproducibility and promote community-driven research and development, the Qwen3 Embedding models are publicly available under the Apache 2.0 license.",
    "link": "https://arxiv.org/abs/2506.05176"
  },
  {
    "id": "stabilizing-reinforcement-learning-with-llms-formu-e9efa743",
    "title": "Stabilizing Reinforcement Learning with LLMs: Formulation and Practices",
    "summary": "This paper proposes a novel formulation for reinforcement learning (RL) with large language models, explaining why and under what conditions the true sequence-level reward can be optimized via a surrogate token-level objective in policy gradient methods such as REINFORCE. Specifically, through a first-order approximation, we show that this surrogate becomes increasingly valid only when both the training-inference discrepancy and policy staleness are minimized. This insight provides a principled explanation for the crucial role of several widely adopted techniques in stabilizing RL training, including importance sampling correction, clipping, and particularly Routing Replay for Mixture-of-Experts (MoE) models. Through extensive experiments with a 30B MoE model totaling hundreds of thousands of GPU hours, we show that for on-policy training, the basic policy gradient algorithm with importance sampling correction achieves the highest training stability. When off-policy updates are introduced to accelerate convergence, combining clipping and Routing Replay becomes essential to mitigate the instability caused by policy staleness. Notably, once training is stabilized, prolonged optimization consistently yields comparable final performance regardless of cold-start initialization. We hope that the shared insights and the developed recipes for stable RL training will facilitate future research.",
    "link": "https://arxiv.org/abs/2512.01374"
  },
  {
    "id": "intellect-3-technical-report-4de9863f",
    "title": "INTELLECT-3: Technical Report",
    "summary": "We present INTELLECT-3, a 106B-parameter Mixture-of-Experts model (12B active) trained with large-scale reinforcement learning on our end-to-end RL infrastructure stack. INTELLECT-3 achieves state of the art performance for its size across math, code, science and reasoning benchmarks, outperforming many larger frontier models. We open-source the model together with the full infrastructure stack used to create it, including RL frameworks, complete recipe, and a wide collection of environments, built with the verifiers library, for training and evaluation from our Environments Hub community platform. Built for this effort, we introduce prime-rl, an open framework for large-scale asynchronous reinforcement learning, which scales seamlessly from a single node to thousands of GPUs, and is tailored for agentic RL with first-class support for multi-turn interactions and tool use. Using this stack, we run both SFT and RL training on top of the GLM-4.5-Air-Base model, scaling RL training up to 512 H200s with high training efficiency.",
    "link": "https://arxiv.org/abs/2512.16144"
  },
  {
    "id": "stabilizing-moe-reinforcement-learning-by-aligning-7aafcd1f",
    "title": "Stabilizing MoE Reinforcement Learning by Aligning Training and Inference Routers",
    "summary": "Reinforcement learning (RL) has emerged as a crucial approach for enhancing the capabilities of large language models. However, in Mixture-of-Experts (MoE) models, the routing mechanism often introduces instability, even leading to catastrophic RL training collapse. We analyze the training-inference consistency of MoE models and identify a notable discrepancy in routing behaviors between the two phases. Moreover, even under identical conditions, the routing framework can yield divergent expert selections across repeated forward passes. To address this foundational inconsistency, we propose Rollout Routing Replay (R3), a method that records routing distributions from the inference engine and replays them during training. R3 significantly reduces training-inference policy KL divergence and mitigates extreme discrepancies without compromising training speed. Extensive experiments on various settings confirm that R3 succeeds in stabilizing RL training, preventing collapse and outperforming methods such as GSPO and TIS. We believe this work can offer a new solution for stabilizing RL in MoE models.",
    "link": "https://arxiv.org/abs/2510.11370"
  },
  {
    "id": "how-to-correctly-report-llm-as-a-judge-evaluations-73672198",
    "title": "How to Correctly Report LLM-as-a-Judge Evaluations",
    "summary": "Large language models (LLMs) are increasingly used as evaluators in lieu of humans. While scalable, their judgments are noisy due to imperfect specificity and sensitivity of LLMs, leading to biased accuracy estimates. Although bias-correction methods exist, they are underutilized in LLM research and typically assume exact knowledge of the model's specificity and sensitivity. Furthermore, in general we only have estimates of these values and it is not well known how to properly construct confidence intervals using only estimates. This work presents a simple plug-in framework that corrects such bias and constructs confidence intervals reflecting uncertainty from both test and calibration dataset, enabling practical and statistically sound LLM-based evaluation. Additionally, to reduce uncertainty in the accuracy estimate, we introduce an adaptive algorithm that efficiently allocates calibration sample sizes.",
    "link": "https://arxiv.org/abs/2511.21140"
  },
  {
    "id": "on-policy-distillation-of-language-models-learning-a46b3ff1",
    "title": "On-Policy Distillation of Language Models: Learning from Self-Generated Mistakes",
    "summary": "Knowledge distillation (KD) is widely used for compressing a teacher model to reduce its inference cost and memory footprint, by training a smaller student model. However, current KD methods for auto-regressive sequence models suffer from distribution mismatch between output sequences seen during training and those generated by the student during inference. To address this issue, we introduce Generalized Knowledge Distillation (GKD). Instead of solely relying on a fixed set of output sequences, GKD trains the student on its self-generated output sequences by leveraging feedback from the teacher on such sequences. Unlike supervised KD approaches, GKD also offers the flexibility to employ alternative loss functions between the student and teacher, which can be useful when the student lacks the expressivity to mimic the teacher's distribution. Furthermore, GKD facilitates the seamless integration of distillation with RL fine-tuning (RLHF). We demonstrate the efficacy of GKD for distilling auto-regressive language models on summarization, translation, and arithmetic reasoning tasks, and task-agnostic distillation for instruction-tuning.",
    "link": "https://arxiv.org/abs/2306.13649"
  },
  {
    "id": "motif-2-127b-technical-report-20a26ddd",
    "title": "Motif 2 12.7B technical report",
    "summary": "We introduce Motif-2-12.7B, a new open-weight foundation model that pushes the efficiency frontier of large language models by combining architectural innovation with system-level optimization. Designed for scalable language understanding and robust instruction generalization under constrained compute budgets, Motif-2-12.7B builds upon Motif-2.6B with the integration of Grouped Differential Attention (GDA), which improves representational efficiency by disentangling signal and noise-control attention pathways. The model is pre-trained on 5.5 trillion tokens spanning diverse linguistic, mathematical, scientific, and programming domains using a curriculum-driven data scheduler that gradually changes the data composition ratio. The training system leverages the MuonClip optimizer alongside custom high-performance kernels, including fused PolyNorm activations and the Parallel Muon algorithm, yielding significant throughput and memory efficiency gains in large-scale distributed environments. Post-training employs a three-stage supervised fine-tuning pipeline that successively enhances general instruction adherence, compositional understanding, and linguistic precision. Motif-2-12.7B demonstrates competitive performance across diverse benchmarks, showing that thoughtful architectural scaling and optimized training design can rival the capabilities of much larger models.",
    "link": "https://arxiv.org/abs/2511.07464"
  },
  {
    "id": "motif-2-127b-reasoning-a-practitioners-guide-to-rl-4982fb63",
    "title": "Motif-2-12.7B-Reasoning: A Practitioner's Guide to RL Training Recipes",
    "summary": "We introduce Motif-2-12.7B-Reasoning, a 12.7B parameter language model designed to bridge the gap between open-weight systems and proprietary frontier models in complex reasoning and long-context understanding. Addressing the common challenges of model collapse and training instability in reasoning adaptation, we propose a comprehensive, reproducible training recipe spanning system, data, and algorithmic optimizations. Our approach combines memory-efficient infrastructure for 64K-token contexts using hybrid parallelism and kernel-level optimizations with a two-stage Supervised Fine-Tuning (SFT) curriculum that mitigates distribution mismatch through verified, aligned synthetic data. Furthermore, we detail a robust Reinforcement Learning Fine-Tuning (RLFT) pipeline that stabilizes training via difficulty-aware data filtering and mixed-policy trajectory reuse. Empirical results demonstrate that Motif-2-12.7B-Reasoning achieves performance comparable to models with significantly larger parameter counts across mathematics, coding, and agentic benchmarks, offering the community a competitive open model and a practical blueprint for scaling reasoning capabilities under realistic compute constraints.",
    "link": "https://arxiv.org/abs/2512.11463"
  },
  {
    "id": "training-llms-with-mxfp4-c1052052",
    "title": "Training LLMs with MXFP4",
    "summary": "Low precision (LP) datatypes such as MXFP4 can accelerate matrix multiplications (GEMMs) and reduce training costs. However, directly using MXFP4 instead of BF16 during training significantly degrades model quality. In this work, we present the first near-lossless training recipe that uses MXFP4 GEMMs, which are $2\\times$ faster than FP8 on supported hardware. Our key insight is to compute unbiased gradient estimates with stochastic rounding (SR), resulting in more accurate model updates. However, directly applying SR to MXFP4 can result in high variance from block-level outliers, harming convergence. To overcome this, we use the random Hadamard tranform to theoretically bound the variance of SR. We train GPT models up to 6.7B parameters and find that our method induces minimal degradation over mixed-precision BF16 training. Our recipe computes $>1/2$ the training FLOPs in MXFP4, enabling an estimated speedup of $>1.3\\times$ over FP8 and $>1.7\\times$ over BF16 during backpropagation.",
    "link": "https://arxiv.org/abs/2502.20586"
  },
  {
    "id": "quartet-native-fp4-training-can-be-optimal-for-lar-69e77a8c",
    "title": "Quartet: Native FP4 Training Can Be Optimal for Large Language Models",
    "summary": "Training large language models (LLMs) models directly in low-precision offers a way to address computational costs by improving both throughput and energy efficiency. For those purposes, NVIDIA's recent Blackwell architecture facilitates very low-precision operations using FP4 variants. Yet, current algorithms for training LLMs in FP4 precision face significant accuracy degradation and often rely on mixed-precision fallbacks. In this paper, we investigate hardware-supported FP4 training and introduce a new approach for accurate, end-to-end FP4 training with all the major computations (i.e., linear layers) in low precision. Through extensive evaluations on Llama-type models, we reveal a new low-precision scaling law that quantifies performance trade-offs across bit-widths and training setups. Guided by this investigation, we design an \"optimal\" technique in terms of accuracy-vs-computation, called Quartet. We implement Quartet using optimized CUDA kernels tailored for Blackwell, demonstrating that fully FP4-based training is a competitive alternative to FP16 half-precision and to FP8 training. Our code is available at https://github.com/IST-DASLab/Quartet.",
    "link": "https://arxiv.org/abs/2505.14669"
  },
  {
    "id": "towards-greater-leverage-scaling-laws-for-efficien-f8fa3dc9",
    "title": "Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models",
    "summary": "Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large Language Models (LLMs) efficiently by decoupling total parameters from computational cost. However, this decoupling creates a critical challenge: predicting the model capacity of a given MoE configurations (e.g., expert activation ratio and granularity) remains an unresolved problem. To address this gap, we introduce Efficiency Leverage (EL), a metric quantifying the computational advantage of an MoE model over a dense equivalent. We conduct a large-scale empirical study, training over 300 models up to 28B parameters, to systematically investigate the relationship between MoE architectural configurations and EL. Our findings reveal that EL is primarily driven by the expert activation ratio and the total compute budget, both following predictable power laws, while expert granularity acts as a non-linear modulator with a clear optimal range. We integrate these discoveries into a unified scaling law that accurately predicts the EL of an MoE architecture based on its configuration. To validate our derived scaling laws, we designed and trained Ling-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active parameters, alongside a 6.1B dense model for comparison. When trained on an identical 1T high-quality token dataset, Ling-mini-beta matched the performance of the 6.1B dense model while consuming over 7x fewer computational resources, thereby confirming the accuracy of our scaling laws. This work provides a principled and empirically-grounded foundation for the scaling of efficient MoE models.",
    "link": "https://arxiv.org/abs/2507.17702"
  },
  {
    "id": "glm-45-agentic-reasoning-and-coding-arc-foundation-e0bb7a16",
    "title": "GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models",
    "summary": "We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5.",
    "link": "https://arxiv.org/abs/2508.06471"
  },
  {
    "id": "minimax-m1-scaling-test-time-compute-efficiently-w-2547bf8b",
    "title": "MiniMax-M1: Scaling Test-Time Compute Efficiently with Lightning Attention",
    "summary": "We introduce MiniMax-M1, the world's first open-weight, large-scale hybrid-attention reasoning model. MiniMax-M1 is powered by a hybrid Mixture-of-Experts (MoE) architecture combined with a lightning attention mechanism. The model is developed based on our previous MiniMax-Text-01 model, which contains a total of 456 billion parameters with 45.9 billion parameters activated per token. The M1 model natively supports a context length of 1 million tokens, 8x the context size of DeepSeek R1. Furthermore, the lightning attention mechanism in MiniMax-M1 enables efficient scaling of test-time compute. These properties make M1 particularly suitable for complex tasks that require processing long inputs and thinking extensively. MiniMax-M1 is trained using large-scale reinforcement learning (RL) on diverse problems including sandbox-based, real-world software engineering environments. In addition to M1's inherent efficiency advantage for RL training, we propose CISPO, a novel RL algorithm to further enhance RL efficiency. CISPO clips importance sampling weights rather than token updates, outperforming other competitive RL variants. Combining hybrid-attention and CISPO enables MiniMax-M1's full RL training on 512 H800 GPUs to complete in only three weeks, with a rental cost of just $534,700. We release two versions of MiniMax-M1 models with 40K and 80K thinking budgets respectively, where the 40K model represents an intermediate phase of the 80K training. Experiments on standard benchmarks show that our models are comparable or superior to strong open-weight models such as the original DeepSeek-R1 and Qwen3-235B, with particular strengths in complex software engineering, tool utilization, and long-context tasks. We publicly release MiniMax-M1 at https://github.com/MiniMax-AI/MiniMax-M1.",
    "link": "https://arxiv.org/abs/2506.13585"
  },
  {
    "id": "longcat-flash-technical-report-41efe649",
    "title": "LongCat-Flash Technical Report",
    "summary": "We introduce LongCat-Flash, a 560-billion-parameter Mixture-of-Experts (MoE) language model designed for both computational efficiency and advanced agentic capabilities. Stemming from the need for scalable efficiency, LongCat-Flash adopts two novel designs: (a) Zero-computation Experts, which enables dynamic computational budget allocation and activates 18.6B-31.3B (27B on average) per token depending on contextual demands, optimizing resource usage. (b) Shortcut-connected MoE, which enlarges the computation-communication overlap window, demonstrating notable gains in inference efficiency and throughput compared to models of a comparable scale. We develop a comprehensive scaling framework for large models that combines hyperparameter transfer, model-growth initialization, a multi-pronged stability suite, and deterministic computation to achieve stable and reproducible training. Notably, leveraging the synergy among scalable architectural design and infrastructure efforts, we complete model training on more than 20 trillion tokens within 30 days, while achieving over 100 tokens per second (TPS) for inference at a cost of \\$0.70 per million output tokens. To cultivate LongCat-Flash towards agentic intelligence, we conduct a large-scale pre-training on optimized mixtures, followed by targeted mid- and post-training on reasoning, code, and instructions, with further augmentation from synthetic data and tool use tasks. Comprehensive evaluations demonstrate that, as a non-thinking foundation model, LongCat-Flash delivers highly competitive performance among other leading models, with exceptional strengths in agentic tasks. The model checkpoint of LongCat-Flash is open-sourced to foster community research.\n  LongCat Chat: https://longcat.ai\n  Hugging Face: https://huggingface.co/meituan-longcat\n  GitHub: https://github.com/meituan-longcat",
    "link": "https://arxiv.org/abs/2509.01322"
  },
  {
    "id": "every-step-evolves-scaling-reinforcement-learning--96fdec8d",
    "title": "Every Step Evolves: Scaling Reinforcement Learning for Trillion-Scale Thinking Model",
    "summary": "We present Ring-1T, the first open-source, state-of-the-art thinking model with a trillion-scale parameter. It features 1 trillion total parameters and activates approximately 50 billion per token. Training such models at a trillion-parameter scale introduces unprecedented challenges, including train-inference misalignment, inefficiencies in rollout processing, and bottlenecks in the RL system. To address these, we pioneer three interconnected innovations: (1) IcePop stabilizes RL training via token-level discrepancy masking and clipping, resolving instability from training-inference mismatches; (2) C3PO++ improves resource utilization for long rollouts under a token budget by dynamically partitioning them, thereby obtaining high time efficiency; and (3) ASystem, a high-performance RL framework designed to overcome the systemic bottlenecks that impede trillion-parameter model training. Ring-1T delivers breakthrough results across critical benchmarks: 93.4 on AIME-2025, 86.72 on HMMT-2025, 2088 on CodeForces, and 55.94 on ARC-AGI-1. Notably, it attains a silver medal-level result on the IMO-2025, underscoring its exceptional reasoning capabilities. By releasing the complete 1T parameter MoE model to the community, we provide the research community with direct access to cutting-edge reasoning capabilities. This contribution marks a significant milestone in democratizing large-scale reasoning intelligence and establishes a new baseline for open-source model performance.",
    "link": "https://arxiv.org/abs/2510.18855"
  },
  {
    "id": "root-mean-square-layer-normalization-37b54583",
    "title": "Root Mean Square Layer Normalization",
    "summary": "Layer normalization (LayerNorm) has been successfully applied to various deep neural networks to help stabilize training and boost model convergence because of its capability in handling re-centering and re-scaling of both inputs and weight matrix. However, the computational overhead introduced by LayerNorm makes these improvements expensive and significantly slows the underlying network, e.g. RNN in particular. In this paper, we hypothesize that re-centering invariance in LayerNorm is dispensable and propose root mean square layer normalization, or RMSNorm. RMSNorm regularizes the summed inputs to a neuron in one layer according to root mean square (RMS), giving the model re-scaling invariance property and implicit learning rate adaptation ability. RMSNorm is computationally simpler and thus more efficient than LayerNorm. We also present partial RMSNorm, or pRMSNorm where the RMS is estimated from p% of the summed inputs without breaking the above properties. Extensive experiments on several tasks using diverse network architectures show that RMSNorm achieves comparable performance against LayerNorm but reduces the running time by 7%~64% on different models. Source code is available at https://github.com/bzhangGo/rmsnorm.",
    "link": "https://arxiv.org/abs/1910.07467"
  },
  {
    "id": "zero-memory-optimizations-toward-training-trillion-09cb83db",
    "title": "ZeRO: Memory Optimizations Toward Training Trillion Parameter Models",
    "summary": "Large deep learning models offer significant accuracy gains, but training billions to trillions of parameters is challenging. Existing solutions such as data and model parallelisms exhibit fundamental limitations to fit these models into limited device memory, while obtaining computation, communication and development efficiency. We develop a novel solution, Zero Redundancy Optimizer (ZeRO), to optimize memory, vastly improving training speed while increasing the model size that can be efficiently trained. ZeRO eliminates memory redundancies in data- and model-parallel training while retaining low communication volume and high computational granularity, allowing us to scale the model size proportional to the number of devices with sustained high efficiency. Our analysis on memory requirements and communication volume demonstrates: ZeRO has the potential to scale beyond 1 Trillion parameters using today's hardware.\n  We implement and evaluate ZeRO: it trains large models of over 100B parameter with super-linear speedup on 400 GPUs, achieving throughput of 15 Petaflops. This represents an 8x increase in model size and 10x increase in achievable performance over state-of-the-art. In terms of usability, ZeRO can train large models of up to 13B parameters (e.g., larger than Megatron GPT 8.3B and T5 11B) without requiring model parallelism which is harder for scientists to apply. Last but not the least, researchers have used the system breakthroughs of ZeRO to create the world's largest language model (Turing-NLG, 17B parameters) with record breaking accuracy.",
    "link": "https://arxiv.org/abs/1910.02054v3"
  },
  {
    "id": "hierarchical-reasoning-model-604d21b7",
    "title": "Hierarchical Reasoning Model",
    "summary": "Reasoning, the process of devising and executing complex goal-oriented action sequences, remains a critical challenge in AI. Current large language models (LLMs) primarily employ Chain-of-Thought (CoT) techniques, which suffer from brittle task decomposition, extensive data requirements, and high latency. Inspired by the hierarchical and multi-timescale processing in the human brain, we propose the Hierarchical Reasoning Model (HRM), a novel recurrent architecture that attains significant computational depth while maintaining both training stability and efficiency. HRM executes sequential reasoning tasks in a single forward pass without explicit supervision of the intermediate process, through two interdependent recurrent modules: a high-level module responsible for slow, abstract planning, and a low-level module handling rapid, detailed computations. With only 27 million parameters, HRM achieves exceptional performance on complex reasoning tasks using only 1000 training samples. The model operates without pre-training or CoT data, yet achieves nearly perfect performance on challenging tasks including complex Sudoku puzzles and optimal path finding in large mazes. Furthermore, HRM outperforms much larger models with significantly longer context windows on the Abstraction and Reasoning Corpus (ARC), a key benchmark for measuring artificial general intelligence capabilities. These results underscore HRM's potential as a transformative advancement toward universal computation and general-purpose reasoning systems.",
    "link": "https://arxiv.org/abs/2506.21734"
  },
  {
    "id": "towards-fully-fp8-gemm-llm-training-at-scale-9b105f50",
    "title": "Towards Fully FP8 GEMM LLM Training at Scale",
    "summary": "Despite the significant potential of FP8 data formats for large language model (LLM) pre-training, their adoption has been limited due to challenges in maintaining stability at scale. Existing approaches often rely on suboptimal fine-grained FP8 kernels or fall back to higher-precision matrix multiplications (GEMMs) in sensitive components, such as attention projections, compromising potential throughput gains. We introduce a new class of LLM architectures that, for the first time, support FP8 computation for all GEMMs within transformer blocks during both forward and backward passes. This enables unprecedented throughput gains, particularly at scale, while matching the downstream performance of standard BF16 training. Our architecture design reduces large outlier activations, promoting stable long-term FP8 training. In addition, we identify key metrics to monitor low-precision training and predict potential future divergences.",
    "link": "https://arxiv.org/abs/2505.20524"
  },
  {
    "id": "scaling-fp8-training-to-trillion-token-llms-0a1684aa",
    "title": "Scaling FP8 training to trillion-token LLMs",
    "summary": "We train, for the first time, large language models using FP8 precision on datasets up to 2 trillion tokens -- a 20-fold increase over previous limits. Through these extended training runs, we uncover critical instabilities in FP8 training that were not observable in earlier works with shorter durations. We trace these instabilities to outlier amplification by the SwiGLU activation function. Interestingly, we show, both analytically and empirically, that this amplification happens only over prolonged training periods, and link it to a SwiGLU weight alignment process. To address this newly identified issue, we introduce Smooth-SwiGLU, a novel modification that ensures stable FP8 training without altering function behavior. We also demonstrate, for the first time, FP8 quantization of both Adam optimizer moments. Combining these innovations, we successfully train a 7B parameter model using FP8 precision on 256 Intel Gaudi2 accelerators, achieving on-par results with the BF16 baseline while delivering up to a $\\sim 34 \\%$ throughput improvement. A reference implementation is supplied in https://github.com/Anonymous1252022/Megatron-DeepSpeed.",
    "link": "https://arxiv.org/abs/2409.12517"
  },
  {
    "id": "an-inquiry-into-datacenter-tco-for-llm-inference-w-cab55b1a",
    "title": "An Inquiry into Datacenter TCO for LLM Inference with FP8",
    "summary": "As large language models (LLMs) continue to scale, the high power consumption of AI accelerators in datacenters presents significant challenges, substantially increasing the total cost of ownership (TCO) for cloud service providers (CSPs) that provide LLM inference. In this work, we analyze the computational characteristics of LLM inference from a TCO perspective and present a generalizable framework to compare AI accelerators across diverse operational requirements. Using this model, we investigate key workload characteristics influencing TCO for AI accelerators from Intel (Gaudi 2 & 3) and NVIDIA (H100 & H200), especially thin GEMM utilization and FP8 quantization. In particular, as FP8 emerges as the baseline precision for next-generation LLMs, understanding how different architectures implement and benefit from low-precision computation is increasingly critical. Throughput on thin GEMMs has a greater impact on TCO than theoretical hardware peak throughput because the memory-bound decode phase is dominated by GEMV-like computations. We find that Gaudi HPUs achieve superior utilization on thin GEMMs compared to their counterparts, especially in FP8-quantized models. Our result underscores the importance of empirical, workload-level analysis in evaluating accelerator performance, rather than relying solely on theoretical hardware specifications. By studying the interaction between power consumption, quantization strategies, and hardware architecture, we provide insights to support informed deployment decisions and guide future accelerator designs aimed at improving the TCO of LLM inference workloads.",
    "link": "https://arxiv.org/abs/2502.01070v1"
  },
  {
    "id": "fp8-lm-training-fp8-large-language-models-05a26396",
    "title": "FP8-LM: Training FP8 Large Language Models",
    "summary": "In this paper, we explore FP8 low-bit data formats for efficient training of large language models (LLMs). Our key insight is that most variables, such as gradients and optimizer states, in LLM training can employ low-precision data formats without compromising model accuracy and requiring no changes to hyper-parameters. Specifically, we propose a new FP8 automatic mixed-precision framework for training LLMs. This framework offers three levels of FP8 utilization to streamline mixed-precision and distributed parallel training for LLMs. It gradually incorporates 8-bit gradients, optimizer states, and distributed learning in an incremental manner. Experiment results show that, during the training of GPT-175B model on H100 GPU platform, our FP8 mixed-precision training framework not only achieved a remarkable 39% reduction in real memory usage but also ran 75% faster than the widely adopted BF16 framework (i.e., Megatron-LM), surpassing the speed of Nvidia Transformer Engine by 37%. This largely reduces the training costs for large foundation models. Furthermore, our FP8 mixed-precision training methodology is generic. It can be seamlessly applied to other tasks such as LLM instruction tuning and reinforcement learning with human feedback, offering savings in fine-tuning expenses. Our FP8 low-precision training framework is open-sourced at {https://github.com/Azure/MS-AMP}{aka.ms/MS.AMP}.",
    "link": "https://arxiv.org/abs/2310.18313"
  },
  {
    "id": "defeating-the-training-inference-mismatch-via-fp16-79636a16",
    "title": "Defeating the Training-Inference Mismatch via FP16",
    "summary": "Reinforcement learning (RL) fine-tuning of large language models (LLMs) often suffers from instability due to the numerical mismatch between the training and inference policies. While prior work has attempted to mitigate this issue through algorithmic corrections or engineering alignments, we show that its root cause lies in the floating point precision itself. The widely adopted BF16, despite its large dynamic range, introduces large rounding errors that breaks the consistency between training and inference. In this work, we demonstrate that simply reverting to \\textbf{FP16} effectively eliminates this mismatch. The change is simple, fully supported by modern frameworks with only a few lines of code change, and requires no modification to the model architecture or learning algorithm. Our results suggest that using FP16 uniformly yields more stable optimization, faster convergence, and stronger performance across diverse tasks, algorithms and frameworks. We hope these findings motivate a broader reconsideration of precision trade-offs in RL fine-tuning.",
    "link": "https://arxiv.org/abs/2510.26788"
  },
  {
    "id": "every-activation-boosted-scaling-general-reasoner--be51c673",
    "title": "Every Activation Boosted: Scaling General Reasoner to 1 Trillion Open Language Foundation",
    "summary": "We introduce Ling 2.0, a series reasoning-oriented language foundation built upon the principle that every activation boosts reasoning capability. Designed to scale from tens of billions to one trillion parameters under a unified Mixture-of-Experts (MoE) paradigm, Ling 2.0 emphasizes high sparsity, cross-scale consistency, and efficiency guided by empirical scaling laws. The series includes three non-thinking (instruct) models - Ling-mini-2.0, Ling-flash-2.0, and Ling-1T - ranging from 16B to 1T total parameters and achieving up to 7-fold active-compute efficiency compared with dense counterparts. Ling 2.0 integrates coordinated innovations across model architecture, pre-training, post-training, and infrastructure: a high-sparsity MoE with MTP for efficient reasoning, reasoning-oriented data and mid-training CoT activation, reinforcement-based fine-tuning (DFT, Evo-CoT), and full-scale FP8 training with fine-grained heterogeneous pipelines. At the trillion scale, Ling-1T establishes a new Pareto frontier of reasoning accuracy versus computational efficiency, demonstrating that sparse activation, when properly aligned with reasoning objectives, enables scalable and efficient intelligence. Collectively, Ling 2.0 provides a coherent, open, and efficient foundation for advancing future reasoning and thinking models, including the Ring series built upon the same base.",
    "link": "https://arxiv.org/abs/2510.22115"
  },
  {
    "id": "kimi-k2-open-agentic-intelligence-e0e90c37",
    "title": "Kimi K2: Open Agentic Intelligence",
    "summary": "We introduce Kimi K2, a Mixture-of-Experts (MoE) large language model with 32 billion activated parameters and 1 trillion total parameters. We propose the MuonClip optimizer, which improves upon Muon with a novel QK-clip technique to address training instability while enjoying the advanced token efficiency of Muon. Based on MuonClip, K2 was pre-trained on 15.5 trillion tokens with zero loss spike. During post-training, K2 undergoes a multi-stage post-training process, highlighted by a large-scale agentic data synthesis pipeline and a joint reinforcement learning (RL) stage, where the model improves its capabilities through interactions with real and synthetic environments.\n  Kimi K2 achieves state-of-the-art performance among open-source non-thinking models, with strengths in agentic capabilities. Notably, K2 obtains 66.1 on Tau2-Bench, 76.5 on ACEBench (En), 65.8 on SWE-Bench Verified, and 47.3 on SWE-Bench Multilingual -- surpassing most open and closed-sourced baselines in non-thinking settings. It also exhibits strong capabilities in coding, mathematics, and reasoning tasks, with a score of 53.7 on LiveCodeBench v6, 49.5 on AIME 2025, 75.1 on GPQA-Diamond, and 27.1 on OJBench, all without extended thinking. These results position Kimi K2 as one of the most capable open-source large language models to date, particularly in software engineering and agentic tasks. We release our base and post-trained model checkpoints to facilitate future research and applications of agentic intelligence.",
    "link": "https://arxiv.org/abs/2507.20534"
  },
  {
    "id": "longdocurl-a-comprehensive-multimodal-long-documen-b177f1cc",
    "title": "LongDocURL: a Comprehensive Multimodal Long Document Benchmark Integrating Understanding, Reasoning, and Locating",
    "summary": "Large vision language models (LVLMs) have improved the document understanding capabilities remarkably, enabling the handling of complex document elements, longer contexts, and a wider range of tasks. However, existing document understanding benchmarks have been limited to handling only a small number of pages and fail to provide a comprehensive analysis of layout elements locating. In this paper, we first define three primary task categories: Long Document Understanding, numerical Reasoning, and cross-element Locating, and then propose a comprehensive benchmark, LongDocURL, integrating above three primary tasks and comprising 20 sub-tasks categorized based on different primary tasks and answer evidences. Furthermore, we develop a semi-automated construction pipeline and collect 2,325 high-quality question-answering pairs, covering more than 33,000 pages of documents, significantly outperforming existing benchmarks. Subsequently, we conduct comprehensive evaluation experiments on both open-source and closed-source models across 26 different configurations, revealing critical performance gaps in this field.",
    "link": "https://arxiv.org/abs/2412.18424"
  },
  {
    "id": "mmlongbench-benchmarking-long-context-vision-langu-da4e589a",
    "title": "MMLongBench: Benchmarking Long-Context Vision-Language Models Effectively and Thoroughly",
    "summary": "The rapid extension of context windows in large vision-language models has given rise to long-context vision-language models (LCVLMs), which are capable of handling hundreds of images with interleaved text tokens in a single forward pass. In this work, we introduce MMLongBench, the first benchmark covering a diverse set of long-context vision-language tasks, to evaluate LCVLMs effectively and thoroughly. MMLongBench is composed of 13,331 examples spanning five different categories of downstream tasks, such as Visual RAG and Many-Shot ICL. It also provides broad coverage of image types, including various natural and synthetic images. To assess the robustness of the models to different input lengths, all examples are delivered at five standardized input lengths (8K-128K tokens) via a cross-modal tokenization scheme that combines vision patches and text tokens. Through a thorough benchmarking of 46 closed-source and open-source LCVLMs, we provide a comprehensive analysis of the current models' vision-language long-context ability. Our results show that: i) performance on a single task is a weak proxy for overall long-context capability; ii) both closed-source and open-source models face challenges in long-context vision-language tasks, indicating substantial room for future improvement; iii) models with stronger reasoning ability tend to exhibit better long-context performance. By offering wide task coverage, various image types, and rigorous length control, MMLongBench provides the missing foundation for diagnosing and advancing the next generation of LCVLMs.",
    "link": "https://arxiv.org/abs/2505.10610"
  },
  {
    "id": "mmlongbench-doc-benchmarking-long-context-document-ae7c8f92",
    "title": "MMLongBench-Doc: Benchmarking Long-context Document Understanding with Visualizations",
    "summary": "Understanding documents with rich layouts and multi-modal components is a long-standing and practical task. Recent Large Vision-Language Models (LVLMs) have made remarkable strides in various tasks, particularly in single-page document understanding (DU). However, their abilities on long-context DU remain an open problem. This work presents MMLongBench-Doc, a long-context, multi-modal benchmark comprising 1,062 expert-annotated questions. Distinct from previous datasets, it is constructed upon 130 lengthy PDF-formatted documents with an average of 49.4 pages and 20,971 textual tokens. Towards comprehensive evaluation, answers to these questions rely on pieces of evidence from (1) different sources (text, image, chart, table, and layout structure) and (2) various locations (i.e. page number). Moreover, 33.2% of the questions are cross-page questions requiring evidence across multiple pages. 22.8% of the questions are designed to be unanswerable for detecting potential hallucinations. Experiments on 14 LVLMs demonstrate that long-context DU greatly challenges current models. Notably, the best-performing model, GPT-4o, achieves an F1 score of only 42.7%, while the second-best, GPT-4V, scores 31.4%. Furthermore, 12 LVLMs (all except GPT-4o and GPT-4V) even present worse performance than their LLM counterparts which are fed with lossy-parsed OCR documents. These results validate the necessity of future research toward more capable long-context LVLMs. Project Page: https://mayubo2333.github.io/MMLongBench-Doc",
    "link": "https://arxiv.org/abs/2407.01523"
  },
  {
    "id": "test-time-adaptation-of-tiny-recursive-models-1f24ed73",
    "title": "Test-time Adaptation of Tiny Recursive Models",
    "summary": "Prior to the close of the 2025 ARC Prize competition, the leading open source approach - known as TRM, or Tiny Recursive Models - involved training a 7M parameter recursive neural network on augmented variants of ARC tasks. That approach scored approximately 7.8% on the public ARC AGI II evaluation set, but required a level of compute far in excess of what is allowed during the competition. This paper shows that, by starting from a tiny recursive model that has been pre-trained on public ARC tasks, one can efficiently fine-tune on competition tasks within the allowed compute limits. Specifically, a model was pre-trained on 1,280 public tasks for 700k+ optimizer steps over 48 hours on 4xH100 SXM GPUs to obtain a ~10% score on the public evaluation set. That model was then post-trained in just 12,500 gradient steps during the competition to reach a score of 6.67% on semi-private evaluation tasks. Notably, such post-training performance is achieved by full-fine tuning of the tiny model, not LoRA fine-tuning or fine-tuning of task embeddings alone.",
    "link": "https://arxiv.org/abs/2511.02886"
  },
  {
    "id": "less-is-more-recursive-reasoning-with-tiny-network-01696993",
    "title": "Less is More: Recursive Reasoning with Tiny Networks",
    "summary": "Hierarchical Reasoning Model (HRM) is a novel approach using two small neural networks recursing at different frequencies. This biologically inspired method beats Large Language models (LLMs) on hard puzzle tasks such as Sudoku, Maze, and ARC-AGI while trained with small models (27M parameters) on small data (around 1000 examples). HRM holds great promise for solving hard problems with small networks, but it is not yet well understood and may be suboptimal. We propose Tiny Recursive Model (TRM), a much simpler recursive reasoning approach that achieves significantly higher generalization than HRM, while using a single tiny network with only 2 layers. With only 7M parameters, TRM obtains 45% test-accuracy on ARC-AGI-1 and 8% on ARC-AGI-2, higher than most LLMs (e.g., Deepseek R1, o3-mini, Gemini 2.5 Pro) with less than 0.01% of the parameters.",
    "link": "https://arxiv.org/abs/2510.04871"
  },
  {
    "id": "why-is-spatial-reasoning-hard-for-vlms-an-attentio-5e83e074",
    "title": "Why Is Spatial Reasoning Hard for VLMs? An Attention Mechanism Perspective on Focus Areas",
    "summary": "Large Vision Language Models (VLMs) have long struggled with spatial reasoning tasks. Surprisingly, even simple spatial reasoning tasks, such as recognizing \"under\" or \"behind\" relationships between only two objects, pose significant challenges for current VLMs. In this work, we study the spatial reasoning challenge from the lens of mechanistic interpretability, diving into the model's internal states to examine the interactions between image and text tokens. By tracing attention distribution over the image through out intermediate layers, we observe that successful spatial reasoning correlates strongly with the model's ability to align its attention distribution with actual object locations, particularly differing between familiar and unfamiliar spatial relationships. Motivated by these findings, we propose ADAPTVIS based on inference-time confidence scores to sharpen the attention on highly relevant regions when confident, while smoothing and broadening the attention window to consider a wider context when confidence is lower. This training-free decoding method shows significant improvement (e.g., up to a 50 absolute point improvement) on spatial reasoning benchmarks such as WhatsUp and VSR with negligible cost. We make code and data publicly available for research purposes at https://github.com/shiqichen17/AdaptVis.",
    "link": "https://arxiv.org/abs/2503.01773"
  },
  {
    "id": "colpali-efficient-document-retrieval-with-vision-l-4ded15fa",
    "title": "ColPali: Efficient Document Retrieval with Vision Language Models",
    "summary": "Documents are visually rich structures that convey information through text, but also figures, page layouts, tables, or even fonts. Since modern retrieval systems mainly rely on the textual information they extract from document pages to index documents -often through lengthy and brittle processes-, they struggle to exploit key visual cues efficiently. This limits their capabilities in many practical document retrieval applications such as Retrieval Augmented Generation (RAG). To benchmark current systems on visually rich document retrieval, we introduce the Visual Document Retrieval Benchmark ViDoRe, composed of various page-level retrieval tasks spanning multiple domains, languages, and practical settings. The inherent complexity and performance shortcomings of modern systems motivate a new concept; doing document retrieval by directly embedding the images of the document pages. We release ColPali, a Vision Language Model trained to produce high-quality multi-vector embeddings from images of document pages. Combined with a late interaction matching mechanism, ColPali largely outperforms modern document retrieval pipelines while being drastically simpler, faster and end-to-end trainable. We release models, data, code and benchmarks under open licenses at https://hf.co/vidore.",
    "link": "https://arxiv.org/abs/2407.01449"
  },
  {
    "id": "areal-a-large-scale-asynchronous-reinforcement-lea-b17b40cd",
    "title": "AReaL: A Large-Scale Asynchronous Reinforcement Learning System for Language Reasoning",
    "summary": "Reinforcement learning (RL) has become a dominant paradigm for training large language models (LLMs), particularly for reasoning tasks. Effective RL for LLMs requires massive parallelization and poses an urgent need for efficient training systems. Most existing large-scale RL systems for LLMs are synchronous, alternating generation and training in a batch setting where rollouts in each training batch are generated by the same model. This approach stabilizes RL training but suffers from severe system-level inefficiency: generation must wait until the longest output in the batch is completed before model updates, resulting in GPU underutilization. We present AReaL, a fully asynchronous RL system that completely decouples generation from training. Rollout workers in AReaL continuously generate new outputs without waiting, while training workers update the model whenever a batch of data is collected. AReaL also incorporates a collection of system-level optimizations, leading to substantially higher GPU utilization. To stabilize RL training, AReaL balances the workload of rollout and training workers to control data staleness, and adopts a staleness-enhanced PPO variant to better handle outdated training samples. Extensive experiments on math and code reasoning benchmarks show that AReaL achieves up to 2.77$\\times$ training speedup compared to synchronous systems with the same number of GPUs and matched or improved final performance. The code of AReaL is available at https://github.com/inclusionAI/AReaL/.",
    "link": "https://arxiv.org/abs/2505.24298"
  },
  {
    "id": "the-art-of-scaling-reinforcement-learning-compute--e788c175",
    "title": "The Art of Scaling Reinforcement Learning Compute for LLMs",
    "summary": "Reinforcement learning (RL) has become central to training large language models (LLMs), yet the field lacks predictive scaling methodologies comparable to those established for pre-training. Despite rapidly rising compute budgets, there is no principled understanding of how to evaluate algorithmic improvements for scaling RL compute. We present the first large-scale systematic study, amounting to more than 400,000 GPU-hours, that defines a principled framework for analyzing and predicting RL scaling in LLMs. We fit sigmoidal compute-performance curves for RL training and ablate a wide range of common design choices to analyze their effects on asymptotic performance and compute efficiency. We observe: (1) Not all recipes yield similar asymptotic performance, (2) Details such as loss aggregation, normalization, curriculum, and off-policy algorithm primarily modulate compute efficiency without materially shifting the asymptote, and (3) Stable, scalable recipes follow predictable scaling trajectories, enabling extrapolation from smaller-scale runs. Combining these insights, we propose a best-practice recipe, ScaleRL, and demonstrate its effectiveness by successfully scaling and predicting validation performance on a single RL run scaled up to 100,000 GPU-hours. Our work provides both a scientific framework for analyzing scaling in RL and a practical recipe that brings RL training closer to the predictability long achieved in pre-training.",
    "link": "https://arxiv.org/abs/2510.13786"
  },
  {
    "id": "part-i-tricks-or-traps-a-deep-dive-into-rl-for-llm-7b17cf17",
    "title": "Part I: Tricks or Traps? A Deep Dive into RL for LLM Reasoning",
    "summary": "Reinforcement learning for LLM reasoning has rapidly emerged as a prominent research area, marked by a significant surge in related studies on both algorithmic innovations and practical applications. Despite this progress, several critical challenges remain, including the absence of standardized guidelines for employing RL techniques and a fragmented understanding of their underlying mechanisms. Additionally, inconsistent experimental settings, variations in training data, and differences in model initialization have led to conflicting conclusions, obscuring the key characteristics of these techniques and creating confusion among practitioners when selecting appropriate techniques. This paper systematically reviews widely adopted RL techniques through rigorous reproductions and isolated evaluations within a unified open-source framework. We analyze the internal mechanisms, applicable scenarios, and core principles of each technique through fine-grained experiments, including datasets of varying difficulty, model sizes, and architectures. Based on these insights, we present clear guidelines for selecting RL techniques tailored to specific setups, and provide a reliable roadmap for practitioners navigating the RL for the LLM domain. Finally, we reveal that a minimalist combination of two techniques can unlock the learning capability of critic-free policies using vanilla PPO loss. The results demonstrate that our simple combination consistently improves performance, surpassing strategies like GRPO and DAPO.",
    "link": "https://arxiv.org/abs/2508.08221"
  },
  {
    "id": "magistral-5de72bf8",
    "title": "Magistral",
    "summary": "We introduce Magistral, Mistral's first reasoning model and our own scalable reinforcement learning (RL) pipeline. Instead of relying on existing implementations and RL traces distilled from prior models, we follow a ground up approach, relying solely on our own models and infrastructure. Notably, we demonstrate a stack that enabled us to explore the limits of pure RL training of LLMs, present a simple method to force the reasoning language of the model, and show that RL on text data alone maintains most of the initial checkpoint's capabilities. We find that RL on text maintains or improves multimodal understanding, instruction following and function calling. We present Magistral Medium, trained for reasoning on top of Mistral Medium 3 with RL alone, and we open-source Magistral Small (Apache 2.0) which further includes cold-start data from Magistral Medium.",
    "link": "https://arxiv.org/abs/2506.10910"
  },
  {
    "id": "ruler-whats-the-real-context-size-of-your-long-con-9eff3185",
    "title": "RULER: What's the Real Context Size of Your Long-Context Language Models?",
    "summary": "The needle-in-a-haystack (NIAH) test, which examines the ability to retrieve a piece of information (the \"needle\") from long distractor texts (the \"haystack\"), has been widely adopted to evaluate long-context language models (LMs). However, this simple retrieval-based test is indicative of only a superficial form of long-context understanding. To provide a more comprehensive evaluation of long-context LMs, we create a new synthetic benchmark RULER with flexible configurations for customized sequence length and task complexity. RULER expands upon the vanilla NIAH test to encompass variations with diverse types and quantities of needles. Moreover, RULER introduces new task categories multi-hop tracing and aggregation to test behaviors beyond searching from context. We evaluate 17 long-context LMs with 13 representative tasks in RULER. Despite achieving nearly perfect accuracy in the vanilla NIAH test, almost all models exhibit large performance drops as the context length increases. While these models all claim context sizes of 32K tokens or greater, only half of them can maintain satisfactory performance at the length of 32K. Our analysis of Yi-34B, which supports context length of 200K, reveals large room for improvement as we increase input length and task complexity. We open source RULER to spur comprehensive evaluation of long-context LMs.",
    "link": "https://arxiv.org/abs/2404.06654"
  },
  {
    "id": "nolima-long-context-evaluation-beyond-literal-matc-05a4204f",
    "title": "NoLiMa: Long-Context Evaluation Beyond Literal Matching",
    "summary": "Recent large language models (LLMs) support long contexts ranging from 128K to 1M tokens. A popular method for evaluating these capabilities is the needle-in-a-haystack (NIAH) test, which involves retrieving a \"needle\" (relevant information) from a \"haystack\" (long irrelevant context). Extensions of this approach include increasing distractors, fact chaining, and in-context reasoning. However, in these benchmarks, models can exploit existing literal matches between the needle and haystack to simplify the task. To address this, we introduce NoLiMa, a benchmark extending NIAH with a carefully designed needle set, where questions and needles have minimal lexical overlap, requiring models to infer latent associations to locate the needle within the haystack. We evaluate 13 popular LLMs that claim to support contexts of at least 128K tokens. While they perform well in short contexts (<1K), performance degrades significantly as context length increases. At 32K, for instance, 11 models drop below 50% of their strong short-length baselines. Even GPT-4o, one of the top-performing exceptions, experiences a reduction from an almost-perfect baseline of 99.3% to 69.7%. Our analysis suggests these declines stem from the increased difficulty the attention mechanism faces in longer contexts when literal matches are absent, making it harder to retrieve relevant information. Even models enhanced with reasoning capabilities or CoT prompting struggle to maintain performance in long contexts. We publicly release the dataset and evaluation code at https://github.com/adobe-research/NoLiMa.",
    "link": "https://arxiv.org/abs/2502.05167"
  },
  {
    "id": "longbench-v2-towards-deeper-understanding-and-reas-bd15c327",
    "title": "LongBench v2: Towards Deeper Understanding and Reasoning on Realistic Long-context Multitasks",
    "summary": "This paper introduces LongBench v2, a benchmark designed to assess the ability of LLMs to handle long-context problems requiring deep understanding and reasoning across real-world multitasks. LongBench v2 consists of 503 challenging multiple-choice questions, with contexts ranging from 8k to 2M words, across six major task categories: single-document QA, multi-document QA, long in-context learning, long-dialogue history understanding, code repository understanding, and long structured data understanding. To ensure the breadth and the practicality, we collect data from nearly 100 highly educated individuals with diverse professional backgrounds. We employ both automated and manual review processes to maintain high quality and difficulty, resulting in human experts achieving only 53.7% accuracy under a 15-minute time constraint. Our evaluation reveals that the best-performing model, when directly answers the questions, achieves only 50.1% accuracy. In contrast, the o1-preview model, which includes longer reasoning, achieves 57.7%, surpassing the human baseline by 4%. These results highlight the importance of enhanced reasoning ability and scaling inference-time compute to tackle the long-context challenges in LongBench v2. The project is available at https://longbench2.github.io.",
    "link": "https://arxiv.org/abs/2412.15204"
  },
  {
    "id": "text-or-pixels-it-takes-half-on-the-token-efficien-71989f4c",
    "title": "Text or Pixels? It Takes Half: On the Token Efficiency of Visual Text Inputs in Multimodal LLMs",
    "summary": "Large language models (LLMs) and their multimodal variants can now process visual inputs, including images of text. This raises an intriguing question: can we compress textual inputs by feeding them as images to reduce token usage while preserving performance? In this paper, we show that visual text representations are a practical and surprisingly effective form of input compression for decoder LLMs. We exploit the idea of rendering long text inputs as a single image and provide it directly to the model. This leads to dramatically reduced number of decoder tokens required, offering a new form of input compression. Through experiments on two distinct benchmarks RULER (long-context retrieval) and CNN/DailyMail (document summarization) we demonstrate that this text-as-image method yields substantial token savings (often nearly half) without degrading task performance.",
    "link": "https://arxiv.org/abs/2510.18279"
  },
  {
    "id": "deepseek-ocr-contexts-optical-compression-2307c6b1",
    "title": "DeepSeek-OCR: Contexts Optical Compression",
    "summary": "We present DeepSeek-OCR as an initial investigation into the feasibility of compressing long contexts via optical 2D mapping. DeepSeek-OCR consists of two components: DeepEncoder and DeepSeek3B-MoE-A570M as the decoder. Specifically, DeepEncoder serves as the core engine, designed to maintain low activations under high-resolution input while achieving high compression ratios to ensure an optimal and manageable number of vision tokens. Experiments show that when the number of text tokens is within 10 times that of vision tokens (i.e., a compression ratio < 10x), the model can achieve decoding (OCR) precision of 97%. Even at a compression ratio of 20x, the OCR accuracy still remains at about 60%. This shows considerable promise for research areas such as historical long-context compression and memory forgetting mechanisms in LLMs. Beyond this, DeepSeek-OCR also demonstrates high practical value. On OmniDocBench, it surpasses GOT-OCR2.0 (256 tokens/page) using only 100 vision tokens, and outperforms MinerU2.0 (6000+ tokens per page on average) while utilizing fewer than 800 vision tokens. In production, DeepSeek-OCR can generate training data for LLMs/VLMs at a scale of 200k+ pages per day (a single A100-40G). Codes and model weights are publicly accessible at http://github.com/deepseek-ai/DeepSeek-OCR.",
    "link": "https://www.arxiv.org/abs/2510.18234"
  },
  {
    "id": "introducing-longcat-flash-thinking-a-technical-rep-780f3ead",
    "title": "Introducing LongCat-Flash-Thinking: A Technical Report",
    "summary": "We present LongCat-Flash-Thinking, an efficient 560-billion-parameter open-source Mixture-of-Experts (MoE) reasoning model. Its advanced capabilities are cultivated through a meticulously crafted training process, beginning with long Chain-of-Thought (CoT) data cold-start and culminating in large-scale Reinforcement Learning (RL). We first employ a well-designed cold-start training strategy, which significantly enhances the reasoning potential and equips the model with specialized skills in both formal and agentic reasoning. Then, a core innovation is our domain-parallel training scheme, which decouples optimization across distinct domains (e.g., STEM, Code, Agentic) and subsequently fuses the resulting expert models into a single, nearly Pareto-optimal model. This entire process is powered by our Dynamic ORchestration for Asynchronous rollout (DORA) system, a large-scale RL framework that delivers a greater than threefold training speedup over synchronous methods on tens of thousands of accelerators. As a result, LongCat-Flash-Thinking achieves state-of-the-art performance among open-source models on a suite of complex reasoning tasks. The model exhibits exceptional efficiency in agentic reasoning, reducing average token consumption by 64.5% (from 19, 653 to 6, 965) on AIME-25, without degrading task accuracy. We release LongCat-Flash-Thinking to promote further advances in reasoning systems and agentic AI research.",
    "link": "https://arxiv.org/abs/2509.18883"
  },
  {
    "id": "llama-nemoretriever-colembed-top-performing-text-i-b7df4642",
    "title": "Llama Nemoretriever Colembed: Top-Performing Text-Image Retrieval Model",
    "summary": "Motivated by the growing demand for retrieval systems that operate across modalities, we introduce llama-nemoretriever-colembed, a unified text-image retrieval model that delivers state-of-the-art performance across multiple benchmarks. We release two model variants, 1B and 3B. The 3B model achieves state of the art performance, scoring NDCG@5 91.0 on ViDoRe V1 and 63.5 on ViDoRe V2, placing first on both leaderboards as of June 27, 2025.\n  Our approach leverages the NVIDIA Eagle2 Vision-Language model (VLM), modifies its architecture by replacing causal attention with bidirectional attention, and integrates a ColBERT-style late interaction mechanism to enable fine-grained multimodal retrieval in a shared embedding space. While this mechanism delivers superior retrieval accuracy, it introduces trade-offs in storage and efficiency. We provide a comprehensive analysis of these trade-offs. Additionally, we adopt a two-stage training strategy to enhance the model's retrieval capabilities.",
    "link": "https://arxiv.org/abs/2507.05513"
  },
  {
    "id": "modernvbert-towards-smaller-visual-document-retrie-98faa052",
    "title": "ModernVBERT: Towards Smaller Visual Document Retrievers",
    "summary": "Retrieving specific information from a large corpus of documents is a prevalent industrial use case of modern AI, notably due to the popularity of Retrieval-Augmented Generation (RAG) systems. Although neural document retrieval models have historically operated exclusively in the text space, Visual Document Retrieval (VDR) models - large vision-language decoders repurposed as embedding models which directly work with page screenshots as inputs - are increasingly popular due to the performance and indexing latency gains they offer. In this work, we show that, while cost-efficient, this approach of repurposing generative models bottlenecks retrieval performance. Through controlled experiments, we revisit the entire training pipeline, and establish a principled recipe for improving visual document retrieval models. We notably measure the impact of attention masking, image resolution, modality alignment data regimes, and late interaction centered contrastive objectives which emerge as central performance factors. Building on these insights, we release ModernVBERT, a compact 250M-parameter vision-language encoder that outperforms recent models up to 10 times larger when fine-tuned on document retrieval tasks, enabling efficient inference on cheap CPU hardware and greatly reducing latency and costs while maintaining strong performance. Models, code and data are available at https://huggingface.co/ModernVBERT.",
    "link": "https://arxiv.org/abs/2510.01149"
  },
  {
    "id": "on-the-interplay-of-pre-training-mid-training-and--c9f91692",
    "title": "On the Interplay of Pre-Training, Mid-Training, and RL on Reasoning Language Models",
    "summary": "Recent reinforcement learning (RL) techniques have yielded impressive reasoning improvements in language models, yet it remains unclear whether post-training truly extends a model's reasoning ability beyond what it acquires during pre-training. A central challenge is the lack of control in modern training pipelines: large-scale pre-training corpora are opaque, mid-training is often underexamined, and RL objectives interact with unknown prior knowledge in complex ways. To resolve this ambiguity, we develop a fully controlled experimental framework that isolates the causal contributions of pre-training, mid-training, and RL-based post-training. Our approach employs synthetic reasoning tasks with explicit atomic operations, parseable step-by-step reasoning traces, and systematic manipulation of training distributions. We evaluate models along two axes: extrapolative generalization to more complex compositions and contextual generalization across surface contexts. Using this framework, we reconcile competing views on RL's effectiveness. We show that: 1) RL produces true capability gains (pass@128) only when pre-training leaves sufficient headroom and when RL data target the model's edge of competence, tasks at the boundary that are difficult but not yet out of reach. 2) Contextual generalization requires minimal yet sufficient pre-training exposure, after which RL can reliably transfer. 3) Mid-training significantly enhances performance under fixed compute compared with RL only, demonstrating its central but underexplored role in training pipelines. 4) Process-level rewards reduce reward hacking and improve reasoning fidelity. Together, these results clarify the interplay between pre-training, mid-training, and RL, offering a foundation for understanding and improving reasoning LM training strategies.",
    "link": "https://arxiv.org/abs/2512.07783"
  },
  {
    "id": "toward-training-superintelligent-software-agents-t-86c94aa1",
    "title": "Toward Training Superintelligent Software Agents through Self-Play SWE-RL",
    "summary": "While current software agents powered by large language models (LLMs) and agentic reinforcement learning (RL) can boost programmer productivity, their training data (e.g., GitHub issues and pull requests) and environments (e.g., pass-to-pass and fail-to-pass tests) heavily depend on human knowledge or curation, posing a fundamental barrier to superintelligence. In this paper, we present Self-play SWE-RL (SSR), a first step toward training paradigms for superintelligent software agents. Our approach takes minimal data assumptions, only requiring access to sandboxed repositories with source code and installed dependencies, with no need for human-labeled issues or tests. Grounded in these real-world codebases, a single LLM agent is trained via reinforcement learning in a self-play setting to iteratively inject and repair software bugs of increasing complexity, with each bug formally specified by a test patch rather than a natural language issue description. On the SWE-bench Verified and SWE-Bench Pro benchmarks, SSR achieves notable self-improvement (+10.4 and +7.8 points, respectively) and consistently outperforms the human-data baseline over the entire training trajectory, despite being evaluated on natural language issues absent from self-play. Our results, albeit early, suggest a path where agents autonomously gather extensive learning experiences from real-world software repositories, ultimately enabling superintelligent systems that exceed human capabilities in understanding how systems are constructed, solving novel challenges, and autonomously creating new software from scratch.",
    "link": "https://arxiv.org/abs/2512.18552"
  },
  {
    "id": "justrl-scaling-a-15b-llm-with-a-simple-rl-recipe-518641bc",
    "title": "JustRL: Scaling a 1.5B LLM with a Simple RL Recipe",
    "summary": "Recent advances in reinforcement learning for large language models have converged on increasing complexity: multi-stage training pipelines, dynamic hyperparameter schedules, and curriculum learning strategies. This raises a fundamental question: \\textbf{Is this complexity necessary?} We present \\textbf{JustRL}, a minimal approach using single-stage training with fixed hyperparameters that achieves state-of-the-art performance on two 1.5B reasoning models (54.9\\% and 64.3\\% average accuracy across nine mathematical benchmarks) while using 2$\\times$ less compute than sophisticated approaches. The same hyperparameters transfer across both models without tuning, and training exhibits smooth, monotonic improvement over 4,000+ steps without the collapses or plateaus that typically motivate interventions. Critically, ablations reveal that adding ``standard tricks'' like explicit length penalties and robust verifiers may degrade performance by collapsing exploration. These results suggest that the field may be adding complexity to solve problems that disappear with a stable, scaled-up baseline. We release our models and code to establish a simple, validated baseline for the community.",
    "link": "https://arxiv.org/abs/2512.16649"
  },
  {
    "id": "from-fx-and-gx-to-fgx-llms-learn-new-skills-in-rl--3d88dc0a",
    "title": "From $f(x)$ and $g(x)$ to $f(g(x))$: LLMs Learn New Skills in RL by Composing Old Ones",
    "summary": "Does RL teach LLMs genuinely new skills, or does it merely activate existing ones? This question lies at the core of ongoing debates about the role of RL in LLM post-training. On one side, strong empirical results can be achieved with RL even without preceding supervised finetuning; on the other, critics argue that RL contributes little beyond reweighting existing reasoning strategies. This work provides concrete evidence that LLMs can acquire genuinely new skills during RL by composing existing ones, mirroring one of the central mechanisms by which humans acquire new cognitive skills. To mitigate data contamination and other confounding factors, and to allow precise control over task complexity, we develop a synthetic framework for our investigation. Specifically, we define a skill as the ability to infer the output of a string transformation function f(x) given x. When an LLM has already learned f and g prior to RL, our experiments reveal that RL enables it to learn unseen compositions of them h(x)=g(f(x)). Further, this compositional ability generalizes to more difficult problems such as compositions of >2 functions unseen during RL training. Surprisingly, our experiments show that compositional skill acquired on a source task transfers to a different target task. This transfer happens even without compositional training on the target, requiring only prior knowledge of the target's atomic skills. Our qualitative analysis shows that RL fundamentally changes the reasoning behaviors of the models. In contrast, next-token training with the same data yields none of these findings. Our systematic experiments provide fresh insights into LLM learning, suggesting the value of first building base models with basic skills, then using RL to incentivize advanced, generalizable skills for complex problems.",
    "link": "https://arxiv.org/abs/2509.25123"
  },
  {
    "id": "low-precision-training-of-large-language-models-me-c508ffb5",
    "title": "Low-Precision Training of Large Language Models: Methods, Challenges, and Opportunities",
    "summary": "Large language models (LLMs) have achieved impressive performance across various domains. However, the substantial hardware resources required for their training present a significant barrier to efficiency and scalability. To mitigate this challenge, low-precision training techniques have been widely adopted, leading to notable advancements in training efficiency. Despite these gains, low-precision training involves several components$\\unicode{x2013}$such as weights, activations, and gradients$\\unicode{x2013}$each of which can be represented in different numerical formats. The resulting diversity has created a fragmented landscape in low-precision training research, making it difficult for researchers to gain a unified overview of the field. This survey provides a comprehensive review of existing low-precision training methods. To systematically organize these approaches, we categorize them into three primary groups based on their underlying numerical formats, which is a key factor influencing hardware compatibility, computational efficiency, and ease of reference for readers. The categories are: (1) fixed-point and integer-based methods, (2) floating-point-based methods, and (3) customized format-based methods. Additionally, we discuss quantization-aware training approaches, which share key similarities with low-precision training during forward propagation. Finally, we highlight several promising research directions to advance this field. A collection of papers discussed in this survey is provided in https://github.com/Hao840/Awesome-Low-Precision-Training.",
    "link": "https://arxiv.org/abs/2505.01043"
  },
  {
    "id": "deepseek-v3.2-exp-bd007c23",
    "title": "DeepSeek-V3.2-Exp",
    "summary": "We introduce DeepSeek-V3.2-Exp, an experimental sparse-attention model, which equips DeepSeek-V3.1-Terminus with DeepSeek Sparse Attention (DSA) through continued training. With DSA, a fine-grained sparse attention mechanism powered by a lightning indexer, DeepSeek-V3.2-Exp achieves significant efficiency improvements in both training and inference, especially in long-context scenarios. The model checkpoints are available at",
    "link": "https://github.com/deepseek-ai/DeepSeek-V3.2-Exp/blob/main/DeepSeek_V3_2.pdf"
  }
]